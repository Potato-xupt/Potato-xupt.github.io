<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[IP协议相关技术]]></title>
    <url>%2Fposts%2F15583%2F</url>
    <content type="text"><![CDATA[DNS我们平常在访问某个网站时不使用IP地址，而是用一串由几个英文字母用点号连接构成的域名。在这种情况下，产生了一个可以有效管理域名和IP地址之间对应关系的系统，那就是DNS系统。 在应用中，当用户输入域名时，DNS会自动检索那个注册了域名和IP地址的数据库，并迅速定位对应的IP地址。而且。如果主机名和IP地址需要进行变更时，也只需要在组织机构内部进行处理即可。 DNS系统是一个分布式数据库，它的结构如下图： DNS查询过程 例如当在浏览器输入www.qq.com域名时，浏览器首先会在自己的缓存中查找是否有该域名对应的IP地址，如果有，就先调用这个IP地址映射，完成域名解析。 如果浏览器中没有缓存，则操作系统会先检查自己本地的hosts文件中是否有这个网址的映射关系，如果有，就先调用这个IP地址映射，完成域名解析。 如果hosts中没有这个域名的映射，则查找本地的DNS解析器缓存，是否有这个网址的映射，如果有则直接返回，完成解析。 如果hosts文件与本地DNS解析器缓存都没有对应的网址映射关系，首先会找这个TCP/IP参数中设置的首选DNS服务器，在此我们叫它本地DNS服务器，此服务器收到查询时，如果本地配置资源中包含要查找到域名的映射关系，则返回解析结果给客户端，完成域名解析，此解析具有权威性。 如果要查询的域名不由本地DNS服务器区域解析，但是该服务器已缓存了此网址的映射，则调用这个映射结果，完成域名解析，此解析不具有权威性。 如果本地DNS服务器的本地区域文件和缓存都解析失败的话，则根据本地DNS服务器端设置进行查询。如果未用转发模式，本地DNS服务器就会把请求发送至13台根DNS服务器，根DNS服务器收到请求后会判断这个域名是谁来授权管理，并会返回一个负责该顶级DNS服务器的IP地址。本地DNS服务器收到IP信息后，将会联系这台顶级DNS服务器。这台顶级DNS服务器收到请求后，如果自己无法解析，他会找一个自己的下一级DNS服务器，也就是权威DNS服务器，给本地DNS服务器。当本地DNS服务器收到返回的权威DNS服务器IP时，重复上面的动作进行查询，直到找到www.qq.com的IP地址。 如果用的是转发模式，次DNS服务器会把请求转发至上一级DNS服务器，由上一级进行解析，上一级如果不能解析，或找根DNS服务器或把请求转发至上上级，以此循环，直到找到www.qq.com的IP地址。 ARP只要确定了IP地址，就可以向这个目标地址发送IP数据报。然而，在底层数据链路层，进行实际通信时却有必要了解每个IP地址所对应的MAC地址。ARP是一种解决地址问题的协议，以目标IP地址为线索，用来定位下一个应该接收数据分包的网络设备对应的MAC地址。 ARP工作机制简单地说，ARP是借助ARP请求与ARP响应两种类型的包确定MAC地址的。假设主机A向同一链路上的主机B发送IP包，主机A的IP地址为172.20.1.1，主机B的IP地址为172.20.1.2，它们互不知道对方的MAC地址。 主机A为了获得主机B的MAC地址，起初要通过广播发送一个ARP请求包。这个包中包含了想要了解其MAC地址的主机IP地址，也就是说，ARP请求包中已经包含了主机B的IP地址172.20.1.2.由于广播的包可以被同一链路上的所有的主机和路由器接收，因此ARP请求的包也就会被这同一链路上的所有注解和路由器进行解析。如果ARP请求包中的目标IP地址与自己的IP地址一致，那么这个节点就将自己的MAC地址塞入ARP响应包返回给主机A。 总之，从一个IP地址发送ARP请求包以了解其MAC地址，目标地址将自己的MAC地址填入其中的ARP响应包返回到IP地址，由此，可以通过ARP从IP地址获得MAC地址，实现链路内的IP通信。 如果没法送一个IP数据包都要进行一次ARP请求将会造成不必要的网络流量，因此，通常的做法就是把获取到的MAC地址缓存一段时间。 RARPRARP是将ARP反过来，从MAC地址定位IP地址的一种协议。例如将打印机服务器等小型嵌入式设备接入到网络时就经常会用得到。 平常我们个人电脑设置IP地址，也可以通过DHCP自动分配获取IP地址。然而对于嵌入式设备，会遇到没有任何输入接口或无法通过DHCP动态获取IP地址的情况。在类似情况下，就可以使用RARP。 为此，需要架设一台RARP服务器，从而在这个服务器上注册设备的MAC地址已经IP地址，然后再将这个设备连接到网络，插电启动时，该设备会发送一条“我的MAC地址是***，请告诉我我的IP地址应该是什么”的请求信息。RARP服务器接受到这个请求后，返回类似于“MAC地址为***，IP地址为***”的信息给这个设备。而设备就根据从RARP服务器所收到的应答信息，从而设置自己的IP地址。 代理ARP通常ARP包会被路由器隔离，但是采用代理ARP的路由器可以将ARP请求转发给邻近的网段。由此，两个以上网段的节点之间就可以像在一个网段中一样进行转发通信。 ARP攻击和防护ARP会引起的攻击有以下几种： ARP泛洪攻击：通过向网关发送大量ARP报文，导致网关无法正常响应。首先发送大量的ARP请求报文，然后又发送大量虚假的ARP响应报文，从而造成网关部分的CPU利用率上升难以响应正常服务请求，而且网关还会被错误的ARP表充满导致无法更新维护正常ARP表，消耗网络带宽资源。 ARP欺骗主机的攻击：ARP欺骗主机的攻击也是ARP众多攻击类型中很常见的一种。攻击者通过ARP欺骗使得局域网内被攻击主机发送给网关的流量信息实际上都发送给攻击者。主机刷新自己的ARP使得在自己的ARP缓存表中对应的MAC为攻击者的MAC，这样一来其他用户要通过网关发送出去的数据流就会发往主机这里，这样就会造成用户的数据外泄。 欺骗网关的攻击：欺骗网关就是把别的主机发送给网关的数据通过欺骗网关的形式使得这些数据通过网关发送给攻击者。这种攻击目标选择的不是个人主机而是局域网的网关，这样就会攻击者源源不断的获取局域网内其他用户韵数据．造成数据的泄露，同时用户电脑中病毒的概率也会提升。 中间人攻击：中间人攻击是同时欺骗局域网内的主机和网关，局域网中用户的数据和网关的数据会发给同一个攻击者，这样，用户与网关的数据就会泄露。 IP地址冲突攻击：通过对局域网中的物理主机进行扫描，扫描出局域网中的物理主机的MAC地址，然后根据物理主机的MAC进行攻击，导致局域网内的主机产生IP地址冲突，影响用户的网络正常使用。 相对应的攻击防护方法有以下几种： 不要把网络信任关系单纯地建立在IP基础上或MAC基础上（RARP同样存在欺骗的问题），应在网络中架设DHCP服务器，绑定网关与客户端IP+MAC，该做法需要注意的是要保证网络中的dhcp服务器相互之间不冲突。 添加静态的ARP映射表，不让主机刷新设定好的映射表，该做法适用于网络中主机位置稳定，不适用在主机更换频繁的局域网中。 停止使用ARP，将ARP作为永久条目保存在映射表中。 架设ARP服务器。通过该服务器查找自己的ARP映射表来响应其他机器的ARP广播。 IP的传输使用“proxy”代理。 使用防火墙等连续监控网络。注意有使用SNMP的情况下，ARP的欺骗有可能导致陷阱包丢失。 ICMP架构IP网络时需要特别注意两点：确认网络是否正常工作，以及遇到异常时进行问题诊断。ICMP正式提供这类功能的一种协议。 ICMP的主要功能包括，确认IP包是否成功送达目标地址，通知在发送过程当中IP包被废弃的具体原因，改善网络设置等。有了这些功能，就可以获得网络是否正常，设置是否有误以及设备有何异常等信息，从而便于进行网络上的问题诊断。不过ICMP时基于尽力而为的IP进行工作的，因此无法保证服务质量，而且在网络安全优先于便利性的环境中往往无法使用ICMP，因此不宜过分依赖于ICMP。 在IP通信中如果主机A向主机B发送了数据包，由于某种原因，途中的路由器2未能发现主机B的存在，这是，路由器2就会像主机A发送一个ICMP包，说明发往主机B的包未能成功。 ICMP的这种通知消息会使用IP进行发送，在ICMP中，包以明文的形式像TCP/UDP一样通过IP进行传输。然而ICMP所承担大二功能并非传输层的补充，而应该把它考虑为IP的一部分。因此，从路由器2返回的ICMP包会按照往常的路由控制先经过路由器1再转发给主机A。收到该ICMP包的主机A则分解ICMP的首部和数据域以后得知具体发生问题的原因。 ICMP的消息大致可以分为两类：一类是通知出错原因的错误消息，另一类是用于诊断的查询消息。 主要的ICMP消息 ICMP目标不可达消息（类型3）：IP路由器无法将IP数据包发送给目标地址时，会给发送端主机返回一个目标不可达的ICMP消息，并在这个消息中显示不可达的具体原因。在实际通信中常会遇到错误代码1，表示主机不可达，它是指路由表中没有该主机的信息，或者该主机没有连接到网络的意思；此外错误代码4则用于前面介绍过的MTU探索。 TCMP重定向消息（类型5）：如果路由器发现发送端主机使用了次优的路径发送数据，那么它会返回一个ICMP重定向的消息给这个主机。在这个消息中包含了最合适的路由信息和源数据。这主要发生在路由器持有更好的路由信息的情况下。 ICMP超时消息（类型11）：IP包中有一个字段叫做TTL，当它的值减到0时该IP包会被丢弃。此时IP路由器将会发送一个ICMP超时的消息（错误号0）给发送端主机，并通知该包已被丢弃。设置IP包生存周期的目的是为了，在路由控制遇到问题发生循环状况时，避免IP包无休止的在网络上被转发。同时也可以控制包的到达范围。 ICMP回送消息（类型0、8）：用于进行通信主机或路由器之间，判断所发送的数据包是否以及成功到达对端的一种消息。可以向对端主机发送回送请求的消息（类型8），也可以接收对端主机发回来的回送应答消息（类型0）。网络上最常用的ping命令就是利用这个消息实现的。 ICMP原点抑制消息（类型4）：在使用低速广域线路的情况下，连接WAN的路由器可能会遇到网络拥堵的问题，ICMP原点抑制消息的目的就是为了缓和这种拥堵情况。 ICMP路由器探索消息（类型9、10）：主要用于发现与自己相连网络中的路由器。 ICMP地址掩码消息（类型17、18）：主要用于主机或路由器想要了解子网掩码的情况。 ICMPv6IPv4中ICMP仅作为一个辅助作用支持IPv4，然而在IPV6中，ICMP作用被扩大，如果没有ICMPv6，IPv6就无法进行正常通信。 在IPv6中，从IP地址定位MAC地址的协议，从ARP转为ICMP的邻居探索消息。这种邻居探索消息融合了IPv4的ARP、TCMP重定向以及ICMP路由器选择消息等功能于一体，甚至还提供自动设置IP地址的功能。 ICMPv6将ICMP大致分为了两类：错误消息和信息消息。 ICMP中从类型133至类型137的消息叫做邻居探索消息。邻居请求消息用于查询IPv6的地址与MAC地址的对应关系，并由邻居宣告消息得知MAC地址。 此外IPv6中实现了即插即用的功能，所以在没有DHCP服务器的情况下也能实现IP地址的自动获取。如果是一个没有路由器的网络，就使用MAC地址作为链路本地单播地址。而在一个有路由器的网络环境中，可以从路由器获得IPv6地址的前面部分，后面部分则由MAC地址进行设置。 DHCP为了实现自动设置IP地址、统一管理IP地址分配，就产生了DHCP协议。有了DHCP，计算机只要连接到网络，就可以进行TCP/IP通信。 DHCP工作机制使用DHCP之前，首先要架设一台DHCP服务器。然后将DHCP所要分配的IP地址设置到服务器上。此外，还需要将相应的子网掩码、路由控制信息以及DNS服务器的地址设置到服务器上。 DHCP分配IP地址有两种方法，一种是由DHCP服务器在特定的IP地址中自动选出一个进行分配。另一种方法是针对MAC地址分配一个固定的IP地址。而且这两种方法可以并用。 DHCP中继代理在一个企业或者学校等大规模组织机构的网络环境中，一般会有多个以太网网段，在这种情况下，若要针对每个网段都设置DHCP服务器将会是个庞大的工程。 因此在这种网络环境中，往往需要将DHCP统一管理。具体方法可以使用DHCP中继代理来实现。有了DHCP中继代理以后，对不同网段的IP地址分配也可以由一个DHCP服务器统一进行管理和运维。 这种方法使得在每个网段架设一个DHCP服务器被取代，只需在每个网段设置一个DHCP中继代理即可。它可以设置DHCP服务器的IP地址，从而可以在DHCP服务器上为每个网段注册IP地址的分配范围。 DHCP客户端会向DHCP中继代理发送DHCP请求包，DHCP中继代理在收到这个广播包以后再以单播的形式发给DHCP服务器。服务端收到该包以后再向DHCP中继代理返回应答，并由DHCP中继代理将此包转发给DHCP客户端。由此，DHCP服务器即使不在同一个链路上也可以实现统一分配和管理IP地址。 NATNAT是用于在本地网络中使用私有地址，在连接互联网时转而使用全局IP地址的技术。除转换IP地址外，还出现了可以转换TCP、UDP端口号的NAPT技术。NAT（NATP）实际上是为了应对正在面临地址枯竭的IPv4而开发的技术。 NAT工作机制如图以10.0.0.10的主机与163.221.120.9的主机进行通信为例。利用NAT，图中的NAT路由器将发送源地址从10.0.0.10转换为全局的IP地址（202.244.174.37）再发送数据。反之，当包从地址163.221.120.9发过来时，目标地址（202.244.174.37）先被转换成私有IP地址10.0.0.10以后再被转发。在NAT（NAPT）路由器的内部，有一张自动生成的用来转换地址的表。当10.0.0.10向163.221.120.9发送第一个包时生成这张表，并按照表中的映射关系进行处理。 NAT-PT（NAPT-PT）为了解决NAT在IPv6中也能正常使用的问题，就产生了NAT-PT（NAPT-PT）规范。NAT-PT（NAPT-PT）是将IPv6的首部转换为IPv4的首部的一种技术。有了这种技术，那些只有IPv6地址的主机也能够与IPv4地址的其他主机进行通信了。 NAT的问题与解决方法由于NAT（NAPT）都依赖于自己的转换表，因此会有如下几点限制： 无法从NAT的外部向内部服务器建立连接。 转换表的生成与转换操作都会产生一定的开销。 通信过程中一旦NAT遇到异常需重新启动时，所有的TCP连接都将被重置。 即使备置两台NAT做容灾备份，TCP连接还是会断开。 解决NAT上述潜在问题有两种方法： 改用IPv6。 即使是在一个没有NAT的环境里，根据所制作的应用，用户可以完全忽略NAT的存在而进行通信。 IP隧道在如图所示的网络环境里，网络A、B使用IPv6，如果处于中间的网络C支持使用IPv4的话，网络A与网络B之间将无法直接进行通信。为了让他它们之间正常通信，这时就必须采用IP隧道的功能。 IP隧道中可以将那些从网络A发过来的IPv6的包统和为一个数据，在为之追加一个IPv4的首部以后转发给网络C。这种在网络层的首部后面继续追加网络层首部的通信方法就叫做“IP隧道”。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[IP协议]]></title>
    <url>%2Fposts%2F18937%2F</url>
    <content type="text"><![CDATA[IP相当于OSI参考模型的第3层IP相当于OSI参考模型中的第3层——网络层。 网络层的主要作用是“实现终端节点之间的通信”。这种终端节点之间的通信也叫“点对点”通信。 从前面的章节可知，网络层的下一层是数据链路层，数据链路层的主要作用是再互联同一种数据链路的节点之间进行包传递。而一但快遇多种数据链路，就需要借助网络层。网络层可以跨越不同的数据链路，即使是在不同的数据链路上也能实现两端节点之间的数据包传输。 IP基础知识在计算机通信中，为了识别通信对端，必须要有一个类似于地址的识别码进行标识，MAC地址正是用来标识同一数据链路中不同计算机的一种识别码；而IP地址用于在“连接到网络中的所有主机中识别出进行通信的目标地址”，因此在TCP/IP通信中所有注解或路由器必须设定自己的IP地址。 路由控制路由控制（Routing）是指将分组数据发送到最终目标地址的功能，即使网络非常复杂，也可以通过路由控制确定到达目标地址的通路。一旦这个路由控制的运行出现异常，分组数据极有可能迷失，无法到达目标地址。因此，一个数据包之所以能成功地到达最终的目标地址，全靠路由控制。 当某个IP包到达路由器时，路由器首先查找目标地址，从而再决定下一步应该将这个包发往哪个路由器，然后将包发送过去。当这个IP包到达那个路由器以后，会再次经历查找下一目标地址的过程，并由该路由器转发给下一个被找到的路由器。这个过程可能会反复多次，知道找到最终的目标地址，将数据包发送给这个节点。 为了将数据包发给目标主机，所有主机都维护着一张路由控制表。该表记录IP数据在下一步应该发给哪个路由器。IP包根据这个路由表在各个数据链路上传输。 数据链路的抽象化IP是实现多个数据链之间通信的协议。数据链路根据种类的不同各有特点，不同数据链路有个最大的区别：就是它们各自的最大传输单位（MTU）不同。MTU的值再以太网是1500字节，再FDDI中是4352字节，而ATM则为9180字节。IP的上一层可能会要求传送比这些MTU更多字节数据，因此必须在线路上传送比包长还要小的MTU。 为了解决这个问题，IP进行分片处理。顾名思义，所谓分片处理是指，将较大的IP包分成多个较小的IP包。分片的包到了对端目标地址以后会再被组合起来传给上一层。 IP属于面向无连接型IP面向无连接，即在发包之前，不需要建立与对端目标地址之间的连接，上层如果遇到需要发送给IP的数据，该数据会立即被压缩成IP包发送出去。 IP采用面向无连接的主要原因有两点： 一是为了简化。面向连接比起面向无连接处理相对复杂。甚至管理每个连接本身就是一个相当繁琐的事情。 二是为了提速。每次通信之前都要事先建立连接，又会降低处理速度。 需要连接时，可以委托上一层提供此项服务。因此IP为了实现简单化与高速化采用面向无连接的方式。 IP提供尽力服务，意指“为了把数据包发送到最终目标地址，尽最大努力”。然而，它并不做“最终收到与否的验证”。IP数据包在途中可能会发生丢包、错位以及数据量翻倍等问题。 IP地址基础知识IP地址的定义IP（IPv4）地址由32为正整数来表示。TCp/IP通信要求将这样的IP地址分配给每一个参与通信的主机。采用将32位的IP地址以每8位为一组，分成4组，每组以“.”隔开，再将每组数转换为十进制数。 IP地址并非是根据主机台数来配置的，而是每一台主机上的每一块网卡都得设置IP地址。通常一块网卡只设置一个IP地址，但其实一块网卡也可以设置多个IP地址。此外，一台路由器通常都会配置两个以上的网卡，因此可以设置两个以上的IP地址。 IP地址的组成IP地址由“网络标识（网络地址）”和“主机标识（主机地址）”两部分组成。 网络标识必须保证相互连接的每个段的地址不相重复。而相同段内相连的主机必须有相同的网络地址。IP地址的“主机标识”则不允许在同一个网段内重复出现。由此，可以通过设置网络地址和主机地址，在相互连接的整个网络中保证每台主机的IP地址都不会相互重复。即IP地址具有了唯一性。 最初二者以分类进行区别，而现在基本以子网掩码（网络前缀）区分。 IP地址的分类IP地址分为四个级别，分别为A类、B类、C类、D类。它根据IP地址中从第1位到第4位的比特列对其网络标识和主机标识进行区分。 A类地址：A类IP地址是首位以“0”开头的地址。从第1位到第8位是他的网络标识。用十进制表示的话，0.0.0.0~127.0.0.0是A类的网络地址。A类地址的后24位相当于主机标识。 B类地址：B类IP地址是前两位为“10”的地址。从第1位到第16位是它的网络标识。用十进制表示的话，128.0.0.0~191.255.0.0是B类的网络地址。B类地址的后16位相当于主机标识。 C类地址：C类IP地址是前三位为“110”的地址。从第1位到第24位是它的网络标识。用十进制表示的话，192.0.0.0~223.255.255.0是C类的网络地址。C类地址的后8位相当于主机标识。 D类地址：D类IP地址是前四位为“1110”的地址，从第1为到第32位是它的网络标识。用十进制表示的话，224.0.0.0~239.255.255.255.255是D类的网络地址。D类网络地址没有主机标识，常被用于多播。在IP地址分配时有一点需要注意，不可以全部为0或全部为1。因为全部为0在表示对应的网络地址或IP地址不可获知的情况下才使用。而全部为1的主机地址通常作为广播地址。 广播地址广播地址用于在一个链路中相互连接的主机之间发送数据包。将IP地址中的主机地址部分全部设置为1，就成为了广播地址。 广播地址分为本地广播和直接广播两种。在本网络内的广播叫做本地广播，在不同网络之间的广播叫做直接广播。 子网掩码起初一个IP地址只要确定了其分类，也就确定了它的网络标识和主机标识，网络标识相同的计算机必须同属于同一个链路。但是这样会造成很多的IP地址浪费，因此后来人们改进使用子网掩码进行区分。 对于子网掩码，目前有两种标识方式。以172.20.100.52的前26位是网络地址的情况为例，以下是其中一种表示方法，它将IP地址与子网掩码的地址分别用两行来表示： 12345678IP地址 172. 20. 100. 52子网掩码 255. 255. 255. 192网络地址 172. 20. 100. 0子网掩码 255. 255. 255. 192广播地址 172. 20. 100. 63子网掩码 255. 255. 255. 192 另一种表示方法如下所示，它在每个IP地址后面追加网络地址的位数用“/”隔开。 123IP地址 172. 20. 100. 52 /26网络地址 172. 20. 100. 0 /26广播地址 172. 20. 100. 63 /26 路由控制发送数据包时所使用的地址是网络地址，即IP地址。然而仅仅有IP地址还不足以将数据包发送到对端目标地址，在数据发送过程中还需要类似于“知名路由器或主机”的信息，以便真正的发往目标地址。保存这种信息的就是路由控制表。 该路由控制表的形成方式有两种：一种是管理员手动设置，另一种是路由器与其他路由器相互交换形信息时自动刷新。前者也叫静态路由控制，后者叫做动态路由控制。 IP地址与路由控制路由控制表中记录着网络地址与下一步应该发送至路由器的地址，在发送IP包时，首先要确定IP包首部中的目标地址，再从路由控制表中找到与该地址具有相同网络地址的记录，根据该记录将IP包转发给相应的下一个路由器。如果控制表中存在多条相同网络地址的记录，就选择一个最为吻合的网络地址，所谓最为吻合是指相同位数最多的意思。 如果一张路由表中包含所有的网络及其子网的信息，将会造成无端的浪费，这是，默认路由是不错的选择。默认路由是指路由表中任何一个地址都能与之匹配的记录。默认路由一般标记为0.0.0.0./0或default。 “IP地址/32”也被称为主机路由。它的意思是整个IP地址的所有位都将参与路由。进行主机路由，意味着基于主机上网卡配置的IP地址本身，而不是基于该地址的网络地址部分进行路由。主机路由多被用于不希望通过网络地址路由的情况。 IP分割处理与再构成处理如前面介绍所说，每种数据链路的最大传输单元（MTU）不尽相同，简与IP数据数据链路上一层，它必须不受限于不同数据链路的MTU大小。 IP报文的分片与重组任何一台主机都有必要对IP分片进行相应的处理。分片往往在网络上遇到比较大的报文无法一下子发送出去时才会进行处理。 分片默认以8个字节的倍数为单位进行，经过分片之后的IP数据报在被重组的时候，只能由目标主机进行。路由器虽然做分片但是不会进行重组。 路径MTU发现分片机制也有它的不足，例如路由器的处理符合加重，一旦某个分片丢失，会造成整个IP数据报作废等等。为了应对这些问题，产生了一种新的技术“路径MTU发现”。 所谓路径MTU是指从发送端主机到接收端主机之间不需要分片时最大的MTU的大小。即路径中存在的所有数据链路中最小的MTU。而路径MTU发现从发送主机按照路径MTU的大小将数据报分片后进行发送，进行路径MTU发现，就可以避免在中途的路由器上进行分片处理，也可以在TCP中发送更大的包。 路径MTU发现的工作原理如下：首先在发送端主机发送IP数据报时将其首部的分片禁止标志设置为1。根据这个标志位，途中的路由器即使遇到需要分片才能处理的大包，也不会去分片，而是将包丢弃。随后，通过一个ICMP的不可达消息将数据链路上MTU的值给发送主机。 下一次，从发送给同一目标主机的IP数据报获得ICMP所通知的MTU值以后，将它设置为当前MTU。发送主机根据这个MTU对数据报进行分片处理。如此反复，直到数据报被发送到目标主机为止没有再收到任何ICMP，就认为最后一次ICMP所统治的MTU即是一个合适的MTU值。 那么，当MTU的值比较多时，最少可以缓存约10分钟。在这10分钟内使用刚刚求得的MTU，但过了这10分钟以后则重新根据链路上的MTU做一次路径MTU发现。 下面是UDP和TCP的路径MTU发现过程： IPv6IPv6是为了根本解决IPv4地址耗尽的问题而被标准化的网络协议。IPv4的地址长度为4个8位字节，即32比特。而IPv6的地址长度则是原来的4倍，即128比特，一般写成8个16字位字节。 IPv6的特点IPv6通过改进IPv4的缺点，减轻了管理员的负担。 IP地址的扩大与路由控制表的聚合 IP地址依然适应互联网分层构造。分配与其地址结构相适应的IP地址，尽可能避免路由表膨大 性能提升 支持即插即用功能，即使没有DHCP服务器也可以实现自动分配IP地址 采用认证与加密功能，应对伪造IP地址的网络安全功能以及防止路线窃听的功能 多播、Mobile IP称为扩展功能 IPv6在IP地址的标记方法一般人们将128比特IP地址以每16为一组，每组用冒号“：”隔开进行标识。而且如果出现连续的0时还可以将这些0省略，并用两个冒号“：：”隔开，但是一个IP地址中只允许出现一次两个连续的冒号。 IPv6分段处理IPv6的分片处理只在作为起点的发送端主机上进行，路由器不参与分片。这也是为了减少路由器的负荷、提高网速。因此IPv6中的“路径MTU发现”功能必不可少。 不过IPv6中最小的MTU为1280字节，因此，在嵌入式系统中对于那些有一定系统资源限制的设备来说，不需要进行“路径MTU发现”，而是在发送IP包时直接以1280字节为单位分片发出。 IPv4首部通过IP进行通信时，需要在数据的前面加入IP首部信息。IP首部中包含用于IP协议进行发包控制时所有的必要信息。 版本：由4比特构成，表示标识IP首部的版本号，IPv4的版本号为4，因此在这个字段上的值即为“4”，IPv6的版本号为6。 首部长度：由4比特构成，表明IP首部的大小，单位为4字节。对于没有可选项的IP包，首部长度则设置为5. 区分服务：由8比特构成，用来表明服务质量，如下表： DSCP段与ECN段：DSCP（差分服务代码点）是TOS的一部分，用来进行质量控制。ECn（显示拥塞通告）用来报告网络拥堵情况，由两个比特构成。 总长度：表示IP首部与数据部分合起来的总字节数，该字段长16比特，因此IP包的最大长度为65535字节。 标识（ID）：由16比特构成，用于分片重组，同一个分片的标识值相同，不同分片的标识值不同。通常每发送一个IP包，它的值也逐渐递增。此外，即使ID相同，如果目标地址、源地址或协议不同的话，也会被认为是不同的分片。 标志：由3比特构成，标识包倍分片的相关信息。具体含义如下： 片位移：由13比特构成，用来标识被分片的每一个分段相对于原始数据的位置。第一个分片对应的位置为0。 生存时间（TTL）：由8比特构成，最初意思是以秒为单位记录当前包在网络上应该生存的限期，实际中它是指可以中转多少个路由器的意思，每经过一个路由器，TTL就会减少1，直到变成0则丢弃包。TTl不会超过256，由此可以避免IP包在网络内无限传递的问题。 协议：由8比特构成，表示IP包传输层的上层协议编号。 首部校验和：由16比特构成，也叫IP首部校验和。盖子都安只校验数据报的首部，不校验数据部分。它主要用来确保IP数据报不被破坏。 源地址：由32比特构成，表示发送端IP地址。 目标地址：由32比特构成，表示接收端IP地址。 可选项：长度可变，通常只在进行实验或诊断时使用，包含信息有如下几点：安全级别、源路径、路径记录和时间戳。 填充：也称作填充物，在有可选项的情况下，首部长度可能不是32比特的整数倍，为此，通过向该字段填充0，调整为32比特的整数倍。 数据：存入数据。将IP上层协议的首部也为作为数据进行处理。 IPv6首部格式IPv6的IP数据首部格式相比IPv4已经发生巨大变化： 版本：同IPv4一样。 通信量类：相当于IPv4的TOS字段。至今没什么用。 流标号：由20比特构成，准备用于服务质量控制，现在也没有用。只有流标号、源地址以及目标地址三项完全一致时，才被认为是一个流。 有效载荷长度：有效载荷是指包的数据部分，这里不包括首部，只表示数据部分的长度。 下一个首部：相当于IPv4的协议字段，由8比特构成。 跳数限制：由8比特构成，相当于IPv4的TTL。 源地址：由128比特构成，表示发送端IP地址。 目标地址：由128比特构成，表示接收端IP地址。 IPv6的首部长度固定，无法将可选项加入其中，取而代之的是通过扩展首部对功能进行了有效扩展。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[计算机网络基础知识]]></title>
    <url>%2Fposts%2F13182%2F</url>
    <content type="text"><![CDATA[OSI参考模型OSI参考模型将复杂的网络协议整理并分为易于理解的7个分层，对通信中必要的功能做了很好的归纳。不过，OSI参考模型始终是一个模型，它也只是对各层的作用做了一系列粗略的界定，并没有对协议和接口进行详细的定义。 OSI参考模型中各个分层的作用 应用层：为应用程序提供服务并规定应用程序中通信相关的细节，包括文件传输、电子邮件、远程登录等协议。 表示层：将应用程序的信息转换为适合网络传输的格式，或将来自下一层的数据转换为上层能够处理的格式，因此它主要负责数据格式转换。 会话层：负责建立和断开通信连接（数据流动的逻辑通路），以及数据的分割等数据传输相关的管理。 传输层：起着传输的作用，只在通信双方节点上进行处理，而无需在路由器上处理。 网络层：将数据传输到目标地址，目标地址可以是多个网络通过路由器连接而成的某一个地址，因此这一层主要负责寻址和路由选择。 数据链路层：负责物理层面上互连的、节点之间的通信传输。 物理层：负责0、1比特流与电压的高低、光的闪灭之间的互换。 OSI参考模型通信处理举例下面举例说明7层网络模型的功能，假设使用主机A的用户A要给使用主机B的用户B发送一封电子邮件。 应用层用户A在主机A上新建一封电子邮件，指定收件人为B，并输入邮件内容为“早上好”。 从用户输入完所要发送到内容并点击“发送”按钮的那一刻开始，就进入了应用层协议的处理，该协议会在所要传送数据的前端附加一个首部（标签）信息。如果主机B上收件人的邮箱空间已满无法接收新的邮件，则会返回一个错误给发送方。对这类异常的处理也属于应用层要解决的问题。 表示层表示层的“表示”有“表现”、“演示”的意思，因此更关注数据的具体表现形式。此外，所使用的应用软件本身的不同也会导致数据的表现形式截然不同，例如有的字处理软件的文件只能由该字处理厂商所提供的特定版本的软件才来打开读取。 利用应用层，将数据从“某个计算机特定的数据格式”转换为“网络通用的标准数据格式”后，再发送出去。接收端主机收到数据以后，将这些网络标准格式的数据恢复为“该计算机特定的数据格式”，然后再进行相应处理。 会话层决定采用何种连接方法是会话层的主要责任。会话层也会像应用层或表示层那样，在其收到的数据前端附加首部信息或标签信息后再转发给下一层。而这些首部或标签中记录着数据传送顺序的信息。 会话层只对何时建立连接、何时发送数据等问题进行管理，并不具有实际传输数据的功能。 传输层主机A确保与主机B之间的通信并准备发送数据。这一过程叫“建立连接”。有了这个通信连接就可以使主机A发送的电子邮件到达主机B中，并由主机B的邮件处理程序获取最终数据。此外当通信传输结束后，有必要将连接断开。 如上，进行建立连接或断开连接的处理，在两个主机之间创建逻辑上的通信连接即是传输层的主要作用。此处请注意，会话层负责决定建立连接和断开连接的时机，而传输层进行实际的连接和断开处理。 此外，传输层为确保所传输的数据到达目标地址，会在通信两端的计算机之间进行确认，如果数据没有到达，它会负责重发。例如，主机A将“早上好”这一数据发送给主机B，期间可能会因为某些原因导致数据被破坏，或由于发生某种网络异常致使只有一部分数据到达目标地址。假设主机B只收到了“早上”这一部分数据，那么会在它收到数据后将自己没有收到“早上”之后那部分数据的事实告知主机A，主机A得知这个情况后就会将后面的“好”重发给主机B，并再次确认对端是否收到。 网络层网络层的作用是在网络与网络相互连接的环境中，将数据从发送端主机发送到接收端主机，两端主机之间虽有众多数据链路，但能够将数据从主机A送到主机B也是网络层的功劳。 在实际发送数据时，目的地址至关重要，这个地址是进行通信的网络中唯一指定的序号。 数据链路层、物理层通信传输实际上是通过物理的传输介质实现的。数据链路层的作用就是在这些通过传输介质互连的设备之间进行数据处理。 物理层中，将数据的0、1转换为电压和脉冲光传输给物理的传输介质，而相互直连的设备之间使用地址实现传输，这种地址被称为MAC地址，也可称为物理地址或硬件地址。采用MAC地址，目的是为了识别连接到同一传输介质上的设备。因此，在这一层中将包含MAC地址信息的首部附加到从网络层转发过来的数据上，将其发送给网络。 传输方式的分类网络与通信中可以根据其数据发送方法进行多种分类，分类方法也有很多，下面介绍几种。 面向有连接型与面向无连接型通过网络发送数据，大致可以分为面向有连接与面向无连接两种类型。面向有连接型包括TCP、ATM、帧中继等协议；面向无连接型包括UDP、IP、以太网等协议。 面向有连接型：面向有连接型中，在发送数据之前，需要在收发主机之间连接一条通信线路；在通信传输结束之后，专门断开连接。 面向无连接型：面向无连接型则不要求建立和断开连接。发送端可于任何时候自由发送数据，反之接收端也永远不知道自己会在何时从哪里收到数据。因此在面向无连接的情况下，接收端需要时常确认是否收到了数据。面向无连接型采用分组交换的情况要多一些。 电路交换与分组交换在电路交换中，交换机主要负责数据的中转处理。计算机首先被连接到交换机上，而交换机与交换机之间则由众多通信线路再继续连接。因此计算机之间再发送数据时，需要通过交换机与目标主机建立通信电路，我们将连接电路称为建立连接。建立好连接之后，用户就可以一直使用这条电路，知道该链接被断开为止。 如果某条电路只是用来连接两台计算机的通信线路，就意味着只需要在这两台计算机之间实现通信，因此这两台计算机是可以独占线路进行数据传输的。但是，如果一条电路上连接了多台计算机，而这些计算机之间需要相互传递数据，就会出现新的问题。鉴于一台计算机在收发信息时会独占整个电路，其他计算机只能等待这台计算机处理结束以后才有机会使用这条电路发送数据，并且在此过程中，谁也无法预测某一台计算机的数据传输从何开始又在何时结束。如果并发用户数超过交换机之间的通信线路数，就意味着通信根本无法实现。 在分组交换中，由分组交换机（路由器）连接通信线路。分组交换的大致处理过程是：发送端计算机将数据分组发送给路由器，路由器收到这些分组数据以后，缓存到自己的缓冲区，然后再转发给目标计算机。因此分组交换也有另一个名称：蓄积交换。路由器收到数据以后会按照顺序缓存到相应的队列中，再以先进先出的顺序将它们逐一发送出去。 在分组交换中，计算机与路由器之间以及路由器与路由器之间只有一条通信线路，因此这条线路其实是一条共享线路。在电路交换中，计算机之间的传输速度不变。然而在分组交换中，通信线路的速度可能会有所不同，根据网络拥堵的情况，数据达到目标地址的时间有长有短。另外，路由器的缓存饱和或溢出时，甚至可能会发生分组数据丢失、无法发送到对端的情况。 根据接收端数量分类 单播：就是一对一通信。 广播：它指的是将消息从一台主机发送给与之相连的所有其他主机。广播通信的一个典型例子就是电视播放。 多播：多播与广播类似，不同之处子啊与多播要限定某一组主机作为接收端。多播通信最典型的例子就是电视会议。 任播：任播是指在特定的多台主机中选出一台作为接收端的一种通信方式。 地址通信传输中，发送端和接收端可以被视为通信主体，他们都能由一个所谓的“地址”的信息加以标识出来。当人们使用电话时，电话号码就相当于“地址”。当地址总数并不是很多的情况下，有了唯一地址就可以定位相互通信的主体。然而当地址的总数越来越多的时候，如何高效的从中找出通信的目标地址将成为一个重要的问题，为此人们发现地址除了具有唯一性还需要具有层次性。 MAC地址和IP地址在标识一个通信主体时虽然都具有唯一性，但是它们当中只有IP地址具有层次性。MAC地址由设备的制造厂商针对没块网卡进行分别指定；IP地址由网络号和主机号两部分组成，即使通信主体的IP地址不同，若主机号不同，网络号相同，说明它们处于同一个网段。 网络传输中，每个节点会根据分组数据的地址信息，来判断报文应该由哪些网卡发出去。为此，各个地址会参考一个发出接口列表。在这一点上MAC寻址与IP寻址一样。只不过MAC寻址所参考的这张表叫做地址转发表；而IP寻址中所参考的叫做路由控制表。 主机A先查看自己的路由控制表，在根据此表将发往主机B的数据先发给路由器1。 接收到该数据的交换机1则根据自己的地址转发表将数据转发给路由器1。 接收到该数据的路由器1根据自己的路由控制表将数据转发给路由器3。 接受到该数据的路由器3则根据自己的路由转发表将数据发给交换机3。 接受到该数据的交换机3再根据自己的地址转发表将数据发给主机B。 网络构成的要素搭建一套网络环境要涉及各种各样的电缆和网络设备，在此仅介绍连接计算机与计算机的硬件设备。 设备 应用 网卡 使计算机连网的设备（Network Interface） 中继器 从物理层上延长网络的设备 网桥/2层交换机 从数据链路层上延长网络的设备 路由器/3层交换机 通过网络层转发分组数据的设备 4-7层交换机 处理传输层以上各层网络传输的设备 网关 转换协议的设备 网卡任何一台计算机连接网络时，必须要使用网卡（全称为网络接口卡），有时也被称为网络适配器、网络接口卡、LAN卡。 中继器中继器是OSI模型的第一层——物理层面上延长网络的设备。由电缆传输过来的电信号或光信号经由中继器的波形调整和放大再传给另一个电缆。 中继器是对减弱的信号进行放大和发送的设备。 中继器通过物理层的连接延长网络。 即使在数据链路层出现某些错误，中继器仍然转发数据。 中继器无法改变传输速度。 一般情况下，中继器的两端连接的是相同的通信媒介，但有的中继器也可以完成不同媒介之间的转接工作，例如可以在同轴电缆与光轴电缆之间调整信号。然而，在这种情况下，中继器也只是单纯的负责信号在0和1比特流之间的替换，并不负责判断数据是否有错误，也不能在传输速度不同的媒介之间转发。 通过中继器而进行的网络延长，其距离也并非可以无限扩大，例如一个10Mbps的以太网最多可以用4个中继器分段连接，而一个100Mbps的以太网则最多只能连两个中继器。 网桥/2层交换机 网桥根据数据帧的内容转发数据给相邻的其他网络 网桥没有连接网段个数的限制 网桥基本上只用于连接相同类型的网络，但是有时也可以连接传输速率不同的网络 网桥是OSI模型的第2层——数据链路层面上连接两个网络的设备，它能识别数据链路中的数据帧，并将这些数据帧临时存储于内存，再重新生成信号作为一个全新的帧转发给相连的另一个网段。由于能够存储这些数据帧，网桥能够连接传输速率完全不同的数据链路，并且不限制连接网段的个数。 数据链路的数据帧中有一个数据为叫做FCS，用以校验数据是否正确送达目的地。网桥通过检查这个域中的值，将那些损坏的数据丢弃，从而避免发送给其他的网段，此外网桥还能通过地址自学机制和过滤功能控制网络流量。 有些网桥能够判断是否将数据报文转发给相邻的网段，这种网桥被称为自学式网桥，这类网桥会记住曾经自己转发的所有数据帧的MAC地址，并保存到自己的内存中，由此可以判断哪个网段中包含持有哪类MAC地址的设备。 路由器/3层交换机 路由器是连接网络与网络的设备 可以将分组报文发送给另一个目标路由器地址 基本上可以连接任意两个数据链路 路由器是在OSI模型的第3层——网络层面上连接两个网络，并对分组报文进行转发到设备。网桥是根据物理地址（MAC地址）进行处理，而路由器/3层交换机则是根据IP地址进行处理的。 路由器可以连接不同的数据链路，例如两个以太网，或者一个以太网与一个FDDI。路由器还有分担网络负荷的作用，甚至有些路由器具备一定的网络安全功能。 4-7层交换机4-7层交换机负责处理OSI模型中从传输层至应用层的数据。如果用TCP/IP分层模型来表述，4-7层交换机就是以TCP等协议的传输层及其上面的应用层为基础，分析收发数据，并对其进行特定的处理。 网关 网关负责协议的转换与数据的转发 在同一种类型的协议之间转发数据叫做应用网关 网关是OSI参考模型中负责将传输层到应用层的数据进行转换和转发的设备。它与4-7层交换机都是处理传输层及以上的数据，但是网关不仅转发数据还负责对数据进行转换，它通常会使用一个表示层或应用层网关，在两个不能进行直接通信的协议之间进行翻译，最终实现两者之间的通信。 为了控制网络流量以及出于安全的考虑，有时会使用代理服务器，这种代理服务器也是网关的一种。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[面向切面的Spring——AOP]]></title>
    <url>%2Fposts%2F5626%2F</url>
    <content type="text"><![CDATA[什么是面向切面编程切面能够帮助我们模块化横切关注点，横切关注点可以被描述为影响应用多处的功能，下图直观的呈现了横切关注点的概念。每个模块的核心功能都是为特定业务领域提供服务，但是如果要做到通用功能的话，最常见的面向对象技术十集成或委托，但是如果在整个应用程序中都使用相同的基类，继承往往会导致一个脆弱的对象体系；而使用委托可能需要对象进行复杂的调用。 切面提供了另一种可选方案，而且在很多场景下更清晰简洁。在使用面向切面编程时，我们仍然在一个地方定义通用功能，但是可以通过声明的方式定义这个功能要以何种方式在何处应用，而无需修改受影响的类。横切关注点可以被模块化为特殊的类，这些类被称为切面（aspect）。这样做的好处就是每个关注点都集中于一个地方，而不是分散到多处代码中，其次服务模块更加简洁，只需要关注核心代码就行了。 定义AOP术语 通知（Advice）:通知定义了切面是什么以及何时使用。除了描述切面要完成的工作，通知还解决了何时执行这个工作的问题。因此可以说，切面的工作被称为通知。Spring切面可以应用5种类型的通知： 前置通知（Before）：在目标方法被调用之前调用通知功能。 后置通知（After）：在目标方法完成之后调用通知，此时不会关心方法的输出。 返回通知（After-returning）：在目标方法成功执行之后调用通知。 异常通知（After-throwing）：在目标方阿飞抛出异常后调用通知。 环绕通知（Around）：通知包裹了被通知的方法，在被通知的方法调用之前和调用之后执行自定义的行为。 连接点（Join point）：我们的应用可能有数以千计的时机应用通知，这些时机被称为连接点，连接点是在应用程序中能够插入切面的一个点。 切点（Pointcut）：如果说通知定义了切面的“什么”和“何时”的话，那么切点就定义了“何处”，切点的定义会匹配通知所要织入的一个或多个连接点。 切面（Aspect）：切面是通知和切点的结合。通知和切点共同定义了切面的全部内容：它是什么，在何时和何处完成其功能。 引入（Introduction）：引入允许我们向现有的类添加新方法或属性。 织入（Weaving）：织入是把切面应用到目标对象并创建新的代理对象的过程。切面在指定的连接点被织入到目标对象中。在目标对象的生命周期里有多个点可以进行织入： 编译器：切面在目标类编译时被织入，AspectJ的织入编译器就是以这种方式织入切面的。 类加载期：切面在目标类加载到JVM时被织入。 运行期：切面在应用运行的某个时刻被织入。Spring AOP就是以这种方式织入切面的。 通过切点来选择连接点切点用于准确定位应该在什么地方应用切面的通知。通知和切点时切面的最基本元素。在Spring AOP中，要使用AspectJ的切点表达式语言来定义切点。下面是Spring AOP所支持的AspectJ切点指示器：在Spring中尝试使用AspectJ其它指示器时，将会抛出IllegalArgumentException异常。 编写切点为阐述Spring中的切面，我们需要有个主题来定义切面的切点，为此我们定义一个Performance接口： 1234package concert;public interface Performance &#123; public void perform();&#125; Performance可以代表任何类型的现场表演，假设我们想编写Performance的perform()方法触发的通知，下面展示了一个切点表达式，这个表达式能够设置当perform()方法执行时触发通知的调用：我们使用execution()指示器选择Preformance的perform（）方法。方法表达式以“*”号开始，表明我们不关心方法返回值的类型。然后我们指定了全限定类名和方法名。对于方法参数列表，我们使用两个点号（..）表明切点要选择任意的perform（）方法，无论该方法的入参是什么。假设我们需要配置的切点仅匹配concert包，可以使用within()指示器来限制匹配。使用“&amp;&amp;”来表示与的关系，“||”和“!”也同理： 在切点中选择beanSpring引入一个新的bean（）指示器，允许我们在切点表达式中使用bean的ID来标识bean。bean（）使用bean ID或bean名称作为参数来限制切点只匹配特定的bean：在这里我们希望在执行Performance的perform（）方法时应用通知，但限定bean的ID为woodstock。 在此场景下，切面的通知会被编织到所有ID不为woodstock的bean中。 使用注解创建切面我们已经定义了Performance接口，他们是切面中切点的目标对象，现在使用AspectJ注解来定义切面。 定义切面从演出的角度来看，观众是非常重要的，因此，将观众定义为一个切面，并将其应用到演出上： 1234567891011121314151617181920212223242526272829package concert;import org.aspectj.lang.annotation.AfterReturning;import org.aspectj.lang.annotation.AfterThrowing;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;@Aspectpublic class Audience &#123; @Before("execution(* concert.Performance.perform(..))") // 表演之前 public void silenceCellPhones() &#123; System.out.println("Silencing cell phones"); &#125; @Before("execution(* concert.Performance.perform(..))") // 表演之前 public void takeSeats() &#123; System.out.println("Taking seats"); &#125; @AfterReturning("execution(* concert.Performance.perform(..))") // 表演之后 public void applause() &#123; System.out.println("CLAP CLAP CLAP!!!"); &#125; @AfterThrowing("execution(* concert.Performance.perform(..))") // 表演失败之后 public void demandRefound() &#123; System.out.println("Demanding a refund"); &#125;&#125; Audience类使用@Aspect注解标注，该注解表明Audience不仅仅是一个POJO，还是一个切面。 Audience四个方法定义了一个观众在观看演出时可能会做的事情：手机调至静音、就坐、喝彩鼓掌以及退款。可以看到这些方法都使用通知注解来表明它们应该在什么时候调用，注解都有如下：你可能注意到所有的这些注解都给定了一个切点表达式作为它的值，同时这四个方法的切点表达式都是相同的。所以我们可以改进一下，如果我们只定义这个切点一次，然后每次需要的时候引用它，那么这会是一个很好的方案。@Poincut注解能够在一个@AspectJ切面内定义可重用的切点： 123456789101112131415161718192021222324252627282930313233package concert;import org.aspectj.lang.annotation.AfterReturning;import org.aspectj.lang.annotation.AfterThrowing;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.aspectj.lang.annotation.Pointcut;@Aspectpublic class Audience &#123; @Pointcut("execution(* concert.Performance.perform(..))") //定义命名的切点 public void performance()&#123;&#125; @Before("performance()") public void silenceCellPhones() &#123; System.out.println("Silencing cell phones"); &#125; @Before("performance()") // 表演之前 public void takeSeats() &#123; System.out.println("Taking seats"); &#125; @AfterReturning("performance()") // 表演之后 public void applause() &#123; System.out.println("CLAP CLAP CLAP!!!"); &#125; @AfterThrowing("performance()") // 表演失败之后 public void demandRefound() &#123; System.out.println("Demanding a refund"); &#125;&#125; 需要注意的是，除了注解和没有操作的performance（）方法，Audience类依然是一个独立的POJO，只不过它通过注解表明会作为切面使用，像其他类一样，它可以装配为Spring中的bean： 1234@Bean public Audience audience() &#123; return new Audience();&#125; 但是到此为止的话，@Audience只会是Spring容器中的一个bean，并不会被视为切面，需要将这些注解启用配置到配置文件中： 12345678910111213141516171819package concert;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.EnableAspectJAutoProxy;// 启动AspectJ自动代理@Configuration @EnableAspectJAutoProxy@ComponentScanpublic class ConcertConfig&#123; @Bean public Audience audience() &#123; return new Audience(); &#125;&#125; 当然使用XML文件配置也是完全可以的： 12345678910111213141516/ 在XML中，通过Spring的aop命名空间启用AspectJ自动代理&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:aop="http://www.springframework.org/schema/aop" // 声明Spring的aop命名空间 xsi:schemaLocation="http://www.springframework.org/schema/aop http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;context:component-scan base-package = "concert" /&gt; &lt;aop:aspectj-autoproxy /&gt; // 启动AspectJ自动代理 &lt;bean class= "concert.Audience" /&gt; // 声明Audience bean&lt;/beans&gt; 创建环绕通知环绕通知是最为强大的通知类型，为了介绍它，我们重写Audiences切面： 1234567891011121314151617181920212223242526// 使用环绕通知重新实现Audience切面package concert;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Pointcut;@Aspectpublic class Audience&#123; @Pointcut("execution(** concert.Performance.perform(..))") //定义命名的切点 public void performance()&#123;&#125; @Around("performance()") public void watchPerformance(ProceedingJoinPoint jp) &#123; try&#123; System.out.println("Silencing cell phones"); System.out.println("Taking seats"); jp.proceed(); System.out.println("CLAP CLAP CLAP!!!"); &#125; catch(Throwable e)&#123; System.out.println("Demanding a refund"); &#125; &#125;&#125; 可以看到，这个通知所达到的效果与之前的前置通知和后置通知是一样的，到那时现在它们处于同一个方法中。关于这个新的通知方法，你首先注意到的可能是它接受ProceedingJoinPoint作为参数，这个对象是必须要有的，因为你要在通知中通过它来调用被通知的方法。需要注意的是，别忘记调用proceed()方法，如果不调用这个方法，那么你的通知实际上会阻塞对被通知方法的调用。 处理通知中的参数之前的例子中都是没有参数的，但是如果切面所通知的方法有参数怎么办呢？有这样一个例子，play()方法会玄幻播放所有磁道并调用playTreck()方法，假设你想记录每个磁道被播放的次数，我们创建TrackCounter类： 12345678910111213141516171819202122232425262728293031// 使用参数化的通知来记录磁道播放的次数package soundsystem;import java.util.HashMap;import java.util.Map;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.aspectj.lang.annotation.Pointcut;@Aspectpublic class TrackCounter&#123; private Map&lt;Integer, Integer&gt; TrackCounts = new HashMap&lt;Integer, Integer&gt;(); // 通知playTrack()方法 @Pointcut( "execution(* soundsystem.CompactDisc.playTrack(int))" + "&amp;&amp; args(trackNumber)" ) public void trackPlayed(int trackNumber)&#123;&#125; @Before("trackPlayed(trackNumber)") // 在播放前，为该磁道计数 public void countTrack(int trackNumber) &#123; int currentCount = getPlayCount(trackNumber); trackCounts.put(trackNumber, currentCount + 1); &#125; public int getPlayCount(int trackNumber) &#123; return trackCounts.containsKey(trackNumber) ? trackCounts.get(trackNumber) : 0; &#125;&#125; 与之前不同的是，切点表达式中的args(trackNumber)限定符，它表明传递给playTrack方法的int类型参数也会传递到通知中去，参数的名称trackNumber也与切点方法签名中的参数相匹配。现在我们可以在Spring配置中将BlankDisc和TrackCounter定义为bean，并启用AspectJ自动代理： 123456789101112131415161718192021222324252627282930313233343536/ 配置TrackCount记录每个磁道播放的次数package soundsystem;import java.util.ArrayList;import java.List;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.EnableAspectJAutoProxy;@Configuration@EnableAspectJAutoProxy // 启用AspectJ自动代理public class TrackCounterConfig&#123; @Bean public CompactDisc sgtPeppers() // CompactDisc bean &#123; BlankDisc cd = new BlankDisc(); cd.setTitle("Sgt. Pepper's Lonely Hearts Club Band"); cd.setArtist("The Beales"); List&lt;String&gt; tracks = new ArrayList&lt;String&gt;(); tracks.add("Sgt. Pepper's Lonely Hearts Club Band"); tracks.add("With a Little Help from My Friends"); tracks.add("Lucy in the Sky with Diamonds"); tracks.add("Getting Better"); tracks.add("Fixing a Hole"); // ...other tracks omitted for brevity... cd.setTracks(tracks); return cd; &#125; @Bean public TrackCounter trackCounter() // TrackCounter bean &#123; return new TrackCounter(); &#125;&#125; 通过注解引入新功能静态语言的Java并不能像动态语言那样有开放类的理念，但是我们依旧可以使用切面为Spring bean添加新方法。在Spring中，切面只是实现了它们所包装bean相同接口的代理，如果除了实现这些接口，代理也能暴露新接口的话，会怎样呢？我们想办法使用AOP为示例中的所有Performance实现引入下面的Encoreable接口： 12345package concert;public interface Encoreable&#123; void performEncore();&#125; 为此我们需要创建一个新的切面： 1234567891011package concert;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.DeclareParents;@Aspectpublic class EncoreableIntroducer&#123; @DeclareParents(value = "concert.Performance+", defaultImpl = DefaultEncoreable.class) public static Encoreable encoreable;&#125; 可以看到EncoreableIntroducer是一个切面，但是他与我们之前的切面有所不同，使用了@DeclareParents注解，该注解由三部分组成： value：该属性制定了哪种类型的bean要引入该接口，在本例中即Performance类型。标记后面的加号表示是Performance的所有子类型，而不是Performance本身。 defaultImpl：该属性指定了为引入功能提供实现的类。 @DeclarationParents：该注解所标注的静态属性指明了要引入了接口。 和其他切面一样，我们需要在Spring应用中将EncoreableIntroducer声明为一个bean： 1&lt;bean class = "concert.EncoreableIntroducer" /&gt; 在XML中声明切面除了使用注解的方式，还可以考虑使用XML文件配置的方式。在Spring的aop命名空间中，提供了多个元素用来在XML中声明切面，如下表：在使用XML文件配置方式之前，先把之前在Audience类中的注解全部移除掉。 声明前置和后置通知接着我们在XML文件中配置将没有注解的Audience类转换为切面： 12345678910 &lt;aop:config&gt; &lt;aop:aspect ref="audience"&gt; &lt;aop:pointcut expression="execution(* XMLconcert.Performance.perform(..))" id="performance"/&gt; &lt;aop:before pointcut-ref="performance" method="silenceCellPhones"/&gt; &lt;aop:before pointcut-ref="performance" method="takeSeats"/&gt; &lt;aop:after-returning pointcut-ref="performance" method="applause"/&gt; &lt;aop:after-throwing pointcut-ref="performance" method="demandRefund"/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 声明环绕通知将之前的watchPerformance()方法及其类中的注解都移除： 12345678910111213141516package concert;import org.aspectj.lang.ProceedingJoinPoint;public class Audience &#123; public void watchPerformance(ProceedingJoinPoint jp) &#123; try &#123; System.out.println("手机静音"); System.out.println("得到座位"); jp.proceed(); System.out.println("鼓掌!!!"); &#125; catch (Throwable e) &#123; System.out.println("这演的啥啊！退票"); &#125; &#125;&#125; 然后在XML文件中配置： 1234567&lt;aop:config&gt; &lt;aop:aspect ref="audience"&gt; &lt;aop:pointcut id="performance" expression="execution(* concert.Performance.perform(..))"/&gt; &lt;aop:round pointcut-ref="preformance" method="watchPerformance"/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 为通知传递参数将TrackCounter类中的注解都移除掉，然后在XML文件中配置如下： 1234567&lt;aop:config&gt; &lt;aop:aspect ref="trackCounter"&gt; &lt;aop:pointcut id="trackPlayed" expression=" execution(* com.springinaction.disc.CompactDisc.playTrack(int)) and args(trackNumber)" /&gt; &lt;aop:before pointcut-ref="trackPlayed" method="countTrack"/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 通过切面引入新的功能依旧将原Encoreable的注解移除掉，然后配置为： 12345&lt;aop:aspect&gt; &lt;aop:declare-parents types-matching="com.springinaction.perf.Performance+" implement-interface="com.springinaction.perf.Encoreable" default-impl="com.springinaction.perf.DefaultEncoreable" /&gt;&lt;/aop:aspect&gt; 我们还可以使用delegate-ref属性来标识： 12345&lt;aop:aspect&gt; &lt;aop:declare-parents types-matching="com.springinaction.perf.Performance+" implement-interface="com.springinaction.perf.Encoreable" delegate-ref="defaultEncoreable" /&gt;&lt;/aop:aspect&gt; delegate-ref属性引入了一个Spring的bean作为引入的委托： 1&lt;bean id="defaultEncoreable" class="com.springinaction.perf.DefaultEncoreable" /&gt;]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[@Autowired和@Resource]]></title>
    <url>%2Fposts%2F13512%2F</url>
    <content type="text"><![CDATA[Spring的@Autowired注解版的自动绑定（@Autowired）在之前的学习中，为了减少配置量，我们可以采用Spring的IoC容器提供的自动绑定功能： 12345&lt;beans default-autowire="byType"&gt; &lt;bean id="newsProvider" class="..FXNewsProvider" autowire="byType"/&gt; &lt;bean id="djNewsListener" class="..DowJonesNewsListener"/&gt; &lt;bean id="djNewsPersister" class="..DowJonesNewsPersister"/&gt; &lt;/beans&gt; 可以通过default-autowired或者autowired来指定每个bean定义的自动绑定方式，Spring2.5之后提供了一个更加方便的方式：@Autowired注解，它可以让容器知道需要为当前类注入哪些依赖。例如： 1234567891011public class FXNewsProvider &#123; private IFXNewsListener newsListener; private IFXNewsPersister newPersistener; @Autowired public FXNewsProvider(IFXNewsListener newsListner,IFXNewsPersister newsPersister) &#123; this.newsListener = newsListner; this.newPersistener = newsPersister; &#125; ... &#125; 与原有的byType类型的自动绑定方式类似，@Autowired也是按照类型匹配进行依赖注入的,当按照类型找不到时，则会使用标注的属性名称，按byName的方式查找。但是@Autowired也更加灵活更加强大，它可以标注于类定义的多个位置。包括如下几个： 域或者说属性（Property）：不管它们声明的访问限制符是private还是public等等，只要标注@Autowired了，它们所需要的依赖注入需求就都能够被满足。 构造方法定义：标注于类的构造方法之上的@Autowired，相当于抢夺了原有自动绑定功能中“constructor”方式的权力，它将更具构造方法参数类型，来决定将什么样的依赖对象注入给当前对象，就如上面的代码演示一样。 方法定义：@Autowired不光可以标注于传统的setter方法之上，而且还可以标注于任意名称的方法定义之上，只要该方法定义了需要被注入的参数。 接下来只需要在容器的配置文件中定义bean就行了： 12345&lt;beans&gt; &lt;bean id="newsProvider" class="..FXNewsProvider"/&gt; &lt;bean id="djNewsListener" class="..DowJonesNewsListener"/&gt; &lt;bean id="djNewsPersister" class="..DowJonesNewsPersister"/&gt; &lt; /beans&gt; 为了给容器中定义的每个bean对应的实例注入依赖，可以遍历它们，然后通过反射，检查每个bean定义对应的类上各种可能位置上的@Autowired。如果存在的话，就可以从当前容器管理的对象中获取符合条件的对象，设置给@Autowired所标注的对象。 那么如果仅仅只是一个注解，在Spring容器中并不能起作用，联想到之前的BeanPostProcessor自定义实现，让这个BeanPostProcessor在实例化bean定义的过程中，来检查当前对象是否有@Autowired标注的依赖需要注入，AutowiredAnnotationBeanPostProcessor就是Spring提供用于这一目的的BeanPostProcesser实现，所以只需要在配置文件中注册一下它就可以了： 1234567&lt;beans&gt; &lt;bean class="org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor"/&gt; &lt;bean id="newsProvider" class="..FXNewsProvider"/&gt; &lt;bean id="djNewsListener" class="..DowJonesNewsListener"/&gt; &lt;bean id="djNewsPersister" class="..DowJonesNewsPersister"/&gt; &lt; /beans&gt; @Qualifier的陪伴@Autowired是按照类型进行匹配，如果当前@Autowired标注的依赖在容器中只能找到一个实例与之对应的话还好，可以是若能够同时找到多个同一类型的实例对象又该怎么办？这时候就可以使用@Qualifier对依赖注入的条件做进一步限定。 @Qualifier实际上是byName自动绑定的注解版，既然IoC容器无法自己从多个同一类型的实例中选取我们真正想要的那个，那么我们就可以使用@Qualifier直接点名需要哪个。假设FXNewsProvider使用的IFNewsListener有两个实现类DowJonesNewsListener和ReutersNewsListener： 12345678&lt;beans&gt; &lt;bean class="org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor"/&gt; &lt;bean id="newsProvider" class="..FXNewsProvider"/&gt; &lt;bean id="djNewsListener" class="..DowJonesNewsListener"/&gt; &lt;bean id="reutersNewsListner" class="..ReutersNewsListener"/&gt; &lt;bean id="djNewsPersister" class="..DowJonesNewsPersister"/&gt; &lt;/beans&gt; 如果我们想让FXNewsProvider使用ReutersNewsListener，那么就可以使用@Qulifier指定选择： 12345678910public class FXNewsProvider &#123; @Autowired @Qualifier("reutersNewsListner") private IFXNewsListener newsListener; @Autowired private IFXNewsPersister newPersistener; ... &#125; JavaEE的@Resource除了使用Spring提供的@Autowired和@Qulifier之外，还可以使用JavaEE提供的@Resource。@Resource与@Autowired不同，它遵循的是byName自动绑定的行为准则，也就是说，IoC容器将更具@Resource所指定的名称，到容器中查找beanName与之对应的实例，然后将查找到的对象实例注入给@Resource所标注的对象。如下： 12345678public class FXNewsProvider &#123; @Resource(name="djNewsListener") private IFXNewsListener newsListener; @Resource(name="djNewsPersister") private IFXNewsPersister newPersistener; ... &#125; 除了标注于属性之上外，还可以与@Autowired一样标注于方法或者构造方法之上，此处与@Autowired大致一样。 @PostConstruct和@PreDestroy确切的说，@PostConstruct和@PreDestroy不是服务于依赖注入的，他们主要用于标注对象生命周期管理相关方法，这与Spring的InitializingBean和DisposableBean接口，以及配置项中的init-method和destroy-method起到类似作用。 如果想某个方法在对象实例化之后被调用，以做某些准备工作，或者想在对象销毁之前调用某个方法做清理工作，可以像如下这样： 1234567891011public class LifecycleEnabledClass &#123; @PostConstruct public void setUp() &#123; ... &#125; @PreDestroy public void destroy() &#123; ... &#125; &#125; annotation-config就像@AutoWired需要AutowiredAnnotationBeanPostProcessor为它与IoC容器牵线搭桥一样，JavaEE同样有这样的配置方法，可以帮助这些注解发挥它们的作用,那就是annotation-config，如下： 123456789101112131415&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:p="http://www.springframework.org/schema/p" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd"&gt; &lt;context:annotation-config/&gt; &lt;bean id="newsProvider" class="..FXNewsProvider"/&gt; &lt;!--其他bean定义--&gt; ... &lt;/beans&gt; &lt;context:annotation-config/&gt;不但帮我们把AutowiredAnnotationBeanPostProcessor和 CommonAnnotationBeanPostProcessor注册到容器，还把PersistenceAnnotationBeanPostProcessor和RequiredAnnotationBeanPostProcessor一并注册，可谓一举四得。 classpath-scanning介绍到现在为止，我们还是需要将相应对象的bean定义，一个个的添加到IoC容器的配置文件中，唯一与之前的区别就是，不用在配置文件中明确指定依赖关系了，那么既然注解做都做了，为了不做添加bean的注解呢》Spring的classpath-scanning就是因此而生的。 使用相应的注解对组成应用程序的相关类进行标注后，classpath-scanning功能可以从某一顶层包开始扫描，当扫描到某个类标注了相应的注解之后，就会提取该类的相关信息，构建对应的BeanDefinition，然后把构建完成的BeanDefinition注册到容器，这之后的事情就不用多说了。 classpath-scanning功能的触发是由&lt;context:component-scan&gt;决定的，只需要配置如下代码即可生效： 12345678910&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd"&gt; &lt;context:component-scan base-package="org.spring21"/&gt; &lt; /beans&gt; 现在&lt;context:component-scan&gt;将扫描org.spring21路径下的所有类型定义，寻找标注了相应注解的类，并添加到IoC容器。 &lt;context:component-scan&gt;默认扫描的注解类型是@Component，不过在@Component语义基础上细化后的@Repository、@Service和@Controller也同样可以获得&lt;context:component-scan&gt;的青睐。 &lt;context:component-scan&gt;在扫描相关类定义并将它们添加到容器的时候，会使用一种默认的命名规则，来生成那些添加到容器的bean的名称（beanName）。比如DowJonesNewsPersister通过默认命名规则将获得dowJonesNewsPersister。如果想改变这种命名规则，就可以在@Component中制定一个自定义的名称。 当在配置文件中添加&lt;context:component-scan&gt;之后，&lt;context:component-scan&gt;将多管闲事，同时会将AutowiredAnnotationBeanPostProcessor和CommonAnnotationBeanPostProcessor一并注入到容器中。 &lt;context:component-scan&gt;还会进一步定制扫描范围，默认情况下他只关系@Component、@Repository、@Service和@Controller四位大员。但是我们可以通过include-filtering和exclude-filtering定制扫描范围，例子如下： 12345678910111213&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/ schema/context/spring-context-2.5.xsd"&gt; &lt;context:component-scan base-package="org.spring21"&gt; &lt;context:include-filter type="annotation" expression="cn.spring21.annotation.FXService"/&gt; &lt;context:exclude-filter type="aspectj" expression=".."/&gt; &lt;/context:component-scan&gt; &lt; /beans&gt; include-filtering和exclude-filtering可以使用的type类型由annotation、assignable、regex和aspectj四种。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[IoC容器之ApplicationContext]]></title>
    <url>%2Fposts%2F30205%2F</url>
    <content type="text"><![CDATA[统一资源加载策略ApplicationContext既然要加载配置文件，就需要一个工具可以用来加载这些资源，因此而出现了ResourceLoader。ApplicationContext实则间接继承了ResourceLoader接口，可以用来对Spring内的资源做一个统一的加载。 在使用中最常用的两个加载器就是ClassPathXmlApplicationContext和FileSystemXmlApplicationContext。简单的使用代码如下： 1234ResourceLoader resourceLoader = new ClassPathXmlApplicationContext("配置文件路径"); // 或者 // ResourceLoader resourceLoader = new FileSystemXmlApplicationContext("配置文件路径"); Resource fileResource = resourceLoader.getResource("D:/spring21site/README"); ResourceLoader中增加了一种新的资源路径协议：classpath：和classpath：，classpath：与classpath：的唯一区别在于，如果能够在classpath中找到多个指定的资源，则返回多个。 ClassPathXmlApplicationContext和FileSystemXmlApplicationContext在处理资源在记得默认行为上有所不用。当ClassPathXmlApplicationContext在实例化的时候，即使没有执行classpath：或者classpath*：等前缀，它会默认从classpath中加载bean定义配置文件。而FileSystemXmlApplicationContext会默认从文件系统中加载bean定义文件。 其他特性ApplicationContext还提供了一些其他特性，这里就简单介绍一下，不过多描述。 国际化信息支持：ApplicationContext有两种国际化支持的方式，一个是Locale结合ResourceBundle，它可以绑定地区。另外一个就是MessageSource可以统一国际化信息的访问方式。 容器内部事件发布：JavaSE提供了实现自定义时间发布功能的基础类，即EventListener和EventObject。 多配置模块加载的简化：在配置应用的时候，为了更好的划分逻辑层次和分工，而延伸到个让容器同时读入划分到不同配置文件的信息。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[IoC容器之BeanFactory]]></title>
    <url>%2Fposts%2F37871%2F</url>
    <content type="text"><![CDATA[Spring提供了两种容器类型：BeanFactory和ApplicationContext。 BeanFactory：基础类型的Ioc容器，提供完整的IoC服务支持。如果没有特殊指定，默认采用延迟初始化策略。即只有当客户端对象需要访问容器中的某个受管对象时，才对该受管对象进行初始化以及依赖注入操作。所以相对来说，该容器启动初期速度较快，所需要的资源有限，因此BeanFactroy比较适合用于资源有限，并且功能要求不是很严格的场景。 ApplicationContext：ApplicationContext是在BeanFactory的基础上构建的，是相对于高级的容器实现，因此ApplicationContext还提供了一些例如事件发布、国际化信息支持的高级特性。ApplicationContext所管理的对象，在该容器启动之后，默认全部初始化并绑定完成。所以相对BeanFactory来说，ApplicationContext要求更多的系统资源，同时启动时间也会长一些。因此ApplicationContext更适合于资源充足并且要求更多功能的场景中。 通过下图可以对BeanFactory和ApplicationContext之间的关系有一个更清晰的认知。 BeanFactory，顾名思义就是生产Bean的工厂，而Spring提倡使用POJO，所以可以把每个业务对象看作一个JavaBean对象。BeanFactory可以完成作为IoC Service Provider的所有职责，包括业务对象的注册和对象间依赖关系的绑定。 BeanFactory就像一个汽车生产厂。你从其他汽车零件厂商或者自己的零件生产部门取得汽车零件送入这个汽车生产厂，最后只需要从生产线的终点取得汽车成品就可以了。相似的，将应用程序所需的所有业务对象交给BeanFactory之后，剩下的就是直接从BeanFactory取得最终组装完成并且可用的对象。至于这个最终业务对象如何组装，你不需要担心，BeanFactory会帮你搞定。 拥有BeanFactory之后的生活确切的说，拥有BeanFactory之后的生活没有太大的变化，有变化的也在只是一些拉拉扯扯的事情，客观一点就是对象之间依赖关系的解决方式改变了。之前系统业务对象需要自己去“拉”所依赖的业务对象，有了BeanFactory之类的IoC之后，需要依赖什么，让BeanFactory为我们推过来就行啦。以一个FX新闻系统的例子来说，FX新闻应用设计和实现框架代码如下： 123456789101112131-设计FXNewsProvider类用于普遍的新闻处理 public class FXNewsProvider &#123; ... &#125; 2-设计IFXNewsListener接口抽象各个新闻社不同的新闻获取方式，并给出相应实现类 public interface IFXNewsListener &#123; ... &#125; 以及 public class DowJonesNewsListener implements IFXNewsListener&#123; ... &#125; 3-设计IFXNewsPersister接口抽象不同数据访问方式，并实现相应的实现类 public interface IFXNewsPersister &#123; ... &#125; 以及 public class DowJonesNewsPersister implements IFXNewsPersister &#123; ... &#125; BeanFactory会说，这些都让我来干吧！此时BeanFactory说这些事情让他来做，但是它并不知道该怎么做，所以就需要你来教它怎么做。通常情况下，会使用XML文件配置的方式来教它，也就是告诉BeanFactory如何来注册并管理各个业务对象之间的依赖关系。下面是BeanFactory的XML配置方式实现业务对象间的依赖关系的一个例子代码： 1234567891011&lt;beans&gt; &lt;bean id="djNewsProvider" class="..FXNewsProvider"&gt; &lt;constructor-arg index="0"&gt; &lt;ref bean="djNewsListener"/&gt; &lt;/constructor-arg&gt; &lt;constructor-arg index="1"&gt; &lt;ref bean="djNewsPersister"/&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; ... &lt;/beans&gt; 在BeanFactory出现之前，我们通常会直接在应用程序的入口类的main方法中，自己动手实例化相应的对象并调用，如以下代码： 12FXNewsProvider newsProvider = new FXNewsProvider(); newsProvider.getAndPersistNews(); 不过现在有了BeanFactory，通过xml文件这张“图纸”告诉BeanFactory怎么做之后，让BeanFactory为我们生产一个FXNewsProvider，如以下代码所示： 12345678910111213BeanFactory container = new XmlBeanFactory(new ClassPathResource("配置文件路径")); FXNewsProvider newsProvider = (FXNewsProvider)container.getBean("djNewsProvider"); newsProvider.getAndPersistNews();/*或者如以下代码所示*/ ApplicationContext container = new ClassPathXmlApplicationContext("配置文件路径"); FXNewsProvider newsProvider = (FXNewsProvider)container.getBean("djNewsProvider"); newsProvider.getAndPersistNews(); /*也可以如以下代码所示*/ApplicationContext container = new FileSystemXmlApplicationContext("配置文件路径"); FXNewsProvider newsProvider = (FXNewsProvider)container.getBean("djNewsProvider"); newsProvider.getAndPersistNews(); BeanFactory的对象注册与依赖绑定方式为了让BeanFactory能够明确管理各个业务对象以及业务对象之间的依赖绑定关系，同样需要需要某种途径来记录和管理这些信息。与上一章的IoC Service Provider提到的三种方式，BeanFactory几乎支持所有这些方式。 直接编码方式先看一下使用直接编码方式FX新闻系统相关类是如何注册并绑定的： 123456789101112131415161718192021222324252627282930public static void main(String[] args) &#123; DefaultListableBeanFactory beanRegistry = new DefaultListableBeanFactory(); BeanFactory container = (BeanFactory)bindViaCode(beanRegistry); FXNewsProvider newsProvider = (FXNewsProvider)container.getBean("djNewsProvider"); newsProvider.getAndPersistNews(); &#125; public static BeanFactory bindViaCode(BeanDefinitionRegistry registry) &#123; AbstractBeanDefinition newsProvider = new RootBeanDefinition(FXNewsProvider.class,true); AbstractBeanDefinition newsListener = new RootBeanDefinition(DowJonesNewsListener.class,true); AbstractBeanDefinition newsPersister = new RootBeanDefinition(DowJonesNewsPersister.class,true); // 将bean定义注册到容器中 registry.registerBeanDefinition("djNewsProvider", newsProvider); registry.registerBeanDefinition("djListener", newsListener); registry.registerBeanDefinition("djPersister", newsPersister); // 指定依赖关系 // 1. 可以通过构造方法注入方式 ConstructorArgumentValues argValues = new ConstructorArgumentValues(); argValues.addIndexedArgumentValue(0, newsListener); argValues.addIndexedArgumentValue(1, newsPersister); newsProvider.setConstructorArgumentValues(argValues); // 2. 或者通过setter方法注入方式 MutablePropertyValues propertyValues = new MutablePropertyValues(); propertyValues.addPropertyValue(new ropertyValue("newsListener",newsListener)); propertyValues.addPropertyValue(new PropertyValue("newPersistener",newsPersister)); newsProvider.setPropertyValues(propertyValues); // 绑定完成 return (BeanFactory)registry; &#125; BeanFactory只是一个接口，我们最终需要一个该接口的实现来进行实际的Bean管理，DefaultListableBeanFactory就是这么一个比较通用的BeanFactory实现类。具体关系可以看下图：BeanDefaultListableBeanFactory除了间接的实现了BeanFactory接口，还实现了BeanDefinitionRegistry接口，该接口才是现在BeanFactory的实现中担当Bean注册管理的角色。打个比方说，BeanDefinitionRegistry就像图书馆的数加，所有的书是放在书架上的，虽然你还书借书都是跟图书馆（也就是BeanFactory）打交道，但书架才是图书馆存放各类图书的地方。 基本上BeanFactory接口只定义如何访问容器内管理Bean的方法，各个BeanFactory的实现类才负责具体Bean的注册以及管理工作。BeanDefinitionRegistry接口定义抽象了Bean的注册逻辑。通常情况下，具体的BeanFactory实现类会实现这个接口来管理Bean的注册。 每一个受管的对象，在容器中都会有一个BeanDefinition的实例与之相对应，该BeanDefinition的实例负责保存对象的所有必要信息，包括其对应的对象的class类型、是否是抽象类、构造方法参数以及其他属性等。当客户端向BeanFactory请求相对应对象的时候，BeanFactory会通过这些信息为客户端返回一个完备可用的对象实例。 所以现在可以回到上面的代码中了，上面代码大致可以分为三个步骤： 在main方法中，首先构造一个DefaultListableBeanFactory作为BeanDefinitionRegistry，然后将其交给bindViaCode方式进行具体的对象注册和相关逻辑管理，然后就可以通过该方法返回的BeanFactory取得需要的对象。 在bindViaCode方法中，首先针对相应的业务对象构造与其相对应的BeanDefinition，使用了RootBeanDefinition作为BeanDefinition的实现类。构造完成后，将这些BeanDefinite注册到通过方法参数传进来的BeanDefinitionRegistry中。 之后我们可以通过构造方法，或者setter方法，为其注入相关依赖。最后以BeanFactory的形式返回已经注册并绑定了所有相关业务对象的BeanDefinitionRegistry实例。 外部配置文件方式Spring的IoC容器支持两种配置文件格式：Properties文件格式和XML文件格式。 采用外部配置文件时，Spring的IoC荣有一个统一的处理方式。通常情况下，需根据不同的外部配置文件格式，给出相应的BeanDefinitionReader实现类，由BeanDefinitionReader的相应实现类负责将相应的配置文件的内容读取并映射到BeanDefinition，然后将映射后的BeanDefinition注册到一个BeanDefinitionRegistry，之后BeanDefinitionRegistry即完成Bean的注册和加载。 Properties配置格式的加载Spring提供了PropertiesBeanDefiniteReader类用于Properties格式配置文件的加载，只需要根据该类的读取规则，提供相应的配置文件即可，示例代码如下： 1234567891011djNewsProvider.(class)=..FXNewsProvider djListener.(class)=..impl.DowJonesNewsListener djPersister.(class)=..impl.DowJonesNewsPersister # ----------通过构造方法注入的时候------------- djNewsProvider.$0(ref)=djListener djNewsProvider.$1(ref)=djPersister # ----------通过setter方法注入的时候--------- djNewsProvider.newsListener(ref)=djListener djNewsProvider.newPersistener(ref)=djPersister 这些内容都是特定于Spring的PropertiesBeanDefinitionReader的，下面是简单的语法介绍： djNewsProvider作为beanName，后面通过（class）表明对应的实现类是什么，实际上使用djNewsProvider.class=…的形式也是可以的，但是不提倡。 通过在表示beanName的名称后添加.$[number]后缀的形式，来表示当前beanName对应的对象需要通过构造方法注入的方式注入相应依赖对象。$0和1$1后面的（ref）用来表示所依赖的是引用对象，而不是普通的类型。如果不加（ref），则会将djListener和djPersister作为简单的String类型进行注入，异常也是自然不可避免了。 setter方法注入与构造方法注入最大的区别就是，它不使用数字顺序来指定注入的位置，而使用相应的属性名称来指定注入，同样也不要忘掉（ref）。 在使用Properties文件配置完后，就可以使用了，下面是一个使用演示： 123456789101112public static void main(String[] args) &#123; DefaultListableBeanFactory beanRegistry = new DefaultListableBeanFactory(); BeanFactory container = (BeanFactory)bindViaPropertiesFile(beanRegistry); FXNewsProvider newsProvider = (FXNewsProvider)container.getBean("djNewsProvider"); newsProvider.getAndPersistNews(); &#125;public static BeanFactory bindViaPropertiesFile(BeanDefinitionRegistry registry) &#123; PropertiesBeanDefinitionReader reader = new PropertiesBeanDefinitionReader(registry); reader.loadBeanDefinitions("classpath:../../binding-config.properties"); return (BeanFactory)registry; &#125; XML配置格式的加载XML配置格式是Spring支持最完整，功能最强大，也是使用最频繁的表达方式。例子如下： 12345678910111213141516171819&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;!DOCTYPE beans PUBLIC "-//SPRING//DTD BEAN//EN" "http://www.springframework.org/dtd/spring-beans.dtd"&gt; &lt;beans&gt; &lt;bean id="djNewsProvider" class="..FXNewsProvider"&gt; &lt;constructor-arg index="0"&gt; &lt;ref bean="djNewsListener"/&gt; &lt;/constructor-arg&gt; &lt;constructor-arg index="1"&gt; &lt;ref bean="djNewsPersister"/&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean id="djNewsListener" class="..impl.DowJonesNewsListener"&gt; &lt;/bean&gt; &lt;bean id="djNewsPersister" class="..impl.DowJonesNewsPersister"&gt; &lt;/bean&gt; &lt;/beans&gt; 相对应的加载XML配置文件的BeanFactory的使用演示如下： 1234567891011121314public static void main(String[] args) &#123; DefaultListableBeanFactory beanRegistry = new DefaultListableBeanFactory(); BeanFactory container = (BeanFactory)bindViaXMLFile(beanRegistry); FXNewsProvider newsProvider = (FXNewsProvider)container.getBean("djNewsProvider"); newsProvider.getAndPersistNews(); &#125;public static BeanFactory bindViaXMLFile(BeanDefinitionRegistry registry) &#123; XmlBeanDefinitionReader reader = new XmlBeanDefinitionReader(registry); reader.loadBeanDefinitions("classpath:../news-config.xml"); return (BeanFactory)registry; // 或者直接 //return new XmlBeanFactory(new ClassPathResource("../news-config.xml")); &#125; 与Properties一样，XML同样有Spring提供的现成的BeanDefinitionReader实现，即XmlBeanDefinitionReader。 注解方式注解是Java5之后才引入的，所以相对来说也更加简洁方便一些，使用注解的方式为FXNewsProvider注入所需要的依赖，现在可以使用@Autowired以及@Component对相关类进行标记。下面是FXNews使用注解标注后的情况。 12345678910111213141516171819@Component public class FXNewsProvider &#123; @Autowired private IFXNewsListener newsListener; @Autowired private IFXNewsPersister newPersistener; public FXNewsProvider(IFXNewsListener newsListner,IFXNewsPersister newsPersister) &#123; this.newsListener = newsListner; this.newPersistener = newsPersister; &#125; ... &#125; @Component public class DowJonesNewsListener implements IFXNewsListener &#123; ... &#125; @Component public class DowJonesNewsPersister implements IFXNewsPersister &#123; ... &#125; @Autowired是这里的主角，它的存在将告知Spring容器需要为当前对象注入哪些依赖对象。而@Component则是配合Spring中的classpath-scanning功能使用。现在我们只要再向Spring的配置文件中增加一个“触发器”，使用@Autowired和@Component标注的类就能获得依赖对象的注入了。下面是在配置文件中配置的方法： 123456789101112131415&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:tx="http://www.springframework.org/schema/tx" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.5.xsd"&gt; &lt;context:component-scan base-package="cn.spring21.project.base.package"/&gt; &lt;/beans&gt; context:component-scan会到指定的包下面扫描有@Component的类，如果找到，则将他们添加到容器进行管理，并根据它们所标注的@Autowired为这些类注入符合条件的依赖对象。所以在做完上面的工作之后，就可以使用了： 12345public static void main(String[] args) &#123; ApplicationContext ctx = new ClassPathXmlApplicationContext("配置文件路径"); FXNewsProvider newsProvider = (FXNewsProvider)container.getBean("FXNewsProvider"); newsProvider.getAndPersistNews(); &#125; XML配置方法XML格式的容器信息管理方式是Spring提供的最为强大、支持最为全面的方式，因此在这里讲述一下XML文件的以标签和一些属性。 &lt;beans&gt;和&lt;bean&gt;所有的Spring容器加载的XML配置文件的头部，都需要基于文档声明，而DTD和XSD的声明方式不一样，下面是一个对比： 123456789101112131415161718192021222324252627&lt;!-- DTD方式 --&gt;&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;!DOCTYPE beans PUBLIC "-//SPRING//DTD BEAN//EN" ➥ "http://www.springframework.org/dtd/spring-beans.dtd"&gt; &lt;beans&gt; ... &lt;/beans&gt; &lt;!-- XSD方式 --&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:util="http://www.springframework.org/schema/util" xmlns:jee="http://www.springframework.org/schema/jee" xmlns:lang="http://www.springframework.org/schema/lang" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-2.0.xsd http://www.springframework.org/schema/jee http://www.springframework.org/schema/jee/spring-jee-2.0.xsd http://www.springframework.org/schema/lang http://www.springframework.org/schema/lang/spring-lang-2.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.0.xsd"&gt; &lt;/beans&gt; 所有注册到容器的业务对象，在Spring中称之为Bean。所以，每一个对象在XML的映射中也自然而然的对应一个&lt;bean&gt;。既然容器最终可以管理所有的业务对象，那么XML中把这些叫做&lt;bean&gt;的元素组织起来，就叫做&lt;beans&gt;。 &lt;beans&gt;之唯我独尊&lt;beans&gt;是XML配置我呢见中最顶层的元素，它下面可以包含0或者1个&lt;description&gt;和多个&lt;bean&gt;以及&lt;import&gt;或者&lt;alias&gt;。&lt;beans&gt;作为所有的“统帅”，它拥有相应的属性对所辖的&lt;bean&gt;进行统一的默认行为设置，包括如下几个： default-lazy-init:其值可以指定为true或者false，默认值为false。用来标志是否对所有的&lt;bean&gt;进行延迟初始化，但是指定为true时并不一定会延迟初始化，如果某个非延迟的bean依赖于该bean，那么该bean就不会延迟初始化。 default-autowire：可以取值为no-不采取任何形式的自动绑定，完全依赖于手工明确配置、byName-与XML文件中声明的bean定义的beanName的值进行匹配、byType-会根据当前bean定义类型，按类型匹配、constructor-同样是按类型绑定，但是是匹配构造放的参数类型，而不是实例属性的类型，以及autodetect-byTye和constructor的结合体，如果是无参构造方法则是由byType，否则使用constructor模式。默认为no。如果使用自动绑定的话，用来标志全体bean使用哪一种默认绑定方式。 default-dependency-check：可以取值为none-不检查、objects-只对对象引用类型依赖进行检查、simple-对简单属性类型以及相关的collection进行依赖检查，对象引用类型的依赖除外，以及all-simple和objects的结合，默认为none，即不做检查。 default-init-method：如果所管辖的所有&lt;bean&gt;都有同样名称的初始化方法，可以在这里统一指定这个初始化方法名，而不用每一个单独指定。 default-destroy-method：与default-init-method相对应，如果所管辖的所有&lt;bean&gt;都有同样名称的销毁方法，可以在这里统一指定这个销毁方法名，而不用每一个单独指定。 &lt;description&gt;、&lt;import&gt;和&lt;alias&gt;这几个元素通常情况下不是必须的，这里只是了解一下。 &lt;description&gt;：在配置文件中指定一些描述性的信息。 &lt;import&gt;：可以在主要的配置文件中通过这个标签元素对其所依赖的配置文件引用，例如在A文件中&lt;import resource=”B.xml”&gt; &lt;alias&gt;：为某些&lt;bean&gt;起一些别名，通常情况下是为了减少输入。 孤孤单单的id和classid属性通常，每个注册到容器的对象都需要一个唯一标志来将其与同处一室的bean兄弟们区分开来，通过id属性来指定当前注册对象的beanName是什么。实际上并非任何情况下都需要指定每个&lt;bean&gt;的id，有些情况下可以省略，会在后面提到。 除了使用id来指定&lt;bean&gt;在容器中的标志，还可以使用name属性来指定&lt;bean&gt;的别名。name比id灵活之处在于，name可以使用id不能使用的一些字符，比如、。而且还可以通过逗号、空格或者冒号分割指定多个name。name的作用和alias的作用基本相同。 class属性每个注册到容器的对象都需要通过&lt;bean&gt;元素的class属性指定其类型。在大部分情况下该属性都是必须的，仅在少数情况下不需要指定，后面会提到。 XML中的继承1234567891011121314151617&lt;bean id="newsProviderTemplate" abstract="true"&gt; &lt;property name="newPersistener"&gt; &lt;ref bean="djNewsPersister"/&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id="superNewsProvider" parent="newsProviderTemplate" class="..FXNewsProvider"&gt; &lt;property name="newsListener"&gt; &lt;ref bean="djNewsListener"/&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id="subNewsProvider" parent="newsProviderTemplate" class="..SpecificFXNewsProvider"&gt; &lt;property name="newsListener"&gt; &lt;ref bean="specificNewsListener"/&gt; &lt;/property&gt; &lt; /bean&gt; 根据上面的代码可以看出，我们在声明superNewsProvider和subNewsProvider的时候，使用了parent属性，将其值指定为newsProviderTemplate，这样我们就继承了newsProviderTemplate定义的默认值，只需要将指定的属性进行更改，而不要全部又重新定义一遍。 parent属性还可以与abstract属性结合使用，达到将相应bean定义模板化的目的。newsProviderTemplate的bean定义通过abstract属性声明为true，说明这个bean定义不需要实例化，这就是之前提到的可以不指定class属性的少数场景之一。该bean只是配置一个模板，不对应任何对象，superNewsProvider和subNewsProvider通过parent指向这个模板定义，就拥有了该模板定义的所有属性配置。 容器在初始化对象实例的时候，不会关注abstract的bean。如果你不想容器在初始化对象实例的时候，那么可以将其abstract属性赋值为true，以避免容器将其实例化。同样对于ApplicationContext也是如此。 bean的scope（作用域）scope用来声明容器中的对象所应该处的限定场景或者说该对象的存活时间，即容器在对象进入相应的scope之前，生产并装配这些对象，在该对象不在处于这些scope的限定之后，容器通常会销毁这些对象。 Spring最初提供了两种bean的scope类型：singleton和prototype，但是2.0发布之后，又引入了另外三种scope类型，即request、session和globa session类型，对应这三种类型只能在Web应用中使用。 singleton标记为拥有singleton scope的对象定义，在Spring的IoC容器中只存在一个实例，所有对该对象的引用将共享这个实例，也就是说它与IoC容器的寿命“几乎”相同。 不要将这里的singleton与设计模式中的singleton相混淆。二者的语义是不同的：标记为singleton的bean是由容器来保证这种类型的bean在同一个容器中只存在一个共享实例；而Singleton模式则是保证在同一个Classloader中只存在一个这种类型的实例。 可以从两个方面看singleton的bean所具有的特性： 对象实例数量：singleton的bean定义，在一个容器中只存在一个共享实例，所有对该类型bean的依赖都引用这一单一实例。 对象存活时间：singleton类型bean定义，从容器启动，到它第一次被请求而实例化开始，只要容器不销毁或者退出，该类型bean的单一实例就会一直存活。 通常情况下，如果你不指定bean的scope，singleton便是容器默认的scope。 prototype针对声明为拥有prototype scope的bean定义，容器在接到该类型对象的请求的时候，会每次都重新生成一个新的对象实例给请求方。虽然这种类型的对象的实例化以及属性设置等工作都是由容器负责的，但是只要准备完毕，并且对象实例返回给请求方之后，容器就不再拥有当前返回对象的引用，请求方需要自己负责当前返回对象的后继生命周期的管理工具，包括销毁。 所以对于那些请求不能共享使用的对象类型，应该将其bean定义的scope设置为prototype，这样可以用它来保存一些用户的状态信息。 requestSpring容器，即XmlWebApplicationContext会为每一个HTTP请求创建一个全新的RequestProcessor对象供当前请求使用，当请求结束后，该对象实例的生命周期即告结束。 sessionSpring容器会为每个独立的session创建属于它们自己的全新的UserPreferences对象实例。与request相比，除了可能有更长的存活时间，其它方面真是没什么区别。 global sessionglobal session只应用于基于porlet的Web应用程序中才有意义，在普通的基于servlet的Web应用中使用了这个类型的scope，容器会将其作为普通的session类型对待。 自定义scope默认的singleton和prototype是硬编码到代码中的，而request、session和globa session，包括自定义scope类型，都实现了Scope接口，该接口定义如下： 123456789public interface Scope &#123; Object get(String name, ObjectFactory objectFactory); Object remove(String name); void registerDestructionCallback(String name, Runnable callback); String getConversationId(); &#125; 要实现自己的scope类型，首先要给出一个Scope接口的实现类，并非4个方法都是必须的，但get和remove方法必须实现。下面是一个例子： 12345678910111213141516171819202122232425262728public class ThreadScope implements Scope &#123; private final ThreadLocal threadScope = new ThreadLocal() &#123; protected Object initialValue() &#123; return new HashMap(); &#125; &#125;; public Object get(String name, ObjectFactory objectFactory) &#123; Map scope = (Map) threadScope.get(); Object object = scope.get(name); if(object==null) &#123; object = objectFactory.getObject(); scope.put(name, object); &#125; return object; &#125; public Object remove(String name) &#123; Map scope = (Map) threadScope.get(); return scope.remove(name); &#125; public void registerDestructionCallback(String name, Runnable callback) &#123; &#125; ... &#125; 接下来就是把这个新定义的scope注册到容器中，才能供相应的bean使用。我们有两种方式可以注册。 一种就是我们可以使用ConfigurableBeanFactory的以下方法去注册：void registerScope(String scopeName, Scope scope)，其中参数scopeName就是使用的bean定义可以指定的名称，参数scope就是我们提供的scope实现类实例。注册例子如下： 12Scope threadScope = new ThreadScope(); beanFactory.registerScope("thread",threadScope); 另一种方式就是可以在xml文件中配置CustomScopeConfigurer来注册scope。例如“ 1234567&lt;bean class="org.springframework.beans.factory.config.CustomScopeConfigurer"&gt; &lt;property name="scopes"&gt; &lt;map&gt; &lt;entry key="thread" value="com.foo.ThreadScope"/&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt; 接下来就可以使用这个自定义的scope了，例如： 123&lt;bean id="beanName" class="..." scope="thread"&gt; &lt;aop:scoped-proxy/&gt; &lt;/bean&gt; 工厂方法与FactoryBean工厂方法（Factory Method）模式，提供一个工厂类来实例化具体的接口实现类，这样主体对象只需要依赖工厂类，具体使用的实现类有变更的话，只是变更工厂类，而主体对象不需要做任何变动，这样就避免了接口与实现类的耦合性。 针对这种方式，Spring的IoC容器同样提供了对应的集成支持，我们要做的，只是将工厂类所返回的具体的接口实现类注入给主体对象。 静态工厂方法假设现在有一个接口BarInterface，为了向使用该接口的客户端对象屏蔽以后可能对BarInterface实现类的变动，因此还提供了一个静态的工厂方法实现类StaticBarInterfaceFactory，代码如下： 12345public class StaticBarInterfaceFactory &#123; public static BarInterface getInstance() &#123; return new BarInterfaceImpl(); &#125; &#125; 为了将该静态方法类返回的实现注入Foo，我们使用以下方式进行配置： 1234567&lt;bean id="foo" class="...Foo"&gt; &lt;property name="barInterface"&gt; &lt;ref bean="bar"/&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id="bar" class="...StaticBarInterfaceFactory" factory-method="getInstance"/&gt; class指定静态方法工厂类，factory-method指定工厂方法名称，然后，容器调用该静态方法工厂类的指定工厂方法，并返回方法调用后的结果，即BarInterfaceImpl的实例。 某些时候，有的工厂类的工厂方法需要参数来返回相应实例，可以通过&lt;constructor-arg&gt;来指定工厂方法需要的参数： 12345public class StaticBarInterfaceFactory &#123; public static BarInterface getInstance(Foobar foobar) &#123; return new BarInterfaceImpl(foobar); &#125; &#125; 12345678910111213&lt;bean id="foo" class="...Foo"&gt; &lt;property name="barInterface"&gt; &lt;ref bean="bar"/&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id="bar" class="...StaticBarInterfaceFactory" factory-method="getInstance"&gt; &lt;constructor-arg&gt; &lt;ref bean="foobar"/&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean id="foobar" class="...FooBar"/&gt; 唯一需要注意的是，针对静态工厂方法实现类的bean定义，使用&lt;constructor-arg&gt;传入的是工厂方法的参数，而不是静态工厂方法的构造方法的参数，况且，静态工厂方法实现类也没有提供显式的构造方法。 非静态工厂方法针对非静态方法的调用方式也很简单，只是需要稍微该变一下代码即可： 123456public class NonStaticBarInterfaceFactory &#123; public BarInterface getInstance() &#123; return new BarInterfaceImpl(); &#125; ... &#125; 123456789&lt;bean id="foo" class="...Foo"&gt; &lt;property name="barInterface"&gt; &lt;ref bean="bar"/&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id="barFactory" class="...NonStaticBarInterfaceFactory"/&gt; &lt; bean id="bar" factory-bean="barFactory" factory-method="getInstance"/&gt; 现在的最主要不同就在于是使用factory-bean属性来指定工厂方法所在的工厂类实例，而不是通过class属性来指定工厂方法所在类的类型。如果需要参数的话，配置方法同静态工厂方法一样。 FactoryBean当某些对象的实例化过程因为繁琐，使用XML配置过于复杂时，就可以考虑通过代码的方式来完成这个实例化过程，FactoryBean接口就是为此而生的。FactoryBean接口只定义了三个方法，如下： 12345public interface FactoryBean &#123; Object getObject() throws Exception; Class getObjectType(); boolean isSingleton(); &#125; getObject()方法会返回该FactoryBean生产的对象实例，我们需要实现该方法以给出自己的对象实例化逻辑； getObjectType()方法仅返回getObject()方法所返回的对象的类型，如果预先无法确定，则返回null； isSingleton()方法返回结果用于表明是否为单例实例。 这是一个例子，得到第二天的日期： 1234567891011121314public class NextDayDateFactoryBean implements FactoryBean &#123; public Object getObject() throws Exception &#123; return new DateTime().plusDays(1); &#125; public Class getObjectType() &#123; return DateTime.class; &#125; public boolean isSingleton() &#123; return false; &#125; &#125; 接下来只需要将其注册到容器中即可： 1234567&lt;bean id="nextDayDateDisplayer" class="...NextDayDateDisplayer"&gt; &lt;property name="dateOfNextDay"&gt; &lt;ref bean="nextDayDate"/&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id="nextDayDate" class="...NextDayDateFactoryBean"&gt; &lt;/bean&gt; 接下来就是最重要的地方，来看看NextDayDateDisplayer的定义： 12345public class NextDayDateDisplayer &#123; private DateTime dateOfNextDay; // 相应的setter方法 // ... &#125; NextDayDateDisplayer所声明的依赖dateOfNextDay的类型为DateTime，而不是NextDayDateFactoryBean，也就是说FactoryBean类型的bean定义，通过正常id引用，容器返回的时FactoryBean所生产的对象类型，而非其本身。当然如果一定要取得FactoryBean本身的话，可以通过bean定义的id之前加前缀“&amp;”来达到目的。 方法注入以及方法替换在引出标题内容之前，需要先提一下有关bean的scope的prototype属性的陷阱，我们知道，拥有prototype类型scope的bean，在请求方法每次向容器请求该类型对象的时候，容器都会返回一个全新的该对象实例，下面看看这个情况： 123456789101112131415public class MockNewsPersister implements IFXNewsPersister &#123; private FXNewsBean newsBean; public void persistNews(FXNewsBean bean) &#123; persistNewes(); &#125; public void persistNews() &#123; System.out.println("persist bean:"+getNewsBean()); &#125; public FXNewsBean getNewsBean() &#123; return newsBean; &#125; public void setNewsBean(FXNewsBean newsBean) &#123; this.newsBean = newsBean; &#125; &#125; 配置为： 12345678&lt;bean id="newsBean" class="..domain.FXNewsBean" singleton="false"&gt; &lt;/bean&gt; &lt;bean id="mockPersister" class="..impl.MockNewsPersister"&gt; &lt;property name="newsBean"&gt; &lt;ref bean="newsBean"/&gt; &lt;/property&gt; &lt;/bean&gt; 当我们多次调用MockNewsPersister的persistNews时，看看结果： 1234567BeanFactory container = new XmlBeanFactory(new ClassPathResource("..")); MockNewsPersister persister = (MockNewsPersister)container.getBean("mockPersister"); persister.persistNews(); persister.persistNews(); 输出： persist bean:..domain.FXNewsBean@1662dc8 persist bean:..domain.FXNewsBean@1662dc8 从输出看对象实例是相同的，而这与我们的初衷时相悖的。问题实际上不出在FXNewsBean的scope类型是否是prototype的，而是出在实例的取得方式上面。虽然FXNewsBean拥有prototype类型的scope，但当容器将一个FXNewsBean的实例注入MockNewsPersister之后，MockNewsPersister就会一直持有这个FXNewsBean实例的引用。虽然每次输出都调用了getNewBean()方法并返回一个FXNewsBean的实例，但实际上每次返回的都是MockNewsPersister持有的容器第一次注入的实例。这就是问题之所在。 换句话说，第一个实例注入后，MockNewsPersister再也没有重新想容器申请新的实例，所以容器也不会重新为其注入新的FXNewsBean类型的实例。 下面就是介绍解决这个问题的方法了。 方法注入若要使用方法注入的方式，该方法必须能够被子类实现或者覆写，因为容器会为我们要进行方法注入的对象使用Cglib动态生成一个子类实现，从而代替当前对象，既然我们的getNewsBean()方法已经满足以上方法声明要求，就只需要正确配置该类了： 123456&lt;bean id="newsBean" class="..domain.FXNewsBean" singleton="false"&gt; &lt;/bean&gt; &lt;bean id="mockPersister" class="..impl.MockNewsPersister"&gt; &lt;lookup-method name="getNewsBean" bean="newsBean"/&gt; &lt;/bean&gt; 通过&lt;lookup-method&gt;的name属性指定需要注入的方法名，bean属性指定需要注入的对象，当getNewsBean方法被调用的时候，容器可以每次返回一个新的FXNewsBean类型的实例。所以这个时候再次检查执行结果，输出的实例引用就是不同的了。 殊途同归除了使用方法注入来达到“每次调用都让容器返回新的对象实例”的目的，还可以使用其他方式达到相同的目的： 使用BeanFactoryAware：我们知道，即使没有方法注入，只要在实现getNewsBean()方法的时候，能够保证每次调用BeanFactory的getBean(“newsBean”)，就同样可以每次都取得新的FXNewsBean对象实例。BeanFactoryAware接口就可以帮我们完成这一步，其定义如下：123public interface BeanFactoryAware &#123; void setBeanFactory(BeanFactory beanFactory) throws BeansException; &#125; 实现BeanFactoryAware接口和配置的代码如下： 12345678910111213141516public class MockNewsPersister implements IFXNewsPersister,BeanFactoryAware &#123; private BeanFactory beanFactory; public void setBeanFactory(BeanFactory bf) throws BeansException &#123; this.beanFactory = bf; &#125; public void persistNews(FXNewsBean bean) &#123; persistNews(); &#125; public void persistNews() &#123; System.out.println("persist bean:"+getNewsBean()); &#125; public FXNewsBean getNewsBean() &#123; return beanFactory.getBean("newsBean"); &#125; &#125; 12345&lt;bean id="newsBean" class="..domain.FXNewsBean" singleton="false"&gt; &lt;/bean&gt; &lt;bean id="mockPersister" class="..impl.MockNewsPersister"&gt; &lt;/bean&gt; 使用ObjectFatoryCreatingFatoryBean：实际上ObjectFatoryCreatingFatoryBean实现了BeanFactoryAware接口，它返回的ObjectFactory实例这是特定于与Spring容器进行交互的一个实现而已。使用它的好处就是，隔离了客户端对象对BeanFactory的直接引用。12345678910111213141516public class MockNewsPersister implements IFXNewsPersister &#123; private ObjectFactory newsBeanFactory; public void persistNews(FXNewsBean bean) &#123; persistNews(); &#125; public void persistNews() &#123; System.out.println("persist bean:"+getNewsBean()); &#125; public FXNewsBean getNewsBean() &#123; return newsBeanFactory.getObject(); &#125; public void setNewsBeanFactory(ObjectFactory newsBeanFactory) &#123; this.newsBeanFactory = newsBeanFactory; &#125; &#125; 1234567891011121314&lt;bean id="newsBean" class="..domain.FXNewsBean" singleton="false"&gt; &lt;/bean&gt; &lt;bean id="newsBeanFactory" class="org.springframework.beans.factory.config.ObjectFactoryCreatingFactoryBean"&gt; &lt;property name="targetBeanName"&gt; &lt;idref bean="newsBean"/&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id="mockPersister" class="..impl.MockNewsPersister"&gt; &lt;property name="newsBeanFactory"&gt; &lt;ref bean="newsBeanFactory"/&gt; &lt;/property&gt; &lt;/bean&gt; 方法替换基本上可以认为，方法替换可以帮助我们实现简单的方法拦截功能。假设某天我看FXNewsProvider不爽，想替换掉它的getAndPersistNews方法默认逻辑，这是我就可以用方法替换将它的原有逻辑给替换掉。 替换的方法就是实现一个叫做MethodReplacer接口，简单的实现例子如下： 12345678910public class FXNewsProviderMethodReplacer implements MethodReplacer &#123; private static final transient Log logger = LogFactory.getLog(FXNewsProviderMethodReplacer.class); public Object reimplement(Object target, Method method, Object[] args) throws Throwable &#123; logger.info("before executing method["+method.getName()+ "] on Object["+target.getClass().getName()+"]."); System.out.println("sorry,We will do nothing this time."); logger.info("end of executing method["+method.getName()+ "] on Object["+target.getClass().getName()+"]."); return null; &#125; &#125; 接下来把它配置到xml文件中就可以了： 12345678910111213&lt;bean id="djNewsProvider" class="..FXNewsProvider"&gt; &lt;constructor-arg index="0"&gt; &lt;ref bean="djNewsListener"/&gt; &lt;/constructor-arg&gt; &lt;constructor-arg index="1"&gt; &lt;ref bean="djNewsPersister"/&gt; &lt;/constructor-arg&gt; &lt;replaced-method name="getAndPersistNews" replacer="providerReplacer"&gt; &lt;/replaced-method&gt; &lt;/bean&gt; &lt;bean id="providerReplacer" class="..FXNewsProviderMethodReplacer"&gt; &lt;/bean&gt; 容器加载过程先看一张IoC容器的工作框图： 两大阶段IoC容器所起的作用就如上图所展示的那样，他会以某种方式加载Configuration MetaData（通常为XML配置文件），然后根据这些信息绑定整个系统对象，最终组装成一个可用的基于轻量级容器的应用系统。 Spring的IoC容器实现以上功能的过程，基本上分为两个阶段：容器启动阶段和Bean的实例化阶段。 容器启动阶段容器启动伊始，首先会通过某种途径加载Configuration MetaData。除了代码方式比较直接，大部分情况下都会依赖于工具类BeanDefinitionReader对加载的配置文件进行解析和分析，并将分析后的信息编组为相应的BeanDefinition，最后把这些保存了bean定义必要信息的BeanDefinition，注册到相应的BeanDefinitionRegistry，这样启动阶段就完成了。 Bean实例化阶段该阶段，容器会首先检查所请求的对象之前是否已经实例化。如果没有，则会根据注册的BeanDefinition所提供的信息实例化被请求对象，并为其注入依赖。如果该对象实现了某些回调接口，也会根据回调接口的要求来装配它。当该对象装配完毕后，容器会立即将其返回请求方使用。 插手两大阶段Spring提供了一种叫做BeanFactoryPostProcessor的容器扩展机制，该机制允许我们在容器实例化相应对象之前，对注册到容器的BeanDefinition所保存的信息做相应的修改。这就相当于在容器第一阶段结束之后，第二阶段开始之前，加入一道工序，让我们对最终的BeanDefinition做一些额外的操作，比如修改其中bean定义的某些属性，为bean定义增加其他信息等。 如果自定义实现BeanFactoryPostProcessor，通常实现接口BeanFactoryPostProcessor就可以了，如果有多个自定义实现的类，则实现Order接口来规定它们的顺序。 但是Spring为我们提供了三种常用的已经实现好的BeanFactoryPostProcessor实现类，它们分别是 PropertyPlaceholderConfigurer、PropertyOverrideConfigurer和CustomEditorConfigure。在介绍这三种实现类之前，先需要了解它们的配置方法是什么，很简单，只需要在XML配置文件中加入以下代码即可： 123456789101112... &lt;beans&gt; &lt;bean class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"&gt; &lt;property name="locations"&gt; &lt;list&gt; &lt;value&gt;conf/jdbc.properties&lt;/value&gt; &lt;value&gt;conf/mail.properties&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; ... &lt;/beans&gt; PropertyPlaceholderConfigurer通常情况下，对于MySQL数据库或者Redis数据库之类的配置，为了不将这些系统管理信息相关的信息与业务对象相关的信息混杂到XML文件中，会单独把这些信息配置到一个.properties文件中。PropertyPlaceholderConfigurer允许我们在XML配置文件中使用占位符，并将这些占位符所代表的资源单独配置到见简单的properties文件中来加载，例如最常见的： 123456789101112131415161718&lt;bean id="dataSource" class="org.apache.commons.dbcp.BasicDataSource" destroy-method="close"&gt; &lt;property name="url"&gt; &lt;value&gt;$&#123;jdbc.url&#125;&lt;/value&gt; &lt;/property&gt; &lt;property name="driverClassName"&gt; &lt;value&gt;$&#123;jdbc.driver&#125;&lt;/value&gt; &lt;/property&gt; &lt;property name="username"&gt; &lt;value&gt;$&#123;jdbc.username&#125;&lt;/value&gt; &lt;/property&gt; &lt;property name="password"&gt; &lt;value&gt;$&#123;jdbc.password&#125;&lt;/value&gt; &lt;/property&gt; &lt;property name="maxActive"&gt; &lt;value&gt;100&lt;/value&gt; &lt;/property&gt; ...&lt;/bean&gt; 对应的properties文件内容则为： 1234jdbc.url=jdbc:mysql://server/MAIN?useUnicode=true&amp;characterEncoding=ms932&amp;failOverReadOnly=false&amp;roundRobinLoadBalance=true jdbc.driver=com.mysql.jdbc.Driver jdbc.username=your usernamejdbc.password=your password 基本就如上面所说的那样，当BeanFactory完成第一阶段加载完成所有配置信息之后，BeanFactory中保存的对象的属性信息还只是以占位符的形式存在，如”${jdbc.url}”。当PropertyPlaceholderConfigurer被应用时，他会使用properties配置文件中的配置信息来替换相应的BeanDefintion中占位符所表示的属性值。PropertyPlaceholderConfigurer不单会从其配置的properties文件中加载配置项，同时还会检查Java的System类中的Properties。 PropertyOverrideConfigurerPropertyOverrideConfigurer的作用就是，通过它可以对容器中篇日志的任何你想处理的bean定义的property信息进行覆盖替换。比如之前的dataSource定义中，maxActive的值为100，如果我们觉得不合适，那么可以通过PropertyOverrideConfigurer将其覆盖为200：dataSource.maxActive=200。 如果要对容器中的某些bean定义的property信息进行覆盖，我们需要按照如下规则提供一个PropertyOverrideConfigurer使用的配置文件：beanName.propertyName=value。然后通过以下方法将PropertyOverrideConfigurer注册到容器即可： 123&lt;bean class="org.springframework.beans.factory.config.PropertyOverrideConfigurer"&gt; &lt;property name="location" value="pool-adjustment.properties"/&gt; &lt; /bean&gt; 当容器中配置的多个PropertyOverrideConfigurer对同一个bean定义的同一个property值进行处理的时候，最后一个会生效。 CustomEditorConfigure因为我们配置XML文件都是String类型，即容器从XML格式的文件中读取的都是字符串形式，最终应用程序却是由各种类型的对象所构成的。要想完成这种由字符串到具体对象的转换，都需要这种转换规则相关的信息，而CustomEditorConfigure就是帮我们传达类似信息的。 bean的生命周期接下来就是第二大阶段了，即bean实例化阶段的实现逻辑。这里还需要再提一次BeanFactory和ApplicationContext对于bean加载的区别： BeanFactory对象实例化默认采用延迟初始化。 ApplicationContext启动之后会实例化所有的bean定义。 当调用getBean()方法时，内部发现该bean定义之前还没有被实例化之后，会通过createBean()方法来进行具体的对象实例化，实例化过程如图所示：下面就来详细看看其中的步骤。 Bean的实例化与BeanWrapper容器在内部实现的时候，采用“策略模式”来决定采用何种方式初始化bean实例。通常，可以通过反射或者CGLIB动态字节码生成来初始化相应的bean实例或者动态生成其子类。 第一步容器只要根据相应的bean定义的BeanDefintion取得实例化信息，结合CglibSubclassingInstantiationStrategy通过反射的方式，以及不同的bean定义类型，就可以返回实例化完成的对象实例。但是，返回的不是构造完成的对象实例，而是以BeanWrapper对构造完成的对象实例进行包括，返回相应的BeanWrapper实例。 第二步第一步结束后返回BeanWrapper实例而不是原先的对象实例，就是为了第二步“设置对象属性”。使用BeanWrapper对bean实例操作很方便，可以免去直接使用Java反射API操作对象实力的反锁，看一段代码就知道Spring容器内部时如何设置对象属性的了： 1234567Object provider = Class.forName("package.name.FXNewsProvider").newInstance(); Object listener = Class.forName("package.name.DowJonesNewsListener").newInstance(); Object persister = Class.forName("package.name.DowJonesNewsPersister").newInstance(); BeanWrapper newsProvider = new BeanWrapperImpl(provider); newsProvider.setPropertyValue("newsListener", listener); newsProvider.setPropertyValue("newPersistener", persister); 各色的Aware接口当对象实例化完成后并且相关属性以及依赖设置完成之后，Spring容器会检查当前对象实例是否实现了一系列的以Aware命名结尾的接口定义，如果是，则将这些Aware接口定义中规定的依赖注入给当前对象实例。下面介绍几个重要的Aware接口： ApplicationContextAware: 获得ApplicationContext对象,可以用来获取所有Bean definition的名字。 BeanFactoryAware:获得BeanFactory对象，可以用来检测Bean的作用域。 BeanNameAware:获得Bean在配置文件中定义的名字。 ResourceLoaderAware:获得ResourceLoader对象，可以获得classpath中某个文件。 ServletContextAware:在一个MVC应用中可以获取ServletContext对象，可以读取context中的参数。 ServletConfigAware： 在一个MVC应用中可以获取ServletConfig对象，可以读取config中的参数。 BeanPostProcessorBeanPostProcessor会处理容器内所有符合条件的实例化后的对象实例，该接口声明了两个方法，分别在不同的时机执行： 1234public interface BeanPostProcessor &#123; Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException; &#125; postProcessBeforeInitialization()方法是上图中BeanPostProcessor前置处理这一步将会执行的方法，postProcessAfterInitialization()则是对应图中BeanPostProcessor后置处理那一步将会执行的方法。BeanPostProcessor的两个方法中都传入了原来的对象实例的引用，这位我们操作扩展容器的对象实例化过程中的行为提供了极大的便利。 InitializingBean和init-methodInitializingBean是容器内部广泛使用的一个对象生命周期表示接口，其定义如下： 123public interface InitializingBean &#123; void afterPropertiesSet() throws Exception; &#125; 其作用在于，在对象实例化过程调用过“BeanPostProcessor”的前置处理之后，会接着检测当前对象是否实现了InitializingBean接口，如果是，则回到调用其afterPropertiesSet()方法进一步调整对象实例的状态。 通过init-method，系统中业务对象的自定义初始化操作可以以任何方式命名。而不再受制于InitializingBean的afterPropertiesSet()。在自己定义init方法后，只需要在XML文件中简单配置一下就好了： 1234567891011&lt;beans&gt; &lt;bean id="tradeDateCalculator" class="FXTradeDateCalculator" init-method="setupHolidays" &gt; &lt;constructor-arg&gt; &lt;ref bean="sqlMapClientTemplate"/&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean id="sqlMapClientTemplate" class="org.springframework.orm.ibatis.SqlMapClientTemplate"&gt; ... &lt;/bean&gt; ... &lt; /beans&gt; 这样FXTradeDateCalculator类中的setupHolidays方法就是会在该阶段执行的方法了。 DisposableBean与destroy-method当所有的一切，该设置的设置，该注入的注入，该调用的调用之后，容器将检查singleton类型的bean实例，看其是否实现了DisposableBean接口。或者其对应的bean定义是否通过&lt;bean&gt;的destroy-method属性制定了自定义的对象销毁方法，如果是，就会为该实例注册一个用于对象销毁的回调，以及在这些singleton类型的对象实例销毁之前，执行销毁逻辑。 不过这些自定义的对象销毁逻辑，在对象实例初始化完成并注册了相关的回调方法之后，并不会马上执行。回调方法注册后，在使用完成后，只有在执行回调方法之后，才会执行销毁操作。 对于BeanFactory注册回调方法的方式为： 1((ConfigurableListableBeanFactory)container).destroySingletons(); 对于ApplicationContext注册回调方法的方式为： 1((AbstractApplicationContext)container).registerShutdownHook(); 至此为止，bean就算走完了它在容器中“光荣”的一生。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[IoC容器和IoC Service Provider]]></title>
    <url>%2Fposts%2F10510%2F</url>
    <content type="text"><![CDATA[IoC的基本概念Ioc的作用简单说就是创建对象由以前的程序员自己new构造方法来调用，变成了交由Spring创建对象。举个例子，出门之前得先穿件外套吧？以前，你得自己跑到衣柜面前取出衣服这一依赖对象，然后自己穿上再出门。而现在，你只要跟你的“另一半”使个眼色或者一句话，她就会心领神会地到衣柜那里为你取出衣服，然后再给你穿上，现在你就可以出门了。 通常情况下，被注入对象会直接依赖于被依赖对象。但是，在IoC的场景中，二者之间通过IoC Service Provider（例子中你的另一半）来打交道，素有的被注入对象和依赖对象现在由IoC Service Provider统一管理。被注入对象需要什么，直接跟IoC Service Provider招呼一声，后者就会把相应的被依赖对象注入到被注入对象中。 依赖注入的方式IoC有三种依赖注入的方式，即构造方法注入、setter方法注入以及接口注入。 构造方法注入构造方法注入，就是被注入对象可以通过在其构造方法中声明依赖对象的参数列表，让外部（通常是Ioc容器）知道它需要哪些依赖对象，例子如下： 1234public FXNewsProvider(IFXNewsListener newsListner,IFXNewsPersister newsPersister&#123; this.newsListener = newsListner; this.newPersistener = newsPersister; &#125; IoC Service Provider会检查被注入对象的构造方法，取得它所需要的依赖对象列表，进而为其注入相应的对象。同一个对象是不可能被构造两次的，因此，被注入对象的构造乃至整个生命周期，应该是由IoC Service Provider来管理的。 构造方法注入方式比较直观，对象被构造完，即进入就绪状态，可以马上使用。 setter方法注入对应Java Bean对象来说，通过setter方法，可以更改相应的对象属性；通过getter方法，可以获得相应属性的状态。所以当前对象只要为其依赖对象所对应的属性添加setter方法，就可以通过setter方法将相对应的依赖对象设置到被注入对象中。。例子如下： 123456789101112131415161718public class FXNewsProvider &#123; private IFXNewsListener newsListener; private IFXNewsPersister newPersistener; public IFXNewsListener getNewsListener() &#123; return newsListener; &#125; public void setNewsListener(IFXNewsListener newsListener) &#123; this.newsListener = newsListener; &#125; public IFXNewsPersister getNewPersistener() &#123; return newPersistener; &#125; public void setNewPersistener(IFXNewsPersister newPersistener) &#123; this.newPersistener = newPersistener; &#125; &#125; 这样外界就可以通过调用setNewsListener和setNewPersistener方法为FXNewsProvider对象注入所依赖的对象了。 setter方法注入相对更宽松一些，可以在对象构造完成后在诸如。 接口注入相对于前两种方式来说，接口注入就没有那么简单明了了。被注入对象如果想要IoC Service Provider为其注入依赖对象，就必须实现某个接口。这个接口提供一个方法，用来为其注入依赖对象。 如上图，FXNewsProvider为了让IoC Service Provider为其注入所依赖的IFXNewsListener，首先需要实现IFXNewsListenerCallable接口，这个接口会声明一个injectNewsListner方法（方法名随意），该方法的参数，就是所依赖对象的类型。这样，InjectionServiceContainer对象，即对应IoC Service Provider就可以通过这个接口方法将依赖对象注入到被注入对象FXNewsProvider中。 相对于前两种依赖注入方式，接口注入比较死板和繁琐。如果需要注入依赖对象，被注入对象就必须声明和实现另外的接口。 三种注入方式比较 接口注入：从使用上来说，接口注入是现在不提倡的一种方式，因为它强制被注入对象实现不必要的接口，带有侵入性。而构造方法注入和setter方法注入则不需要如此。 构造方法注入：这种注入方法的优点就是，对象在构造完后，即进入就绪状态，可以马上使用。**缺点就是，当依赖对象比较多的时候，构造方法的参数列表会比较长。而通过反射构造对象的时候，对相同类型的参数的处理会比较困难，维护和使用上也比较麻烦。而且在Java方法中，构造方法无法被继承，无法设置默认值。对于非必须的依赖处理，可能需要引入多个构造方法，而参数列表的变动可能造成维护上的不便。** setter方法注入：因为方法可以命名，所以setter方法注入在描述上要比构造方法注入好一些。另外setter方法可以被继承，允许设置默认值，而且有良好IDE支持。缺点就是对象无法再构造完成马上进入就绪状态。 IoC Service Provider简介Ioc Service Provider是一个抽象出来的概念，它可以指代任何将IoC场景中的业务对象绑定到一起的实现方式。它可以是一段代码，也可以是一组相关的类，甚至可以是比较通用的IoC框架或者IoC容器。 Ioc Service Provider的职责IoC Service Provider的职责相对来说比较简单，主要有两个：业务对象的构建管理和业务对象间的依赖绑定。 业务对象的构建管理：在IoC场景中，业务对象无需关心所依赖的对象如何构建如何取得，但这部分工作始终需要有人来做，所以IoC Service Provider需要将对象的构建逻辑从客户端对象那里剥离出来，以免这部分逻辑污染业务对象的实现。 业务对象间的绑定：Ioc Service Provider通过结合之前构建和管理的所有业务对象，以及各个业务对象间可以识别的依赖关系，将这些对象所依赖的对象注入绑定，从而保证每个业务对象在使用的时候，可以处于就绪状态。 IoC Service Provider如何管理对象间的依赖关系介绍归纳当前流行的IoC Service Provider产品使用的注册对象管理信息的方式主要有以下几种。 直接编码方式当前大部分的IoC容器都应该支持直接编码方式，包括Spring。在容器启动之前，我们就可以通过程序编码的方式将被注入对象和依赖对象注册到容器中，并明确它们相互之间的依赖注入关系。下面代码演示了这样的一个过程： 1234567IoContainer container = ...;container.register(FXNewsProvider.class,new FXNewsProvider());container.register(IFXNewsListener.class,new DowJonesNewsListener());... FXNewsProvider newsProvider = (FXNewsProvider)container.get(FXNewsProvider.class); newProvider.getAndPersistNews(); 通过为相应的类指定对应的具体实例，可以告知IoC容器，当我们要这种类型的对象实例时，请将容器中注册的、对应的那个具体实例返回给我们。 如果是接口注入方式，可能伪代码要多一些，但是本质的道理上是一样的。 12345678910IoContainer container = ...;container.register(FXNewsProvider.class,new FXNewsProvider()); container.register(IFXNewsListener.class,new DowJonesNewsListener());... container.bind(IFXNewsListenerCallable.class, container.get(IFXNewsListener.class));... FXNewsProvider newsProvider = (FXNewsProvider)container.get(FXNewsProvider.class); newProvider.getAndPersistNews(); 通过bind方法将“被注入对象”所依赖的对象，绑定为容器中注册过的IFXNewsListener类型的对象实例。 配置文件方式这是一种较为普遍的依赖注入关系管理方式，像properties文件、XML文件等，都可以成为管理依赖注入关系的载体，其中最为常见的就是XML文件的方式。代码例子如下： 123456789101112&lt;bean id="newsProvider" class="..FXNewsProvider"&gt; &lt;property name="newsListener"&gt; &lt;ref bean="djNewsListener"/&gt; &lt;/property&gt; &lt;property name="newPersistener"&gt; &lt;ref bean="djNewsPersister"/&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id="djNewsListener" class="..impl.DowJonesNewsListener"&gt; &lt;/bean&gt; &lt;bean id="djNewsPersister" class="..impl.DowJonesNewsPersister"&gt; &lt;/bean&gt; 最后，在代码中通过“newProvider”这个名字，即可从容器中取得已经组装好的FXNewsProvider并直接使用。 1234... container.readConfigurationFiles(...); FXNewsProvider newsProvider = (FXNewsProvider)container.getBean("newsProvider"); newsProvider.getAndPersistNews(); 元数据方式这种方式使用Google Guice实现的一种注解方式，代码如下： 123456789101112public class FXNewsProvider &#123; private IFXNewsListener newsListener; private IFXNewsPersister newPersistener; @Inject 5 public FXNewsProvider(IFXNewsListener listener,IFXNewsPersister persister)&#123; this.newsListener = listener; this.newPersistener = persister; &#125; ... &#125; 通过@Inject，我们指明需要IoC Service Provider通过构造方法注入方式，为FXNewsProvider注入其所依赖的对象。至于余下的依赖信息，在Guice中是由相应的Module来提供的。下面是FXNewsProvider所使用的Module实现： 123456789public class NewsBindingModule extends AbstractModule &#123; @Override protected void configure() &#123; bind(IFXNewsListener.class) ➥ .to(DowJonesNewsListener.class).in(Scopes.SINGLETON); bind(IFXNewsPersister.class) ➥ .to(DowJonesNewsPersister.class).in(Scopes.SINGLETON); &#125; &#125; 通过Module指定进一步的依赖注入相关信息之后，就可以直接从Guice那里取得最终已经注入完毕，并且直接可用的对象了。 123Injector injector = Guice.createInjector(new NewsBindingModule()); FXNewsProvider newsProvider = injector.getInstance(FXNewsProvider.class); newsProvider.getAndPersistNews();]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[查询性能优化]]></title>
    <url>%2Fposts%2F39035%2F</url>
    <content type="text"><![CDATA[为什么查询速度会慢在尝试编写快速的查询之前，需要清楚的一点就是，真正重要的是响应时间。如果把查询看作是一个任务，那么它由一系列子任务组成，每个字任务都会消耗一定的时间。如果要优化查询，实际上要优化其子任务，要么消除其中一些子任务，要么减少子任务的执行次数，要么让子任务运行的更快。 通常来说，查询的生命周期大致可按照顺序来看：从客户端，到服务器，然后在服务器上进行解析，生成执行计划，执行，并返回结果给客户端。其中“执行”可以认为是整个生命周期中最重要的阶段，这其中包括了大量为了检索数据到存储引擎的调用以及调用后的数据处理，包括排序、分组等。 对于一个查询的全部生命周期，上面列的并不完整，这里只是想说明，了解查询的生命周期，清楚查询的时间消耗情况对于优化查询有很大的意义。 慢查询基础：优化数据访问查询性能低下最基本的原因是访问的数据太多，大部分性能低下的查询都可以通过减少访问的数据量的方式进行优化。对于低效的查询，通过下面两个步骤来分析总是很有效的： 确认应用程序是否在检索大量超过需要的数据。这通常意味着访问了太多的行，但有时候也有可能是访问了太多的列。 确认MySQL服务器是否在分析大量超过需要的数据行。 是否向数据库请求了不需要的数据有些查询会请求超过实际需要的数据，然后这些多余的数据会被应用程序丢弃，这会给MySQL服务器带来额外的负担，并增加网络开销，另外也会消耗应用服务器的CPU和内存资源。下面是一些典型的案例： 查询不需要的记录：一个常见的错误是常常会误以为MySQL会只返回需要的数据，实际上MySQL确实先返回全部结果集再进行计算。最简单有效的解决方法就是在这样的查询后面加上LIMIT。 多表关联时返回全部列：例如你想查询所有在电影Academy Dinosaur中出现的演员，千万不要下面这种写法编写查询：1234SELECT * FROM actorINNER JOIN film_actor USING(actor_id)INNER JOIN film USING(film_id)WHERE fime.title = 'Academy Dinosaur'; 这将返回这三个表的全部数据列。正确的方式应该是像下面这样只取需要的列：SELECT actor.* FROM actor...。 总是取出全部列：每次看到SELECT * 的时候都需要用怀疑的眼光审视，是不是真的需要返回全部的列？这无疑会增加额外的消耗。但是如果这种有点浪费数据库资源的方式可以简化开发，因为提高相同代码片段的复用性，如果清楚这样做的性能影响，那么这种做法也是值得考虑的。如果应用程序使用了某种缓存机制也可以，获取并缓存所有的列的查询，相比多个独立的只获取部分列的查询可能就更有好处。 重复查询相同的数据：如果不断地重复执行相同的查询，然后每次都返回相同的数据，这样是完全没有必要的。比较好的方案是，当初次查询的时候将这个数据缓存起来，需要的时候从缓存中取出，这样的性能显然会更好。 MySQL是否在扫描额外的记录在确定查询只返回需要的数据以后，接下来该看看查询为了返回结果是否扫描了过多的数据。对于MySQL，最简单的衡量查询开销的三个指标如下： 响应时间 扫描的行数 返回的行数 响应时间响应时间只是一个表面上的值，但是响应时间仍然是最重要的指标。响应时间是两个部分之和：服务时间和排队时间。服务时间是指数据库处理这个查询真正花了多长时间；排队时间是指服务器因为等待某些资源而没有真正执行查询的时间，一般最常见和重要的是I/O等待和锁等待。 当你看到一个查询的响应时间的时候，首先需要问问自己，这个响应时间是否是一个合理的值。实际上可以使用“快速上限估计”法来估算查询的响应时间，概括地说，了解这个查询需要哪些索引以及它的执行计划是什么，然后计算大概需要多少个顺序和随机I/O，再用其乘以在具体硬件条件下一次I/O的消耗时间，最后把这些消耗都加起来，就可以获得一个大概参考值来判断当前响应时间是不是一个合理的值。 扫描的行数和返回的行数分析查询时，查看该查询扫描的行数是非常有效的，这在一定程度上能说明该查询找到需要的数据和效率高不高。并不是所有的行的访问代价都是相同的，较短的行的访问速度更快，内存中的行比磁盘中的行的访问速度要快得多。 理想情况下扫描的行数和返回的行数应该是相同的，但实际中这种情况并不多，扫描的行数对返回的行数的比率通常很小，一般在1：1和10：1之间，不过有时候这个值也可能非常大。 扫描的行数和访问类型在评估查询开销的时候，需要考虑一下从表中找到某一行数据的成本。MySQL的好几种访问类型，有些方式可能需要扫描很多行才能返回一行，也有些方式可能无需扫描就能返回结果。 在EXPLAIN语句中的type列反应了访问类型，访问类型有很多种，从全表扫描到索引扫描、范围扫描、唯一索引查询、常数索引等，这里列的这些，速度是从慢到快，扫描的行数也是从多到少的。 如果查询没有办法找到合适的访问类型，那么解决的最好办法通常就是增加一个合适的索引，索引让MySQL以最高效、扫描行数最少的访问找到需要的记录。 一般MySQL能够使用如下三种方式应用WHERE条件，从好到坏依次为： 在索引中使用WHERE条件来过滤不匹配的记录。这是在存储引擎层完成的。 使用索引覆盖扫描（在Extra中出现了Using index）来返回记录，直接从索引中过滤不需要的记录并返回命中的结果。这是在MySQL服务层完成的，但无须再回表查询记录。 从数据表中返回数据，然后过滤不满足条件的记录（在Extra列中出现Using Where）。这在MySQL服务器层完成，MySQL需要先从数据表读出记录然后过滤。 好的索引很重要，可以让查询使用合适的访问类型，尽可能的只扫描需要的数据行，但也不是说增加索引就能让扫描的行数等于返回的行数。如果发现查询需要扫描大量的数据但只返回少数的行，那么通常可以尝试下面的技巧去优化： 使用索引覆盖扫描，把所有需要用的列都放到索引中，这样存储引擎就无须回表获取对应行就可以返回结果了。 改变库表结构，例如使用单独的汇总表。 重写这个复杂的查询，让MySQL优化器能以更优化的方式执行这个查询。 重构查询的方式在优化有问题的查询时，目标应该是找到一个更优的方法获得实际需要的结果，而不一定总是需要从MySQL获取一模一样的结果集，有时可以将查询转换一种写法让其返回一样的结果，但是性能更好。 一个复杂查询还是多个简单查询在传统实现中，总是强调需要数据库层完成尽可能多的工作，这样做的逻辑在于以前总是认为网络通信、查询解析和优化时是一件代价很高的事情。但是这样的想法对于MySQL并不适用，MySQL从设计上让连接和断开连接都很轻量级，在返回一个小的查询结果方面很高效。 MySQL内部每秒能扫描内存中上百万行数据，相比之下，MySQL响应数据给客户端就慢得多了。在其他条件都相同的时候，使用尽可能少的查询当然是最好的。但是有时候将一个大查询分解为多个小查询是很有必要的。 切分查询有时候对于一个大查询我们需要“分而治之”，将大查询切分成小查询，每个查询功能完全一样，只完成一小部分，每次只返回一小部分查询结果。 例如在删除旧的数据时，定期清除大量数据时，如果用一个大的语句一次性完成的话，则可能需要一次锁住很多数据，占满整个事务日志，耗尽系统资源、阻塞很多小的但是重要的查询。将一个大的DELETE语句切分成多个较小的查询可以尽可能小的影响MySQL性能，同时还可以减少MySQL复制的延迟。例如我们可以每次只删除一部分数据，每次删除数据后都暂停一会在做下一次删除，这样也可以将服务器上原本一次性的压力分散到一个很长的时间段中，就可以大大降低对服务器的影响，还可以大大减少删除时锁的持有时间。 分解关联查询很多高性能的应用都会对关联查询进行分解，简单地，可以对每一个表进行一次单表查询，然后将结果在应用程序中进行关联。例如下面这个查询： 1234SELECT * FROM tag JOIN tag_post ON tag_post.tag_id = tag.id JOIN post ON tag_post.post_id = post.idWHERE tag.tag = 'mysql'; 可以分解成下面这些语句来代替： 123SELECT * FROM tag WHERE tag = 'mysql';SELECT * FROM tag_post WHERE tag_id = 1234;SELECT * FROM post WHERE post.id in(123,456,567,9098,8904); 用分解关联查询有如下优势： 让缓存的效率更高。许多应用程序可以方便地缓存单表查询对应的结果对象，对MySQL的查询缓存来说，如果关联中的某个表发生了变化，那么就无法使用查询缓存了，而拆分后，如果某个表很少改变，那么基于该表的查询就可以重复利用查询缓存结果了。 将查询分解后，执行单个查询可以减少锁的竞争。 在应用层做关联，可以更容易对数据库进行拆分，更容易做到高性能和可扩展。 查询本身效率也可能会有所提升，例如用IN()代替关联查询，可以让MySQL按照ID顺序进行查询，这可能比随机的关联更高效。 可以减少冗余记录的查询。在应用层做关联查询，意味着对于某条记录应用只需要查询一次，而在数据库中做关联查询，则可能需要重复地访问一部分数据，所以这样的重构还可能会减少网络和内存的消耗。 更进一步，这样相对于在应用中实现了哈希关联，而不是使用MySQL的嵌套循环关联。 在很多场景下，通过重构查询将关联放在应用程序中将会更加高效，比如：当应用能够方便的缓存单个查询的结果的时候、当可以将数据分不到不同的MySQL服务器上的时候、当能够使用IN（）的方式代替关联查询的时候、当查询中使用同一个数据表的时候。 查询执行的基础下图可以看出当向MySQL发送一个请求的时候，MySQL到底做了些什么： 客户端先发送一条查询给服务器。 服务器先检查缓存，如果命中缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段。 服务端进行SQL解析、预处理，再由优化器生成对应的执行计划。 MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询。 将结果返回给客户端。 MySQL客户端/服务器通信协议MySQL客户端和服务器之间的通信协议是半双工的，这意味着，在任何一个时刻，要么是由服务器向客户端发送数据，要么是由客户端向服务器发送数据，这两个动作不能同时发生。所以，无法也无需将一个消息切成小块独立来发送。这种协议虽然让MySQL通信简单快速，但是一个明显的限制就是没法进行流量控制，一旦一端开始发送消息，另一端要接收完整个消息才能响应它。即一旦客户端发送了请求，它能做的事情就只是等待结果了。 相反的，一般服务器相应给客户的数据很多，由多个数据包组成。当服务器开始响应客户端请求时，客户端必须完整地接收整个返回结果，而不能简单地只取几条结果。这也是在必要时侯一定要在查询中加上LIMIT限制的原因。 多数连接MySQL的库函数都可以获得全部结果集并缓存到内存里，还可以逐行获取需要的数据。默认一般是获得全部结果集并缓存到内存中。MySQL通常需要等所有的数据都已经发送给客户端才能释放这条查询所占用的资源，所以接受全部结果并缓存通常可以减少服务器的压力，早点释放相应的资源。 查询状态对于一个MySQL连接，或者说一个线程，任何一个时刻都有一个状态，该状态标识了MySQL当前正在做什么。最简单的查看方式是使用SHOW FULL PROCESSLIST命令，在一个查询生命周期中的状态有这么几种： Sleep：线程正在等待客户端发送新的请求。 Query：线程正在执行查询或者正在将结果发送给客户端。 Locked：在MySQL服务层，该线程正在等待表锁。在存储引擎级别实现的锁，例如InnoDB的行锁并不会体现在线程装态中。对于MyISAM来说这是一个比较典型的状态，但在其他没有行锁的引擎中也会经常出现。 Analyzing and statistics：线程正在收集存储引擎的统计信息，并生成查询的执行计划。 Copying to tmp table [on disk]：线程正在执行查询，并且将其结果集都复制到一个临时表中，这种状态一般要么是在做GROUP BY操作，要么是文件排序操作，或者UNION操作。如果这个状态后面还有“on disk”标记，那么表示MySQL正在将一个内存临时表放在磁盘上。 Sorting result：线程正在对结果集进行排序。 Sending data：这种情况下线程可能在多个状态之间传送数据，或者在生成结果集，或者在向客户端返回数据。 查询缓存在解析一个查询语句之前，如果查询缓存是打开的，那么MySQL会优先检查这个查询是否命中查询缓存的数据。这个查询时通过一个对大小写敏感的哈希查找实现的。 如果当前查询恰好命中查询缓存，那么在返回查询结果之前MySQL会检查一次用户权限。这仍然是无须解析查询SQL语句的。如果没有权限问题，MySQL会直接从缓存中拿到结果并返回给客户端。这总情况下，查询不会解析，不用生成执行计划，不会被执行。 查询优化处理查询的生命周期的下一步是将一个SQL转换成一个执行计划，MySQL再按照这个执行计划和存储引擎进行交互。这包括多个子阶段：解析SQL、预处理、优化SQL执行计划。这个过程中出现任何错误都可能终止查询。 语法解析器和预处理首先MySQL通过关键字将SQL语句进行解析，并生成一颗对应的“解析树”。MySQL解析器将使用MySQL语法规则验证和查询解析。例如验证是否使用错误的关键字，关键字顺序是否正确等。 预处理器则根据一些MySQL规则进一步检查树是否合法。例如检查数据表和列是否存在、名字和别名是否有歧义等待。 下一步预处理器会验证权限。这通常很快，除非服务器上有非常多的权限配置。 查询优化器在语法树被验证合法以后，将由优化器将其转化成执行计划。一条查询可以有很多种执行方式，最后都返回相同的结果。优化器的作用就是找到这其中最好的执行计划。 MySQL使用基于成本的优化器，它将尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。当然优化器并不能完全保证所选择的执行计划是最优的，有很多种原因会导致MySQL优化器选择错误的执行计划： 统计信息不准确。MySQL依赖存储引擎提供的统计信息来评估成本，但有的存储引擎提供的信息是不准确的，例如InnoBD因为其MVCC的架构，并不能维护一个数据表的精确统计信息。 执行计划的成本估算不等同于实际执行的成本。例如MySQL层面并不知道哪些页面在内存中，哪些在磁盘上，所以执行的I/O时间也无从得知。所以我们看到的执行成本来选择执行计划并不是完美的模型。 MySQL从不考虑其他并发执行的查询。 MySQL也并不完全是基于成本的优化，优势也会基于一些固定的规则。例如，如果存在MATCH()子句，则在全文索引的时候就使用全文索引。即使有时候使用别的索引和WHERE条件可以远比这种方式要快。 MySQL不会考虑不受其控制的操作的成本。例如执行存储过程或用户自定义函数的成本。 优化器有时候无法去估算所有可能的执行计划，所以可能错过实际上最优的执行计划。 MySQL的查询优化器使用了很多优化策略生成一个最优的执行计划。优化策略可以简单的分为两种：静态优化和动态优化。 静态优化可以直接对解析树进行分析，并完成优化。例如优化器可以通过一些简单的代数变换将WHERE条件转换成另一种等价形式。静态优化不依赖于特别的数值，如WHERE条件中带入的一些常数等。静态优化在第一次完成之后一直有效，即使使用了不同的参数重复查询也不会发生变化，可以认为这是一种“编译时优化”。 动态优化和查询的上下文有关，也可能和很多其他因素有关。例如WHERE条件的中的取值、索引中条目对应的数据行数等。这需要在每次查询的时候都重新评估，可以认为这是“运行时优化”。 MySQL对查询的静态优化只需要做一次，但对查询的动态优化规则在每次运行时都需要重新评估，有时甚至在查询的执行过程中也会重新优化。 下面是一些MySQL能够处理的优化类型： 重新定义关联表的顺序：**数据表的关联并不总是按照在查询中指定的顺序进行。**决定关联的顺序是优化器很重要的一部分功能。 将外连接转化成内连接：并不是所有的OUTER JOIN语句都必须以外连接的方式执行。例如WHERE条件、库表结构都可能会让外连接等价于一个内连接，MySQL可以自动识别这一点并重写查询，让其调整关联顺序。 使用等价变换规则：**MySQL可以使用一些等价变换来简化并规范表达式。**它可以合并和减少一些比较，还可以移除一些恒成立和一些恒不成立的判断。 优化COUNT(*)、MIN()和MAX()：索引和列是否可为空通常可以帮助MySQL优化这类表达式。例如，要找到某一列的最小值，只需要查询对应的B-Tree索引最左端的记录，MySQL可以直接获取索引的第一行记录。如果MySQL使用了这种类型的优化，那么在EXPLAIN中就可以看到“Select tables optimized away”，从字面意思可以看出来，他表示优化器已经从执行计划中移除了该表，并以一个常数代替。 预估并转化为常数表达式：**当MySQL检测到一个表达式可以转化为常数的时候，就会一直把该表达式作为常数进行优化处理。** 索引覆盖扫描：当索引中的列包含所有查询中需要使用的列的时候，MySQL就可以使用索引返回需要的数据，而无需查询对应的数据行。 子查询优化：MySQL在某些情况下可以将子查询转换一种效率更高的形式，从而减少多个查询多次对数据进行访问。 提前终止查询：**在发现已经满足查询需求的时候，MySQL总是能够立刻终止查询。**例如当使用了LIMIT的时候、发现了一个不成立的条件等。 等值传播：如果两个列的值通过等式关联，那么MySQL能够把其中一个列的WHERE条件传递到另一个列上。 列表IN()的比较：在很多数据库系统中，IN()完全等同于多个OR条件的子句，因为这二者完全等价。但是在MySQL中这点是不成立的，MySQL将IN()列表中的数据先进行排序，然后通过二分查找的方式来确定列表中的值是否满足条件，这是一个O(log n)复杂度的操作，等价的变换成OR查询的复杂度为O(n)，对IN()列表中有大量取值的时候，MySQL的处理将会更快。 数据和索引的统计信息MySQL服务层没有任何统计信息，所以MySQL查询优化器在生成查询的计划时，需要向存储引擎获取相应的统计信息。存储引擎则提供给优化器对应的统计信息，包括：每个表或者索引有多少个页面、每个表的每个索引的基数是多少、数据行和索引长度、索引的分布信息等。优化器根据这些信息来选择一个最优的执行计划。 MySQL如何执行关联查询MySQL认为任何一个查询都是一次“关联”，并不仅仅是一个查询需要到两个表匹配才叫关联，所以在MySQL中，每一次查询，每一个片段都可能是关联。 举一个UNION查询的例子，对于UNION查询，MySQL先将一系列的单个查询结果放在一个临时表中，然后再重新读出临时表数据来完成UNION查询，所以读取结果临时也是一次关联。 当前MySQL关联执行的策略很简单：MySQL对任何关联都执行嵌套循环关联操作，即MySQL先在一个表中循环取出单条数据，然后再嵌套循环到下一个表中寻找匹配的行，依次下去，直到找到所有表中匹配的行为止。然后根据各个表匹配的行，返回查询中需要的各个列。MySQL会先尝试在最后一个关联表中找到所有匹配的行，如果最后一个关联表无法找到更多的行以后，MySQL返回到上一层关联表，看是否能找到更多的匹配记录，依次类推迭代执行。 从本质上来说，MySQL对所有的类型的查询都以同样的方式运行。例如，MySQL在FROM子句中遇到子查询时，先执行子查询并将其结果放在一个临时表中，然后将这个临时表当作一个普通表对待（正如其名“派生表”）。MySQL在执行UNION查询时也是用类似的临时表。在遇到右外连接的时候，MySQL将其改写成等价的左外连接。但是全外连接无法通过嵌套循环和回溯的方式完成，这大概也是MySQL并不支持全外连接的原因。 执行计划和很多其他关系数据库不同，MySQL并不会生成查询字节码来执行查询，MySQL生成查询一棵指令树，然后通过存储引擎执行完成这棵指令树并返回结果。MySQL总是从一个表开始一直嵌套循环、回溯完成所有表关联。所以MySQL的执行计划总是如下图所示，是一棵左侧深度优先的树： 关联查询优化器MySQL优化器最重要的一部分就是关联查询优化，他决定了多个表关联时的顺序。通常多表关联的时候，可以有多种不同的关联顺序来获得相同的执行顺序，关联查询优化器则通过评估不同顺序时的成本来选择一个代价最小的关联顺序。 关联优化器会尝试再所有的关联顺序中选择一个成本最小的来生成执行计划，尽量保证每次关联查询后，能排除掉更多的无用数据。如果可能，优化器会遍历每一个表然后逐个做嵌套循环计算每一棵可能的执行计划树的成本，最后返回一个最优的执行计划。 不过糟糕的是，如果有超过n个表的关联，那么需要检查n的阶乘种关联计划。我们称之为所有可能的执行计划的“搜索空间”，搜索空间的增长速度非常快，例如若10个表的关联，那么共有3628800种不同的关联顺序。 排序优化无论如何排序都是一个成本很高的操作，所以从性能考虑，应尽可能避免排序或者尽可能对大量数据进行排序。 当不能使用索引生成排序结果的时候，MySQL需要自己进行排序，如果数据量小则在内存中进行，如果数据量大则需要使用磁盘，不过MySQL将这个过程统一称为文件排序（filesort）。 如果需要排序的数据量小于“排序缓冲区”，MySQL使用内存进行“快速排序”操作。如果内存不够排序，那么MySQL会先将数据分块，对每个独立的块使用“快速排序”进行排序，并将各个块的排序结果存放在磁盘上，然后将各个排好序的块进行合并，最后返回排序结果。 MySQL有两种排序算法： 两次传输排序（旧版本使用）：**读取行指针和需要排序的字段，对其进行排序，然后再根据排序结果读取所需要的数据行。** 这需要进行两次数据传输，即需要从数据表中读取两次数据，第二次读取数据的时候，因为是读取排序列进行排序后的所有记录，这会产生大量的随机I/O，所以两次数据传输的成本非常高。 单次传输排序（新版本使用）：**先读取所需要的所有列，然后再根据给定列进行排序，最后直接返回排序结果。因为不需要从数据表中读取两次数据，对于I/O密集型的应用，这样做的效率高了很多。但是缺点是如果需要返回的列非常多、非常大，会额外占用大量的空间，**而这些列对排序操作本身来说是没有任何作用的。 MySQL在进行文件排序的时候需要使用的临时存储空间可能会比想象的要大很多，原因在于MySQL在排序时，对每一个排序记录都会分配一个足够长的定长空间来存放。这个定长空间必须足够长以容纳其中最长的字符串，如果使用UTF-8字符集，那么MySQL将会为每个字符预留三个字节。因此排序消耗的临时空间可能会比磁盘上的原表要大很多倍。 在关联查询的时候如果需要排序，MySQL会分两种情况来处理这样的文件排序。如ORDER BY子句中所有的列都来自关联的第一个表，那么MySQL在关联处理第一个表的时候就进行文件排序，这种情况MySQL在EXPLAIN结果中的Extra字段会有“Using filesort”。除此之外所有情况，MySQL都会先将关联的结果存放到一个临时表中，然后在所有的关联都结束后，再进行文件排序，这种情况MySQL在EXPLAIN结果中的Extra字段会有“Using temporary；Using filesort”。如果查询中有LIMIT的话，LIMIT也会在排序之后应用。 MySQL5.6后，当只需要返回部分排序结果的时候，例如使用了LIMIT子句，MySQL不再对所有的结果进行排序，而是根据实际情况，选择抛弃不满足条件的结果，然后再进行排序。 查询执行引擎在解析和优化阶段，MySQL将生成查询对应的执行计划，MySQL的查询执行引擎则根据这个执行计划来完成整个查询。这里执行计划是一个数据结构，而不是和很多其他关系型数据库那样会生成对应的字节码。 MySQL只是简单的根据执行计划给出的指令逐行执行，在根据执行计划逐步执行的过程中，有大量的操作需要通过调用存储引擎实现的接口来完成。 返回结果给客户端查询执行的最后一个阶段是将结果返回给客户端。即使查询不需要返回结果集给客户端，MySQL仍然会返回这个查询的一些信息，如该查询影响到的行数。 如果查询可以被缓存，那么MySQL在这个阶段也会将结果存放到查询缓存中。 MySQL将结果集返回给客户端是一个增量、逐步返回的过程，一旦服务器处理完最后一个关联表，开始生成第一条结果时，MySQL就可以开始向客户端逐步返回结果集了。 这样处理有两个好处：服务器无须存储太多的结果，也就不会因为要返回太多的结果而消耗太多内存。另外这样的处理也让MySQL客户端第一时间获得返回的结果。]]></content>
      <categories>
        <category>MySQL数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[创建高性能索引]]></title>
    <url>%2Fposts%2F28327%2F</url>
    <content type="text"><![CDATA[索引（也叫做“键（key）”）是存储引擎用于快速找到记录的一种数据结构。索引对于良好的性能非常关键，尤其是当表中的数据量越来越大时，索引对性能的影响越发重要。其外索引优化也很重要，索引优化应该是对查询性能优化最有效的手段了，索引能轻易将查询性能提高几个数量级。 索引基础要理解MySQL中索引时如何工作的，最简单的方法就是去看看一本书的“索引”部分：如果想在一本书中找到某个特定主题，一般会先看书的“索引”，找到对应的页码。 在MySQL中，存储引擎用类似的方法使用索引，其先在索引中找到对应值，然后根据匹配的索引记录找到对应的数据行。也就是说，MySQL先在索引上按值进行查找，然后返回所有包含该值的数据行。 索引可以包含一个或多个列的值。如果索引包含多个列，那么列的顺序也十分重要，因为MySQL只能高效的使用索引的最左前缀列。 索引的类型在MySQL中，索引是在存储引擎层而不是服务层实现的。索引不同存储引擎的索引的工作方式不一样，也不是所有的存储引擎都支持所有类型的索引。下面是MySQL支持的索引类型，以及它们的优点和缺点。 B-Tree索引如果没有特别说明索引类型，默认是B-Tree索引，它使用B-Tree数据结构来存储数据，大多数MySQL引擎都支持这种索引（Archive除外）。不过底层的存储引擎也可能使用不同的数据结构，例如InnoDB则使用的是B+Tree。 存储引擎以不用的方式使用B-Tree索引，性能也各有不同，各有优势。例如：MyISAM使用前缀压缩技术使得索引更小，但InnoDB则按照原数据格式进行存储。再如MyISAM索引通过数据的物理位置引用被索引的行，而InnoDB则根据主键引用被索引的行。 B-Tree通常意味着所有的值都是按顺序存储的，并且每一个叶子页到根的距离相同。下图展示B-Tree索引的抽象表示： B-Tree索引能加快访问数据的速度，因为存储引擎不再需要进行全表扫描来获取需要的数据，取而代之的是从索引的根节点开始进行搜索。根节点的槽中存放了指向子节点的指针，存储引擎根据这些指针向下层查找。通过比较节点页的值和要查找的值可以找到合适的指针进入下层子节点，这些指针实际上定义了子节点中值的上限和下限。最终存储引擎要么是找到对应的值，要么该记录不存在。叶子节点比较特别，它们的指针指向的是被索引的数据，而不是其他的节点页。 B-Tree对索引列是顺序组织存储的，所以很适合查找范围数据。例如，在一个基于文本域的索引树上，按字母顺序传递连续的值进行查找是非常合适的，所以像“找出所有以I到K开头的名字”这样的查找效率会非常高。 建立如下表： 1234567CREATE TABLE people( last_name varchar(50) not null, first_name varchar(50) not null, dob date not null, gender enum('m','f') not null, key(last_name,first_name,dob)); 索引对多个值进行排序的依据是CREATE TABLE语句中定义索引时列的顺序。例如上图中，最后两个条目中，两个人的姓和名都一样，则根据他们的出生日期来排序。 可以使用B-Tree索引的查询类型。B-Tree索引适用于全键值、键值范围或前缀查询。其中键前缀查找只适用于根据最左前缀的查找。故所述的索引对如下类型的查询有效： 全值匹配：全值匹配指的是和索引中的所有列进行匹配。例如上图中，索引可用于查找姓名为Cuba Allen、同时出生于1960-01-01的人。 匹配最左前缀：索引可用于查询所有姓为Allen的人，即只使用索引的第一列。 匹配列前缀：也可以只匹配某一列的值的开头部分。例如可以查找所有以J开头的性的人，这里也只使用了索引的第一列。 匹配范围值：例如索引可用于查找姓在Allen和Barrymore之间的人，这里也只使用了索引的第一列。 精确匹配某一列并范围匹配另外一列：例如可以查找所有姓为Allen，并且名字是字母K开头的人。即第一列全匹配，第二列范围匹配。 只访问索引的查询：B-Tree通常可以支持“只访问索引的查询”，即查询只需要访问索引，而无须访问数据行。 因为索引树中的节点是有序的，所以除了按值查找之外，索引还可以用于查询中的ORDER BY操作。一般来说，如果B-Tree可以按照这种方式用于排序。 但是B-Tree索引也有一些限制： 如果不是按照索引的最左列开始查找，则无法使用索引。即最左列不能跳过，例如上面例子中的索引无法用于查找名字为Bill的人，也无法查找某个特定生日的人，因为这两列都不是最左数据列。 不能跳过索引中的列。也就是说不能查找姓名为Smith并且某个特定日期出生的人。如果不指定名，则MySQL只能使用索引的第一列。 如果查询中有某个列的范围查找，则其右边所有列都无法使用索引优化查找。 总而言之这些限制都和索引列的顺序有关。在优化性能的时候，可能需要使用相同的列但顺序不同的索引来满足不同类型的查询需求。 哈希索引哈希索引基于哈希表实现，只有精确匹配索引所有列的查询才有效。对于每一行数据，存储引擎哦都会对所有的索引列计算一个哈希码，哈希码是一个较小的值，并且不同键值的行计算出来的哈希码也不一样。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。 假设有如下表： 12345CREATE TABLE testhash ( fname VARCHAR(50) NOT NULL, lname VARCHAR(50) NOT NULL, KEY USING HASH(fname))ENGINE= MEMORY; 表中包含数据：假设索引使用假想的哈希函数f()，它返回下面的值（以下为实例数据）： 1234f('Arjen') = 2323f('Baron') = 7437f('Peter') = 8784f('Vadim') = 2458 则哈希索引的数据结构如下：注意，每个槽的编号是顺序的，但是数据行不是。 因为索引自身只需存储对应的哈希值，所以索引的结构十分紧凑，这也让哈希索引查找到速度非常快。但是哈希索引也有它的限制： 哈希索引只包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行。不过访问内存中的行的速度很快，所以大部分情况下这一点对性能的影响并不明显。 哈希索引值并不是按照索引值顺序存储的，所以也就无法用于排序。 哈希索引也不支持部分索引列匹配查找，因为哈希索引始终是使用索引列的全部内存容来计算哈希值的。 哈希索引只支持等值比较查询，包括=、IN()、&lt;=&gt;，也不支持任何范围查询。 访问哈希索引的数据非常快，除非有很多哈希冲突。当出现哈希冲突的时候，存储引擎会遍历链表中所有的行指针，逐行进行比较查找。 如果哈希冲突很多的话，一些索引维护操作的代价也会很高，冲突越多，代价越高。 因为这些限制，哈希索引只适用于某些特定的场合，而一旦适合哈希索引，则它带来的性能提升将非常明显。 InnoDB引擎有一个特殊的功能叫做“自适应哈希索引”。当InnoDB注意到某些索引值被使用的非常频繁时，它会在内存中基于B-Tree所以之上再创建一个哈希索引。这是一个完全自动的、内部的行为，用户无法控制或者配置，但是可以关闭。 自定义哈希索引时，记住不要使用SHA1()和MD5()作为哈希函数，因为这两个函数计算出来的哈希值时非常长的字符出阿奴，会浪费大量空间，比较时也会更慢。使用简单哈希函数导致的冲突在一个可以接受的范围，同时又能提供更好的性能。自定义哈希函数要返回整数，而不是字符串，一个简单的方法可以使用MD5()函数返回值的一部分来作为哈希函数。 当使用哈希索引进行查找的时候，要避免冲突问题，必须在WHERE条件中带入哈希值和对应列值。例如SELECT * FROM words WHERE crc = CRC32(&#39;gnu&#39;) AND word = &#39;gnu&#39;;。如果只是统计记录数（不精确的），则可以不带入列值。 其他索引 空间数据索引（R-Tree）：MyISAM表支持空间索引，可以用作地理数据存储。不同于B-Tree索引，这类索引无须前缀查询。空间索引会从所有维度来索引数据。查询时，可以有效地使用任意维度来组合查询。 全文索引：全文索引是一种特殊类型的索引，它查找的时文本中的关键字，而不是直接比较索引中的值。全文索引更类似于搜索引擎做的事情。在相同的列上同时创建全文索引和基于值的B-Tree索引不会有冲突，全文索引适用于MATCH AGAINST操作，而不是普通的WHERE条件操作。 索引的优点索引可以让服务器快速地定位到表的指定位置，但这不是索引的唯一作用，因为根据创建索引的数据结构不同，索引也有一些其他的附加作用。 最常见的B-Tree索引，按照顺序存储数据，所以MySQL可以用来做ORDER BY和GROUP BY操作；因为数据是有序的，所以B-Tree也就会将相关的列值都存储在一起；因为索引中存储了实际的列值，所以某些查询只使用索引就能完成全部查询。所以总结下来索引有如下三个优点： 索引大大减少了服务器需要描述的数据量 索引可以帮助服务器避免排序和临时表 索引可以将随机I/O变为顺序I/O 有一个简单评价一个索引是否适合某个查询的方法叫做“三星系统”：索引将相关的记录放在一起则获得一星；如果索引中的数据顺序和查找中的排列顺序一致则获得二星；如果索引的列包含了查询中需要的全部列则获得三星。 高性能的索引策略独立的列如果查询时不当的使用索引，或使得MySQL无法使用已有的索引。如果查询中的列不是独立的，则MySQL就不会使用索引。“独立的列”是指索引列不能是表达式的一部分，也不能是函数的参数。例如：SELECT actor_id FROM sakila.actor WHERE actor_id+1 = 5;和SELECT ... WHERE TO_DAYS(CURRENT_DATE) - TO_DAYS(data_col) &lt;= 10; 所以我们应该养成简化WHERE条件的习惯，始终将索引列单独放在符号的一侧。 前缀索引和索引选择性通常可以索引开始的部分字符，这样可以大大节约索引空间，从而提高索引效率。但是这样也会降低索引的选择性。索引的选择性是指：不重复的索引值（也称为基数）和数据表的记录总数（#T）的比值，范围从 1/#T 到 1 之间。索引的选择性越高则查询效率越高，因为选择性高的索引可以让MySQL在查找时过滤掉更多的行，唯一索引的选择性是1，即选择性最好，性能也最好。 对于BLOB、TEXT或者很长的VARCHAR类型的列，必须使用前缀索引，因为MySQL不允许索引这些列的完整长度。 诀窍在于要选择足够长的前缀以保证较高的选择性，同时又不能太长，以便节约空间。 为了决定前缀的合适长度，需要找到最常见的值的列表，然后和最常见的前缀列表进行比较，适当增加前缀长度，直到这个前缀的选择性接近完整列的选择性。另外一个方法就是计算完整列的选择性，并使前缀的选择性接近于完整列的选择性，计算方法如下：SELECT COUNT(DISTINCT city)/COUNT(*) FROM city_demo。通常来说，如果前缀的选择性能够接近0.031，基本上就可用了。 前缀索引是一种能使索引更小、更快的有效办法，但另一方面也有其缺点：MySQL无法使用前缀索引做ORDER BY和GROUP BY，也无法使用前缀索引做覆盖扫描。 多列索引很多人对多列索引的理解一个常见的错误就是：为每个列创建独立的索引，或者按照错误的顺序创建多列索引。在多个列上建立独立的单独索引大部分情况下并不能提高MySQL的查询性能。 MySQL5.0和最新版本引入了一种叫“索引合并”的策略，一定程度上可以使用表上的多个单列索引来定位指定的行。这种算法有三个变种：OR条件的联合（union），AND条件的相交，组合前两种情况的联合及相交。 索引合并策略有时候是一种优化的结果，但实际上更多时候说明了表上的索引建的很差。如果使用EXPLAIN语句中看到有索引合并，应该好好检查一下查询和表的结构，有无优化的余地。 选择合适的索引列顺序正确的索引顺序依赖于使用该索引的查询，并且同时需要考虑如何更好地满足排序和分组的需要。 在一个多列的B-Tree索引中，索引列的顺序意味着索引首先按照最左列进行排序，其次是第二列，等等。所以索引可以按照升序或者降序进行扫描，以满足精确符合列顺序的ORDER BY、GROUP BY和DISTINCT等子句的查询需求。所以多列索引的列顺序至关重要。 对于如何选择索引的列顺序有一个经验法则：将选择性最高的列放在索引最前列。但不需要考虑排序和分组时，将选择性最高的列放在前面通常是很好的。这时候索引的作用只是用于优化WHERE条件的查找。 聚簇索引聚簇索引并不是一种单独的索引类型，而是一种数据存储方式，具体的细节依赖于其实现方式，InnoDB的聚簇索引实际上在同一个结构中保存了B-Tree索引的数据行。**当表有聚簇索引时，它的数据行实际上存放在索引的叶子页中。“聚簇”表示数据行和相邻的键值紧凑的存储在一起。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。** 下图展示聚簇索引中的记录是如何存放的：注意到，叶子页中包含了行的全部数据，但是节点页只包含了索引列。InnoDB通过主键聚集数据，也就是说上图中的“被索引的列”就是主键列。如果没有定义主键，InnoDB会选择一个唯一非空索引代替。如果没有这样的索引，InnoDB会隐式定义一个主键来作为聚簇索引。下面讨论一些聚簇索引的优缺点。 优点： 可以把相关的数据保存在一起。 数据访问更快。聚簇索引将索引和数据保存在同一个B-Tree中，因此从聚簇索引中获取数据通常比在非聚簇索引中查找更快。 使用覆盖索引扫描的查询可以直接使用页节点的主键值。 缺点： 聚簇索引最大限度的提高了I/O密集型应用的性能，但如果全部数据都放在内存中，那么访问的顺序就没有关系了，因此也就无法体现出什么优势了。 插入速度严重依赖于插入顺序。按照主键的顺序插入是数据加载到InnoDB表中最快速度的方式；但如果不是按照主键顺序加载数据，那么在加载完成后最好使用OPTIMIZE TABLE命令重新组织一下表。 更新聚簇索引列的代价很高，因为会强制InnoDB将每个被更新的行移动到新的位置。 基于聚簇索引的表在插入新行，或者主键被更新导致需要移动行的时候，可能面临“页分裂”的问题，页分裂会导致表占用更多的磁盘空间。当行的主键值要求必须将这一行插入到某个已满的页中时，存储引擎会将该页分裂成两个页面来容纳改行，这就是一次页分裂操作。 聚簇索引可能导致全表扫描变慢，尤其是行比较稀疏，或者由于页分裂导致数据存储不连续的时候。 二级索引（非聚簇索引）可能比想象的要更大，因为在二级索引的叶子节点包含了引用行的主键列。 二级索引访问需要两次索引查找，而不是一次。导致这个问题的原因在于二级索引中保存的“行指针”的实质。二级索引叶子节点保存的不是指向行的物理位置的指针，而是行的主键值。这意味着通过二级索引查找行，存储引擎需要找到二级索引的叶子节点获得对应的主键值，然后根据这个值去聚簇索引中查找到对应的行。对于InnoDB自适应哈希索引可以减少这样的重复工作。 在InnoDB和MyISAM中，对于聚簇索引和非聚簇索引的实现有区别，所以就以下表为例对比二者的区别： 123456CREATE TABLE layout_test( col1 int NOT NULL, col2 int NOT NULL, PRIMARY KEY(col1), KEY(col2)); 假设该表的主键取值为110000，并且数据在磁盘上的存储方式已经最优，但行的顺序是随机的；列col2的值是从1100之间随机复制，可能有很多重复值。 MyISAM的数据分布MyISAM的数据分布非常简单，它按照数据插入的顺序存储在磁盘上，如图所示：在行的旁边显示了行号，从0开始递增。因为行是定长的，索引MyISAM可以从表的开头跳过所需的字节知道需要的行。这种分布方式很容易创建索引，如图： MyISAM中主键索引和其他索引在结构上没什么不同，因此列col2的索引分布和col1一样。主键索引就是一个名为PRIMARY的唯一非空索引。 InnoDB的数据分布可以看到该图显示了整张表，而不只有索引，因为在InnoDB中，聚簇索引就是表，所以不需要像MyISAM那样需要独立的行存储。聚簇索引的每一个叶子节点都包含了主键值、事务ID、用于事务和MVCC的回滚指针以及所有的剩余列。 还有一点与MyISAM不同的是，InnoDB的二级索引和聚簇索引很不相同。InnoDB二级索引的叶子节点中存储的不是“行指针”，而是主键值，并以此作为指向行的“指针”。这样的好处就是减少了当出现行移动或者数据页分裂时二级索引的维护工作，因此InnoDB在移动行时无需更新二级索引的这个“指针”。下图是InnoDB中的二级索引分布： 聚簇和非聚簇对比如图： 在InnoDB表中按主键顺序插入行如果正在使用的InnoDB表没有什么数据需要聚集，那么可以定义一个与数据无关的代理键作为主键，最简单方法就是使用AUTO_INCREMENT自增列，这样可以保证数据行是按顺序写入，对于根据主键做关联的性能也会更好。最好避免随机的聚簇索引，下面对比以自增的整数ID作为主键和以随机字符串UUID作为主键的情况。可以注意到UUID主键插入不仅花费时间更长，而且索引占用的空间也更大。这一方面是由于主键字段更长，另一方面毫无疑问是因为页分裂和碎片导致的。如图所示，因为主键的值是顺序的，所以InnoDB把每一条记录都存储在上一条记录的后面。当达到页的最大填充因子时（InnoDB默认的最大填充因子是页大小的15/19，留出部分空间用于以后修改），下一条记录就会写入新的页中。一旦数据按照这种顺序的方式加载，主键页就会近似于被顺序的记录填满，这也正是所期望的结果。 但是对比使用UUID聚簇索引的表插入数据，因为新行的主键值不一定比之前的插入大，所以InnoDB无法简单的总是把新行插入到索引的最后，而是需要为新的行寻找合适的位置————通常是已有数据的中间位置————并且分配空间。这会增加很多的额外工作，并导致数据分布不够优化，下面是总结的一些缺点： 写入到目标页可能已经刷到磁盘上并从缓存中移除，或者是还没有被加载到缓存中，InnoDB再插入之前不得不先找到并从磁盘读取目标页到内存中，这将导致大量的随机I/O。 因为写入是乱序的，InnoDB不得不频繁的做分页操作，以便为新的行分配空间。页分裂会导致移动大量数据，一次插入最少需要修改三个页面而不是一个页面。 由于频繁的页分裂，页会变得稀疏并被不规则地填充，所以最终数据会有碎片。 所以总结结论就是，使用InnoDB时应该尽可能的按主键顺序插入数据，并且尽可能地使用单调增加的聚簇键的值来插入新行。 索引覆盖索引确实是一种查找数据的高效方式，但是MySQL也可以使用索引来直接获取列的数据，这样就不再需要读取数据行。如果一个索引包含（或者说覆盖）所有需要的字段的值，我们就称之为“索引覆盖”。索引覆盖是非常有效的工具，能够极大地提高性能，如果查询只需要扫描索引而无需回表，会带来多少好处： 索引条目通常远小于数据行的大小，所以如果只需要读取索引，那MySQL就会极大的减少数据访问量。这对缓存的负载非常重要，因为这种情况下响应时间大部分花费在数据拷贝上。 因为索引是按照列值的顺序存储的，所以对于I/O密集型的范围查找会比随机从磁盘读取每一行的I/O要少得多。对于MyISAM甚至可以通过OPTIMIZE命令使得索引完全顺序排列，这让简单的范围查询能完全顺序的索引访问。 一些存储引擎存储引擎入MyISAM在内存中只缓存索引，数据则依赖于操作系统来缓存，因此访问数据需要一次系统调用，所以这里减少了因调用系统导致的开销。 由于InnoDB二级索引在叶子节点中保存了行的主键值，所以如果二级主键能够覆盖查询，则可以避免主键索引的二次查询。 不是所有类型的索引都可为成为覆盖索引，覆盖索引必须存储索引列的值，而哈希索引、空间索引和全文索引等都不存储索引列的值，所以MySQL只能使用B-Tree索引做覆盖索引。另外也不是所有的索引都支持覆盖索引。 当发起一个被索引覆盖的查询（也叫做索引覆盖查询）时，在EXPLAIN的Extra列可以看到“Using index”的信息。索引覆盖查询还有很多陷阱可能导致无法实现优化。MySQL查询优化器会在执行查询前判断是否有一个索引能进行覆盖。加入索引覆盖了WHERE条件中的字段，但是不是整个查询涉及的字段。如果条件为假，MySQL5.5及更早总是会回表获取数据行，尽管并不需要这一行且最终会被过滤掉。 如下例子：这里索引无法覆盖该查询，有两个原因： 没有任何索引能够覆盖这个查询，因为查询从表中选择了所有的列，而每一任何索引覆盖了所有列。 MySQL不能在索引中执行LIKE操作。这是底层存储引擎API的限制，MySQL5.5及更早版本只允许在索引中做简单比较操作。MySQL能在索引中做最左前缀匹配的LIKE比较，因为该操作可以转换为简单的比较操作，但是如果时通配符开头的LIKE查询，存储器引擎就无法做比较匹配。 也有办法解决上面说的问题，需重写查询并巧妙的设计索引。先将索引拓展至覆盖三个数据列，然后按如下方式重写查询：这种方式叫做“延迟关联”，因为延迟了对列的访问。在查询的第一阶段MySQL可以使用覆盖索引，在FROM子句的子查询中找到匹配的prod_id，然后根据这些prod_id值在外层查询匹配获取需要的所有列值。虽无法完全使用覆盖查询，但总比不使用覆盖查询好。 这样的优化效果取决于WHERE条件匹配返回的数据行。在示例3中，因为索引过滤时符合第一个条件的结果集已经很小，所以子查询带来的成本反而比从表中直接提取完整行更高。 可以考虑更进一步优化InnoDB：InnoDB的二级索引的叶子节点都包含了主键的值，这意味着InnoDB的二级索引可以有效地利用这些“额外”的主键列来覆盖查询。 使用索引扫描扫描来做排序MySQL有两种方式可以生成有序的结果：通过排序操作或者按索引顺序扫描。如果EXPLAIN出来的type列的值为“index”，则说明MySQL使用了索引扫描来做排序。扫描索引本身是很快的，但是如果索引不能覆盖所需的全部列，那就不得不每扫描一条索引记录都回表查询一次对应的行，这基本上都是随机I/O，因此按索引顺序读取数据的速度通常要比顺序地全表扫描。 MySQL可以使用同一个索引既满足排序又用于查找行，因此，如果可能，设计索引时应该尽可能地满足这两种任务。只有当索引的列顺序和ORDER BY子句的顺序完全一致，并且所有列的排序方向都一样时，MySQL才能使用索引来对结果做排序。**如果查询时需要关联多张表，则只有当ORDER BY子句引用的字段全部为第一个表时，才能使用索引来做排序，，ORDER BY子句和查找型查询的限制是一样的：需要满足索引的最左前缀的要求；否则MySQL都需要执行排序操作，而无法利用索引排序。有一种情况下ORDER BY子句可以不满足索引的最左前缀的要求：前导列为常量的时候。**如果WHERE子句或者JOIN子句种对这些列指定了常量，就可以弥补索引的不足。例如有如下表： 12345678910(retal_date,inventory_id,customer_id):CREATE TABLE retal( ... PRIMARY KEY (retal_id), UNIQUE KEY retal_date(retal_date,inventory_id,customer_id), KEY idx_fk_inverntory_id(inventory_id), KEY idx_fk_customer_id(customer_id), KEY idx_fk_staff_id(staff_id), ...); MySQL可以在使用retal_date索引为下面的查询做排序，从EXPLAIN中可以看到没有出现文件排序（filesort）操作： 123EXPLAIN SELECT retal_id,staff_id FROM sakila.rental WHERE retal_date = '2005-05-25' ORDER BY inventory_id,customer_id\G 即使ORDER BY子句不满足索引的最左前缀要求，也可以用于查询查询排序，这是因为索引的第一列被指定为一个常数。还有更多可以使用索引做排序的查询示例。下面这个查询可以利用查询排序，是因为查询为索引的第一列提供了常量条件，而使用第二列进行排序，将两列组合在一起，就形成了索引的最左前缀： 1...WHERE retal_date = '2005-05-25' ORDER BY inventory_id DESC; 下面这个查询也可以，因为ORDER BY使用的两列就是索引的最左前缀： 1...WHERE retal_date &gt; '2005-05-25' ORDER BY retal_date,inventory_id; 下面是一些不能使用索引做排序的查询： 查询使用两种不同的排序方向，但是索引列都是正序排序的。 查询的ORDER BY子句中引用了一个不在索引中的列。 查询的WHERE 和ORDER BY中的列无法组合成索引的最左前缀。 查询在索引列的第一列上使范围条件，MySQL无法使用索引的其余列。 在某一列上有多个等于条件，对于排序来说，这也是一种范围查询。 使用索引做排序的一个最重要的用法是当查询同时有ORDER BY和LIMIT子句的时候。 压缩（前缀压缩）索引MyISAM使用前缀压缩来减少索引的大小，从而让更多的索引可以放入内存中，这在某些情况下能极大的提高性能。默认只压缩字符串，也可以设置对整数压缩。MyISAM压缩每个索引块的方法是：先完全保存索引块中的第一个值，然后将其他值和第一个值进行比较得到相同前缀的字节数和剩余的不同后缀部分，把这部分存储起来即可。例如索引块中的第一个值是“perform”，第二个值是“performance”，那么这个值的前缀压缩后存储的是类似“7.ance”这样的形式，同样MyISAM对行指针也采用类似的前缀压缩方式。 压缩块使用更少的空间，代价是某些操作可能更慢。因为每个值的压缩前缀都依赖前面的值，所以MyISAM查找时无法在索引块使用二分查找而只能从头开始扫描。正序的扫描速度还不错，但是倒序就不是很好了。对于CPU密集型应用，因为扫描需要随机查找，压缩索引使得MyISAM在索引查找上要慢好几倍；但如果是I/O密集型应用，对某些查询带来的好处会比成本多很多。 冗余和重复索引MySQL允许在相同列上创建多个索引，MySQL需要单独与维护重复的索引，并且优化器在优化查询的时候也需要逐个地进行考虑，这会影响性能。 重复索引是指在相同的列上按照相同的顺序创建的相同类型的索引。应该避免这样创建重复索引。 冗余索引和重复索引有一些不同。如果创建了索引（A，B），在创建索引（A）就是冗余索引，因为这是前一个索引的前缀索引。因此索引（A，B）也可以当作索引（A）来使用。但是如果再创建索引（B，A）则不是冗余索引，因为B不是索引（A，B）的最左前缀列。 冗余索引通常发生在为表添加新索引的时候。例如，可能会增加一个新的索引（A,B）而不是扩展已有的索引（A），还有一种情况是将一个索引扩展为（A，ID），其中ID是主键，对于InnoDB来说主键列已经包含在二级索引中了，所以这也是冗余的。大多数情况下都不需要冗余索引，应该尽量扩展已有的索引而不是创建新索引。但也有时候出于性能方面考虑需要冗余索引，因为扩展已有的索引会太大导致影响性能。 有多个索引的缺点是索引成本更高，表中的索引越多插入速度会越慢，一般来说，增加新索引将导致INSERT、UPDATE、DELETE等操作速度变慢。解决冗余索引和重复索引的方法很简单，删除这些索引就可以。 未使用的索引除了冗余索引和重复索引，可能还会有一些服务器永远不用的索引，这样的索引完全是累赘，建议考虑删除。 索引和锁索引可以让查询锁定更少的行。InnoDB只有在访问行的时候才会对其进行加锁，而索引能够减少InnoDB访问的行数，从而减少锁的行数，但这只有当InnoDB在存储引擎层能够过滤掉索引不需要的行时才有效。 关于InnoDB、索引和锁有一些细节很少有人知道：InnoDB在二级索引上使用共享（读）锁，但访问主键索引需要排他（写）锁。这消除使用覆盖索引的可能性，并且使得SELECT FOR UPDATE比LOCK IN SHARE MODE或非锁定查询要慢的多。 总结在MySQL中，大多数情况下都会使用B-Tree索引。其他类型的索引大多只适用于特殊的目的。如果在合适的场景中，将大大提高查询的响应时间。在选择索引和编写利用这些索引的查询时有如下三个原则始终需要记住： 单行访问是很慢的。特别是在机械硬盘存储中，如果服务器从存储中读取一个数据块只是为了获取其中一行，那么就浪费了很多工作。最好读取的块中能包含尽可能多所需要的行。使用索引可以创建位置引用以提升效率。 按顺序访问范围数据是很快的，这有两个原因：第一，I/O不需要多次磁盘寻道，所以比随机I/O要快很多；第二，如果服务器能够按需要顺序读取数据，那么就不再需要额外的排序操作，并且GROUP BY查询也无须再做排序和将行按组进行聚合计算了。 索引覆盖查询是很快的。如果一个索引包含了查询的所有列，那么存储引擎就不需要再回表查找行，这避免了大量的单行访问。]]></content>
      <categories>
        <category>MySQL数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Schema与数据类型优化]]></title>
    <url>%2Fposts%2F64582%2F</url>
    <content type="text"><![CDATA[选择优化的数据类型MySQL支持的数据类型非常多，选择正确的数据类型对于获得高性能至关重要。不管存储哪种类型的数据库，下面几个简单的原则都有助于做出更好的选择： 更小的通常更好：一般情况下，应该尽量使用可以正确存储数据的最小数据类型。更小的数据类型通常更快，占用更少的磁盘、内存和CPU缓存，处理时需要的CPU周期更少。但是要在保证没有低估需要存储的值的范围的前提下，如果无法确定哪个数据类型是最好的，就选择你认为不会超过范围的最小类型。 简单就好：简单数据类型的操作通常需要更少的CPU周期。有两个例子：一个应该使用MySQL内建的类型而不是字符串来存储日期和时间；另外一个是应该用整型存储IP地址。 尽量避免NULL：很多表都包含可唯NULL的列，这是因为NULL是列的默认属性。通常情况下最好指定列为NOT NULL，除非真的需要存储NULL值。因为如果查询中包含可为NULL的列，对MySQL来说更难优化，因为可为NULL的列使得索引、索引统计和值比较都复杂。可为NULL的列会使用更多的存储空间，在MySQL中也需要特殊处理。当可为NULL的列被索引时，每个索引记录需要一个额外字节，在MyISAM里甚至还可能会导致固定大小的索引变成可变大小的索引。 在为列选择数据类型时，第一步需要确定合适的大类型：数字、字符串、时间等，下一步是选择具体类型，很多MySQL的数据类型可以存储相同类型的数据，只是存储的长度和范围不一样、允许的精度不同，或需要的物理空间不同等等。 例如DATETIME和TIMESTAMP都可以存储时间和日期，精确到秒，然而TIMESTAMP只是用DATETIME一半的存储空间，并且会根据时区变化，具有特殊的自动更新能力。但是另一方面，TIMESTAMP允许的时间范围要小得多。 整数类型有两种类型的数字：整数和实数，如果存储整数，可以使用这几种整数类型：TINYINT、SMALLINT、MEDIUMINT、INT和BIGINT，分别使用8、16、24、32、64位存储空间。 整数类型又可选的UNSIGNED属性，表示不允许负值，这大致可以使正数的上限提高一倍。例如TINYINT的存储范围是-128 ~ 127，而TINYINT UNSIGNED可以存储的范围是0 ~ 255。 有符号和无符号类型使用相同的存储空间，并且具有相同的性能。整数计算一般使用64位的BIGINT整数，即使在32位环境下也是如此。 MySQL可以为整数类型指定宽度，但是对于大多数应用没有意义:它不会限制值的合法范围，只是规定了MySQL一些交互工具用来显示字符的个数。因此对于存储和计算来说，INT(1)和INT(20)是相同的。 实数类型实数是带有小数部分的数字。但是它们不只是为了存储小数部分，也可以使用DECIMAL存储比BIGINT还大的整数。MySQL既支持精确类型，也支持不精确类型。 FLOAT和DOUBLE类型支持使用标准的浮点运算进行近似计算。 DECIMAL类型用于存储精确的小数，支持精确计算。因为CPU不支持对DECIMAL的直接计算，所以MySQL服务器自身实现了DECIMAL的高精度计算，相对而言，CPU直接支持原生浮点计算，所以浮点运算明显更快。 浮点和DECIMAL类型都可以指定精度。对于DECIMAL列，可以指定小数点前后所允许的最大位数，这会影响列的空间消耗。MySQL中将数字打包到一个二进制字符串中（每4个字节存9个数字），例如：DECIMAL（18，9）小数点两边各存储9个数字，一共使用9个字节：小数点前的数字同4个字节，小数点后的数字用4个字节，小数点本身占1个字节。DECIMAL类型允许最多65个数字，因为DECIMAL只是一种存储格式，在计算中DECIMAL会转换为DOUBLE类型。 浮点类型在存储同样范围的值时，通常比DECIMAL使用更少的空间。FLOAT使用4个字节存储；DOUBLE占用8个字节，相比FLOAT有更高的精度和更大的范围。 因为需要额外的空间和计算开销，所以应该尽量只在对小数进行精确计算时才使用DECIMAL，例如存储财务数据。在数据量比较大的时候，可以考虑使用BIGINT代替DECIMAL，将需要存储的货币单位根据小数的位数乘以相应的倍数即可。 字符串类型VARCHAR和CHAR两种最主要的字符串类型，存储的具体方式与存储引擎的实现有关，所以下面描述的内容，假设存储引擎是InnoDB或者MyISAM。 VARCHAR类型VARCHAR用于存储可变长字符串，是最常见的字符串数据类型。它更加节省空间，因为它仅使用必要的空间（即越短的字符串使用越少的空间）。但是如果表使用ROW_FORMAT = FIXED创建的话每一行都会使用定长存储，这样很浪费空间。 VARCHAR需要使用1或2个额外字节记录字符串的长度：如果列的最大长度小于或等于255字节，则只使用1个字节表示，否则使用2个字节。 由于行是变长的，在UPDATE时可能使行变得更长，这就导致需要做额外的工作。如果一个行占用的空间增长，并且在页内没有更多的空间可以存储，MyISAM会将行拆成不同的片段存储，InnoDB则需要分裂页来使行可以放进页内。 下面这些情况下使用VARCHAR是合适的：字符串列的最大长度比平均长度大很多；列的更新很少，所以碎片不是问题；使用了像UTF-8这样复杂的字符集，每个字符都使用不同的字节数进行存储。 MySQL在存储和检索时会保留末尾空格，但是在4.1或者老版本中则会剔除末尾空格。InnoDB则更为灵活，它可以把过长的VARCHAR存储为BLOB。 CHAR类型CHAR类型是定长的：MySQL总是根据定义的字符串长度分配足够的空间。当存储CHAR值时，MySQL会删除所有的末尾空格。 CHAR适合存储很短的字符串，或者所有值都接近同一个长度。对于经常变更的数据，CHAR也比VARCHAR更好，因为定长的CHAR类型不容易产生碎片 与CHAR和VARCHAR类似的类型还有BINARY和VARBINARY，它们存储的是二进制字符串，二进制字符串存储的是字节码而不是字符，填充也不一样：MySQL填充BINARY采用的是“\0”而不是空格，在检索时也不会去掉填充。二进制的优势并不仅仅体现在大小写敏感上，MySQL比较BINARY字符串时，每次按一个字节，并且根据该字节的数值进行比较。因此，二进制比较比字符比较简单很多，所以也就更快。 在使用VARCHAR时，更长的列会消耗更多的内存，因为MySQL通常会分配固定大小的内存块来保存内部值。尤其是使用内存临时表进行排序或操作是会特别糟糕，所以最好的策略是只分配真正需要的空间。 BLOB和TEXT类型BLOB和TEXT都是为存储很大的数据而设计的字符串数据类型，分别采用二进制和字符方式存储。字符类型是TINYTEXT、SMALLTEXT、TEXT、MEDIUMTEXT和LONGTEXT；二进制类型是TINYBLOB、SMALLBLOB、BLOB、MEDIUMBLOB、LONGBLOB。BLOB是SMALLBLOB的同义词，TEXT是SMALLTEXT的同义词。 MySQL把每个BLOB和TEXT值当作一个独立的对象处理，存储引擎在存储时通常会做特殊处理，当BLOB和TEXT值太大时，InnoDB会使用专门的“外部”存储区域来进行存储，此时每个值在行内需要1~4个字节存储一个指针，然后再外部存储区域存储实际的值。 BLOB没有排序规则或字符集，而TEXT类型有字符集和排序规则。MySQL对BLOB和TEXT列进行排序与其他类型是不同的：它只对每个列的最前面max_sort_length字节而不是整个字符串做排序。如果只需要排序前面一小部分的字符，则可以减小max_sort_length的配置，或者使用ORDER BY SUBSTRING(column,length)。 MySQL不能将BLOB和TEXT列全部长度的字符串进行索引，也不能使用这些索引消除排序。 使用枚举（ENUM）代替字符串类型有时可以使用枚举代替常用的字符串类型。枚举列可以把一些不重复的字符串存储成一个预定义的集合。MySQL在存储枚举时非常紧凑，会根据列表值的数量压缩到一个或者两个字节中。 MySQL在内部会将每个值在列表中的位置保存为整数，所以实际存储为整数，而不是字符串。所以使用数字作为ENUM枚举常量，这种双重性很容易导致混乱。而且枚举字段是按照内部存储的整数而不是定义的字符串进行排序的。因此一种绕过这种限制的方式是按照需要的顺序来定义枚举列，另外也可以在查询中使用FIELD()函数显示地指定排序顺序，但是这会导致MySQL无法利用索引消除排序。 枚举最不好的地方就是：字符串列表是固定的，添加或删除字符串必须使用ALTER TABLE，除非能接受只在列表末尾添加元素。 因此一个通用的设计实践：在查找表时采用整数主键而避免采用基于字符串的值进行关联。ENUM可以让表的大小缩小，因此它可以更加节省内存。 日期和时间类型MySQL能存储的最小时间粒度为秒，但是MySQL也可以使用微秒级的粒度进行临时运算：可以使用BIGINT类型存储微秒级别的时间戳，或者使用DOUBLE存储秒之后的小数部分。 MySQL提供两种相似的日期类型：DATETIME和TIMESTAMP。 DATETIME：这个类型能保存大范围的值，从1001到9999年，精度为秒。它把日期和时间封装到格式为YYYYMMDDHHMMSS的整数中，与时区无关。使用8个字节的存储空间。默认情况下，MySQL以一种可排序的、无歧义的格式显示DATETIME值。 TIMESTAMP：TIMESTAMP类型保存了从1970年1月1日以来的秒数，他和UNIX时间戳相同。TIMESTAMP只使用4个字节的存储空间，因此它的范围比DATETIME小得多：只能从1970年到2038年。**MySQL按照DATETIME的方式格式化TIMESTAMP的值，但是这仅仅是显示格式上的区别，TIMESTAMP的存储格式在各个版本都是一样的。TIMESTAMP显式的值也依赖于时区。默认情况下，如果插入时没有指定第一个TIMESTAMP列的值，MySQL则设置这个列的值为当前时间，再插入一行记录时，MySQL默认也会更新第一个TIMESTAMP列的值。TIMESTAMP列默认为NOT NULL。** 除了特殊行为之外，通常也应该尽量使用TIMESTAMP，因为它比DATETIME空间效率更高。 位数据类型MySQL有少数几种存储类型使用紧凑的位存储数据，所以这些存储类型，不管底层存储格式和处理方式如何，从技术上讲都是字符串类型。 BIT：可以使用BIT列在一列中存储一个或多个true/false值。BIT(1)定义一个包含单个位的字段，BIT(2)存出2个位，以此类推。BIT列的最大长度位64个位。BIT在MyISAM中会打包存储所有的BIT列，所以更加节省空间；但是在InnoDB中，为每个BIT列使用一个足够存储的最小整数类型来存放，所以不能节省空间。MySQL把BIT当作字符串类型，而不是数字类型。所以对于大部分应用，最好避免使用这种类型。 SET：它在MySQL内部以一系列打包的位的集合来表的，这样就有效的利用了存储空间。可以使用一个整数包装一系列的位。 选择标识符(identifier)为标识列选择合适的数据类型非常重要。一般来说更有可能用标识列与其他值进行比较，或者通过标识列寻找其他列。 整数类型：整数通常是标识列最好的选择，因为它们很快并且可以使用AUTO_INCREMENT。 ENUM和SET：对于标识列来说，ENUM和SET类型通常是一个糟糕的选择，所以大部分情况下都要避免这么做。 字符串类型：应该尽量避免使用字符串类型作为标识列，因为它们很消耗空间，并且通常比数字类型慢。 特殊数据类型人们通常使用VARCHAR（15）列来存储IP地址，然而，它们实际上是32位无符号整数，不是字符串。用小数点将地址分成四段的表示方法只是为了让人们容易阅读，所以应该用无符号整数存储IP地址。MySQL提供INET_ATON()和INET_NTOA()函数在这两种表示方法之间转换。 Schema设计中的陷阱有一些问题是由MySQL的实现机制导致的，所以这里讨论一下MySQL的schema的设计上的问题。 太多的列：MySQL的存储引擎API工作时需要在服务器层和存储引擎层之间通过行缓冲格式拷贝数据，然后再服务器曾将缓冲解码成各个列。从行缓冲中将编码过的列转换成行数据结构的操作代价非常高，转换的代价依赖于列的数量。如果一个非常宽的表，然而只有一小部分列会实际用到，这时转换的代价就非常高。 太多的关联：关联操作会导致MySQL解析和优化查询的代价非常高。MySQL限制了每个关联操作最多只能有61张表，如果希望查询执行得快速且并发性能好，单个查询最好在12个表以内做关联。 全能的枚举：注意防止过度使用枚举（ENUM）。 变相的枚举：枚举列允许在列中存储一组定义值中的单个值，集合列则允许在列中存储一组定义值中的一个或多个值，有时候这可能比较容易导致混乱。 非此发明的NULL：之前说过建议尽可能地考虑替代NULL的方案，例如可以使用0、某个特殊值或空字符串作为替代。但是遵循这个原则也不要走极端，当确实需要表示未知值时也不要害怕使用NULL。值得一提的是，MySQL会在索引中存储NULL值，而Oracle不会。 范式和反范式范式的优点和缺点优点： 范式化的更新操作通常比反范式化要快。 当数据较好地范式化时，就只有很少或者没有重复度的数据，所以只需要修改更少的数据。 范式化的表通常更小，可以更好地放在内存里，所以执行操作会更快。 很少有多余的数据意味着检索列表数据会更少需要DISTINCT或者GROUP BY语句，而在非范式化的结构中必须使用DISTINCT或者GROUP BY才能获得一份唯一的列表。 缺点： 范式化的设计表的缺点是通常需要关联，这不但代价昂贵，也可能使一些索引策略无效。 反范式的优点和缺点优点： 反范式化的schema因为所有数据都在一张表中，可以更好地放在内存里，所以执行操作会更快。 如果不需要关联表，则对大部分查询最差的情况————即使表没有索引————是全表扫描。可以更好地放在内存里，所以执行操作会更快。 单独的表也能使用更有效的索引策略。 缺点见范式化的优点部分。 混用范式化和反范式化完全的范式化和完全的反范式化schema都是实验室才有的东西，在真实世界中很少会这么极端地使用。在实际应用中经常需要混用。 最常见的反范式化数据的方法是复制或者缓存，在不同的表中存储相同的特定列。在MySQL5.0和更新版本中，可以使用触发器更新缓存值。 缓存表和汇总表有时提升性能最好的方法是在同一张表中保存衍生的冗余数据，然而有时也需要创建一张完全独立的汇总表或缓存表（特别是为满足检索到需求时）。 我们用术语“缓存表”来表示存储那些可以简单地从schema其他表获取、但是每次获取的速度比较慢的数据的表；而术语“汇总表”则保存的是使用GROUP BY语句聚合数据的表。 实时计算统计值是很昂贵的操作，因为要么需要扫描表中大部分数据，要么查询语句只能在某些特定的索引上才能有效运行，而这类特定索引一般会对UPDATE操作有影响，所以一般不希望创建这样的索引。而缓存表则相反，其对优化搜索和检索查询语句很有效。 一个有用的技巧是对缓存表使用不同的存储引擎。例如，如果主表使用InnoDB，用MyISAM作为缓存表的引擎将会得到更小的索引占用空间，并且可以做全文搜索。 在使用缓存表和汇总表时，必须决定是实时维护数据还是定期重建。哪个更好依赖于应用程序，但是定期重建并不只是节省资源，也可以保持表不会有很多碎片，以及有完全顺序组织的索引，这会更高效。 当重建汇总表和缓存表时，通常需要保证数据在操作时依然可用，这就需要通过使用“影子表”来实现。“影子表”指的是一张在真实的表背后创建的表，当完成了建表操作后，可以通过一个原子的重命名操作切换影子表和原表。而且如果新表有问题，则可以很容易地进行快速回滚操作。 物化视图物化视图实际上是预先计算并且存储在磁盘上的表，可以通过各种各样的策略刷新和更新。在第七章会详细探讨物化方法。 计数器表如果在应用中保存计数器，则在更新计数器时可能碰到并发问题。这时可以创建一张独立的表存储计时器，这样可使计数器表小并且快，使用独立的表可以避免查询缓存失效。 然而问题在于，对于任何想要更新这一行的事务来说，这条记录都有一个全局的互斥锁。这会使这些事务只能串行执行。要获得更高的并发性能，也可以将计数器保存在多行中，每次随机选择一行进行更新操作。 更快的读，更慢的写为了提升读查询的速度，经常会需要建一些额外索引，增加冗余列，甚至是创建缓存表和汇总表。这些方法会增加写查询到负担，但是虽然写操作变慢了，但更显著地提高了读操作的性能。 加快ALTER TABLE操作的速度MySQL执行大部分修改表结构操作的方法是用新的结构常见一个空表，从旧表中查出所有数据插入新表，然后删除旧表。问题是这样的操作可能需要很长时间。 一般而言，大部分ALTER TABLE操作将导致MySQL服务中断，但是能使用两种技巧进行改善： 一种是先在一台不提供服务的机器上执行ALTER TABLE操作，然后和提供服务端主库进行切换。 另外一种是“影子拷贝”，是用要求的表结构创建一张和源表无关的新表，然后通过重命名和删表操作交换两张表。 不是所有的ALTER TABLE操作都会引起表重建。例如修改一个列的默认值： 12ALTER TABLE filmMODIFY COLUMN rental_duration TINYINT(3) NOT NULL DEFAULT 5; 这个语句会拷贝整张表到一张新表，但是甚至列的类型、大小和可否为NULL属性都没改变。因为所有的MODIFY COLUMN 操作都会导致表重建。另一种方法是通过ALTER COLUMN操作来改变列的默认值： 12ALTER TABLE filmALTER COLUMN rental_duration SET DEFAULT 5; 这个语句会直接修改.frm文件而不涉及表数据。所以这个操作是非常快的。 只修改.frm文件从上面的例子可以看出修改表的.frm文件是很快的。下面这些操作是有可能不需要重建表的： 移除（不是增加）一个列的AUTO_INCREMENT属性。 增加、溢出，或更改ENUM和SET常量，如果移除的是已有行数据用到其值的变量，查询将会返回一个空字符串。 基本的技术是为想要的表结构创建一个新的.frm文件，然后用它替换掉已经存在的那张表的.frm文件。 快速创建MyISAM索引为了高效地载入数据到MyISAM表中，有一个常用的技巧是：先禁用索引、载入数据，然后重新启用索引。 因为构建索引的工作被延迟到数据完全载入之后，这个时候已经可以通过排序来构建索引饿了。这样做会快很多，并且使得索引树的碎片更少、更紧凑。 不幸的是，这个方法对唯一索引无效，因为DISABLE KEYS只对非唯一索引有效。 因此在InnoDB中有一个类似的技巧：先删除所有非唯一索引，然后增加新的列，最后重新创建删除掉的索引。Percona Server可以自动完成这些操作步骤。 总结总之来说，尽可能保持任何东西小而简单总是好的。MySQL喜欢简单，需要使用数据库的人也会同样喜欢简单的原则： 尽量避免过度设计。例如会导致极其复杂查询的schema设计，或者有很多列的表设计。 使用小而简单的合适数据类型，除非真实数据模型中有确切需要，否则应该尽可能地避免使用NULL。 尽量使用相同的数据类型存储相似或相关的值，尤其是要在关联条件中使用的列。 注意可变长字符串，其在临时表和排序时可能导致悲观的按最大长度分配内存。 尽量使用整型定义标识列。 避免使用MySQL已经遗弃的特性，例如指定浮点数的精度，或者整数的显示宽度。 小心使用ENUM和SET，最好避免使用BIT]]></content>
      <categories>
        <category>MySQL数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[LinkedList源码阅读]]></title>
    <url>%2Fposts%2F44983%2F</url>
    <content type="text"><![CDATA[LinkedList的数据结构LinkedList的底层数据结构是基于双向链表实现的，如图：因为链表在物理内存上可以是不连续的，所以理论上，只要计算机的内存足够大的情况下，LinkedList的长度可以无限长。 顶部注释 双向链表实现了List和Deque接口。 实现所有可选列表操作，并允许所有元素（包括null ）。所有的操作都能像双向列表一样预期。 索引到列表中的操作将从开始或结束遍历列表，以更接近指定的索引为准。 请注意，此实现不同步。 如果多个线程同时访问链接列表，并且至少有一个线程在结构上修改列表，则必须在外部进行同步。 （结构修改是添加或删除一个或多个元素的任何操作;仅设置元素的值不是结构修改。）这通常通过在自然封装列表的对象上进行同步来实现。 如果没有这样的对象存在，列表应该使用Collections.synchronizedList方法“包装”。 这最好在创建时完成，以防止意外的不同步访问列表：List list = Collections.synchronizedList(new LinkedList(...)); 该类的iterator和listIterator方法返回的迭代器是fail-fast的 ：如果列表在迭代器创建后的任何时间进行结构修改，除了通过Iterator自己的remove或add方法外，迭代器将抛出一个ConcurrentModificationException 。 因此，面对并发修改，迭代器将快速而干净地失败，而不是在未来未确定的时间冒着任意的非确定性行为。 请注意，迭代器的故障快速行为无法保证，因为一般来说，在不同步并发修改的情况下，无法做出任何硬性保证。 失败快速的迭代器ConcurrentModificationException扔掉ConcurrentModificationException 。 因此，编写依赖于此异常的程序的正确性将是错误的： 迭代器的故障快速行为应仅用于检测错误。 从上面LinkedList的源码顶部注释可以总结以下几点： 底部实现：双向链表。 是否允许null值：允许所有元素，包括null。 线程安全：线程不安全。 迭代器：迭代器是fast-fail，但是迭代器的快速失败行为不能得到保证。 运行时间：因为是链表的实现，所以任何需要查询到操作都需要遍历链表，即O(n)的时间。 LinkedList的定义123public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable LinkedList：支持泛型的存储模式。 extends AbstractSequentialList：继承于AbstractSequentialList，继承了其中的方法，方便操作。 implements List：实现了List接口，实现该接口提供的方法，方便了实现。 implements Deque：实现了双端队列的接口，因此双向链表操作起来更方便。 implements Cloneable：实现了Cloneable接口，内部可以调用clone()方法来返回实例的浅拷贝(shallow copy)。 implements Serializable：实现了Serializable接口，表明该类是可以序列化的。 全局变量12345678910111213141516171819202122232425262728293031//记录元素个数的常量transient int size = 0;//静态内部类Node，即存储每个数据的节点private static class Node&lt;E&gt; &#123;/*** 内存保存的内容有三个，分别是：* item:储存的元素* next:该节点后面一个节点* prev:该节点前面一个节点*/ E item; Node&lt;E&gt; next; Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125; &#125; /** * 指向第一个节点的指针。 */ transient Node&lt;E&gt; first; /** * 指向最后一个节点的指针。 */ transient Node&lt;E&gt; last; 构造方法LinkedList只有两个构造方法： 12345678910111213141516/** * 构造一个空列表。 */ public LinkedList() &#123; &#125; /** * 构造包含指定集合的元素的列表，按集合的迭代器返回元素的顺序排列。 * * @param c 要将其元素放入此列表的集合 * @throws NullPointerException 如果指定的集合为空 */ public LinkedList(Collection&lt;? extends E&gt; c) &#123; this(); addAll(c); &#125; 可以从上面的代码中看出，两个构造方法分别是无参方法和传入一个集合的方法。 无参方法的方法体为空，因为是链表结构，而链表的头指针和尾指针已经在上面定义过了，而且不同于数组结构，不需要指定大小和扩容，所以方便很多，因此无参方法什么都不需要做。 传入一个集合的构造方法使用addAll()方法将传入的集合按顺序添加到链表的尾部，addAll()方法可以的具体实现可以看下面的核心方法部分。 核心方法getFirst和getLast方法123456789101112/** * 返回列表中的第一个元素。 * * @return 列表中的第一个元素 * @throws NoSuchElementException 如果这个列表是空的 */public E getFirst() &#123; final Node&lt;E&gt; f = first; if (f == null) throw new NoSuchElementException(); return f.item;&#125; 根据代码可以看出，获取到first节点，如果列表中没有元素，所以列表是空的，first节点自然是null，所以抛出NoSuchElementException异常；否则返回first节点的数据。 123456789101112/** * 返回列表中的最后一个元素。 * * @return 列表中的最后一个元素 * @throws NoSuchElementException 如果这个列表是空的 */public E getLast() &#123; final Node&lt;E&gt; l = last; if (l == null) throw new NoSuchElementException(); return l.item;&#125; 同getFirst方法很相似，获取到last节点的值，依旧判断last是否为空，为空则抛出异常；否则返回最后一个节点last的值。 removeFirst方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * 从列表中移除并返回第一个元素。. * * @return 列表中的第一个元素 * @throws NoSuchElementException 如果这个列表是空的 */public E removeFirst() &#123; final Node&lt;E&gt; f = first; if (f == null) throw new NoSuchElementException(); return unlinkFirst(f);&#125;/** * 断开第一个非空节点f的链接。 */private E unlinkFirst(Node&lt;E&gt; f) &#123; // 判断 f == first &amp;&amp; f != null; final E element = f.item; //头节点的下一个节点，当头节点移除后，它就是头节点 final Node&lt;E&gt; next = f.next; /** * 因为头节点是第一个节点，所以它的prev自然为null * 把头结点的值和next字段赋值为null，有助于GC回收 */ f.item = null; f.next = null; // 有助于 GC //然后把next作为头节点 first = next; /** * 如果头节点的下一个节点是null，那么说明列表中只有一个节点 * 因此把last也赋值为null，说明至此链表中已无元素 * * 否则把头结点的prev字段置为null，说明该节点是头节点 */ if (next == null) last = null; else next.prev = null; /** * 列表中元素数量减一 * 修改次数加一 * 返回被删除节点的值 */ size--; modCount++; return element;&#125; removeLast方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * 从列表中移除并返回最后一个元素。 * * @return 列表中的最后一个元素 * @throws NoSuchElementException 如果这个列表是空的 */public E removeLast() &#123; final Node&lt;E&gt; l = last; if (l == null) throw new NoSuchElementException(); return unlinkLast(l);&#125;/** * 断开最后非空的一个节点l。 */private E unlinkLast(Node&lt;E&gt; l) &#123; // 判断 l == last &amp;&amp; l != null; final E element = l.item; //尾节点的前一个节点，当尾节点移除后，它就是尾节点 final Node&lt;E&gt; prev = l.prev; /** * 因为尾节点是第一个节点，所以它的next自然为null * 把头结点的值和prev字段赋值为null，有助于GC回收 */ l.item = null; l.prev = null; // 有助于 GC //然后把prev作为头节点 last = prev; /** * 如果尾节点的前一个节点是null，那么说明列表中只有一个节点 * 因此把first也赋值为null，说明至此链表中已无元素 * * 否则把尾结点的next字段置为null，说明该节点是尾节点 */ if (prev == null) first = null; else prev.next = null; /** * 列表中元素数量减一 * 修改次数加一 * 返回被删除节点的值 */ size--; modCount++; return element;&#125; addFirst方法1234567891011121314151617181920212223242526272829303132333435363738/** * 在列表的开头插入指定的元素。 * * @param e 要添加的元素 */public void addFirst(E e) &#123; linkFirst(e);&#125;/** * 链接e作为第一个元素。 */private void linkFirst(E e) &#123; final Node&lt;E&gt; f = first; //调用Node的构造方法，定义一个prev字段为null、next字段为f的newNode节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f); //然后把newNode作为头节点 first = newNode; /** * 如果原来的头节点f为空的话，说明列表中原来没有任何元素 * 所以现在插入了一个新的节点，自然头节点和尾节点都是它，所以尾节点也是它。 */ if (f == null) last = newNode; else /** * 否则列表中原来不为空，只需要把新节点和原来的头节点连起来就好了 * 连起来的方法就是把原结点f的prev字段赋为现在的新头节点即可 * 即f节点是链表中第二个节点 */ f.prev = newNode; /** * 元素数量加一 * 修改次数加一 */ size++; modCount++;&#125; add和addLast方法add和addLast方法中同是调用了linkLast方法，将新的元素添加到链表的尾部，二者的唯一却别就是add方法添加成功的话，会返回true表示插入成功，而addLast方法则无返回值，在内部实现上无任何区别。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 将指定的元素追加到此列表的末尾。 * * &lt;p&gt;这个方法相当于 addLast() 方法 * * @param e 要附加到此列表中的元素 * @return true （由 Collection.add(E)指定） */public boolean add(E e) &#123; linkLast(e); return true;&#125; /** * 将指定的元素追加到此列表的末尾。 * * &lt;p&gt;这个方法相当于 add() 方法 * * @param e 要添加的元素 */public void addLast(E e) &#123; linkLast(e);&#125;/** * 链接e作为最后一个元素。 */void linkLast(E e) &#123; final Node&lt;E&gt; l = last; //定义一个前驱节点是last、后置节点是null的节点newNode，即将作为尾节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); //把刚刚新定义的节点作为新的尾节点 last = newNode; /** * 如果原来的尾节点是null的话，说明原来列表中无元素，所以first自然也为null * 所以这里新插入一个元素后，first和last节点同时指向仅有的一个元素节点 * 仅有的该元素节点既是头节点，也是尾节点 */ if (l == null) first = newNode; else /** * 如果原来列表中元素不为空，就把新的尾节点和旧的尾节点连接起来 * 方法就是把旧的尾节点的next字段置为新的尾节点，即l节点是倒数第二个节点 */ l.next = newNode; /** * 元素数量加一 * 修改次数加一 */ size++; modCount++;&#125; contains和indexOf方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * 如果此列表包含指定的元素，则返回true。更正式地说，当且仅当此列表包含至少一个元素e，使得Objects.equals(o, e)。 * * * @param o 其在此列表中的存在性将被测试的元素 * @return &#123;@code true&#125; 如果此列表包含指定的元素 */public boolean contains(Object o) &#123; /** * 内部则调用indexOf方法，判断该元素的下标是否大于等于零 * 下标大于等于零则表示元素存在，如果不存在，indexOf方法会返回-1 */ return indexOf(o) &gt;= 0;&#125;/** * 返回此列表中指定元素的第一个出现项的索引，如果该列表不包含该元素，则返回-1。 * 更正式地说，返回最低索引i满足Objects.equals(o, get(i))，如果没有这样的索引，则为-1。 * * * @param o 要搜索的元素 * @return 此列表中指定元素的首次出现的索引，如果此列表不包含元素，则为-1 */public int indexOf(Object o) &#123; int index = 0; /** * 因为LinkedList允许元素为null，所以要分开判断元素是null和非null的情况 * 因为null的判定方法是 == 号，而非null的元素则使用Objects.equals(o, get(i))方法 */ if (o == null) &#123; /** * 当要查找的元素是null的时候，从头节点first节点开始遍历 * 直到找到值为null或者节点为null为止，节点为null的也就是last之后的那个节点为止 */ for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; /** * 如果节点中的值为null，则返回其下标 * 否则继续向后遍历，下标加一 */ if (x.item == null) return index; index++; &#125; &#125; else &#123; /** * 当要查找的元素是非null的时候，从头节点first节点开始遍历 * 直到找到o.equals(x.item)或者节点为空为止，也就是last之后的那个节点为止 */ for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; /** * 如果节点中的值为与要查找的节点一致，则返回其下标 * 否则继续向后遍历，下标加一 */ if (o.equals(x.item)) return index; index++; &#125; &#125; //如果遍历过整个链表都没有找到符合要求的节点的话，就返回-1 return -1;&#125; size方法123456789 /** * 返回此列表中的元素数量。 * * @return 列表中元素的数量 */ public int size() &#123;//即返回记录链表中元素数量的遍历size return size; &#125; remove方法remove方法有三个重载方法，分别是无参方法、传入下标和传入指定对象的方法。 remove(Object o)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990/** * 从列表中删除指定元素的第一个出现项(如果存在)。如果这个列表不包含这个元素，它将保持不变。 * 更正式地说，删除索引i最低的元素，使得Objects.equals(o, get(i)) (如果存在这样一个元素)。 * 如果此列表包含指定的元素，则返回true(如果此列表由于调用而更改，则返回true)。 * * @param o 元素，如果存在，则从该列表中删除 * @return &#123;@code true&#125; 如果此列表包含指定的元素 */ public boolean remove(Object o) &#123; if (o == null) &#123; /** * 如果传入的对象是null，则遍历链表，找到第一个值为null的节点 * 然后调用unlink方法将其删除 */ for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (x.item == null) &#123; unlink(x); return true; &#125; &#125; &#125; else &#123; /** * 如果传入的对象是非null的，则遍历链表，找到第一个符合o.equals(x.item)的节点 * 然后调用unlink方法将其删除 */ for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (o.equals(x.item)) &#123; unlink(x); return true; &#125; &#125; &#125; //如果删除成功，则返回true，否则返回false。 return false; &#125; /** * 断开非空节点x的链接。 */ E unlink(Node&lt;E&gt; x) &#123; /** * 判断 x != null * 记录要删除的节点和其前驱节点和后置节点 */ final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; /** * 如果前驱节点是null的话，说明要删除的节点就是第一个节点 * 故删除后就把它的后置节点（也就是第二节点）作为新的头节点 * * 否则把前驱节点的下一个节点赋为当前节点的后一个节点 * 然后把当前节点的前驱节点置为null */ if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; /** * 如果后置节点是null的话，说明要删除的节点就是最后一个节点 * 故删除后就把它的前驱节点（也就是倒数第二个节点）作为新的头节点 * * 否则把后置节点的前一个节点赋为当前节点的前一个节点 * 然后把当前节点的后置节点置为null */ if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; /** * 经过上面两个if判断，当前节点的前驱节点和后置节点已经全部赋值为null * 并且已经把它的前驱节点和后置节点相互连接了 * 这里当前节点把值赋为null，就意味着删除了，然后等待GC回收即可 */ x.item = null; /** * 元素数量减一 * 修改次数加一 * 返回被删除的节点的值 */ size--; modCount++; return element; &#125; remove(int index)12345678910111213141516/** * 移除列表中指定位置的元素。将任何后续元素向左移动(从它们的索引中减去一个)。返回从列表中删除的元素。 * * @param index 要删除的元素的索引 * @return 先前位于指定位置的元素 * @throws IndexOutOfBoundsException 表示某种索引(如数组索引、收敛索引或向量索引)超出范围。 */public E remove(int index) &#123; /** * 这里先对传入的下标进行检查，判断是否越界 * 如果没有越界则使用node(index)方法找到指定位置的结点 * 然后使用unlink方法将这个节点删除 */ checkElementIndex(index); return unlink(node(index));&#125; 先看以下判断下标是否越界的checkElementIndex方法： 1234567891011121314private void checkElementIndex(int index) &#123; if (!isElementIndex(index)) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125;/** * checkElementIndex 方法实际调用了这个方法判断下标是否越界 * 如果越界则抛出异常 * * @param index * @return */private boolean isElementIndex(int index) &#123; return index &gt;= 0 &amp;&amp; index &lt; size;&#125; 在下标符合要求后，使用node方法查找到指定位置的节点： 1234567891011121314151617181920212223/** * 返回指定元素索引处的(非空)节点。 */Node&lt;E&gt; node(int index) &#123; /** * 这里先判断要查找的下标是在链表的前半部分还是后半部分 * 如果是在前半部分就从头指针开始查找 * 如果是在后半部分就从尾指针开始查找 */ if (index &lt; (size &gt;&gt; 1)) &#123; //在前半部分，从头指针开始查找 Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; //在后半部分，从尾指针开始查找 Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 在使用node方法找到指定下标位置的节点之后，使用unlink方法将该节点删除掉，unlink方法在上面remove(Object o)方法中有讲过。 remove()1234567891011/** * 检索并删除此列表的头部(第一个元素)。 * 该方法与removeFirst无差别，不多赘述，二者一模一样。 * * @return 这个列表的头 * @throws NoSuchElementException 如果这个列表是空的 * @since 1.5 */public E remove() &#123; return removeFirst();&#125; lastIndexOf方法12345678910111213141516171819202122232425262728/** * 返回此列表中指定元素的最后一次出现的索引，如果该列表不包含该元素，则返回-1。 * 更正式地说，返回最高索引i，满足Objects.equals(o, get(i))，如果没有这样的索引，则为-1。 * * * @param o 要搜索的元素 * @return 此列表中指定元素的最后一次出现的索引，如果该列表不包含该元素，则为-1 */public int lastIndexOf(Object o) &#123; /** * 这个方法实现indexOf方法唯一的区别就是：这个方法从后向前查找，而indexOf方法从前向后查找，其余部分无区别 */ int index = size; if (o == null) &#123; for (Node&lt;E&gt; x = last; x != null; x = x.prev) &#123; index--; if (x.item == null) return index; &#125; &#125; else &#123; for (Node&lt;E&gt; x = last; x != null; x = x.prev) &#123; index--; if (o.equals(x.item)) return index; &#125; &#125; return -1;&#125; toArray方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** * 返回一个数组，该数组按适当的顺序(从第一个元素到最后一个元素)包含列表中的所有元素。 * * 返回的数组将是“安全的”，因为这个列表不维护对它的引用。(换句话说，这个方法必须分配一个新的数组)。 * 因此，调用者可以自由地修改返回的数组。 * * 此方法充当基于数组和基于集合的api之间的桥梁。 * * @return 一个数组，按适当的顺序包含列表中的所有元素 */public Object[] toArray() &#123; /** * 实现方法就是先创建一个与size大小相等的数组 * 然后从头结点开始遍历整个链表，把链表中的每个节点的值存入数组中 * 最后把数组返回 */ Object[] result = new Object[size]; int i = 0; for (Node&lt;E&gt; x = first; x != null; x = x.next) result[i++] = x.item; return result;&#125;/** * 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）; * 返回的数组的运行时类型是指定数组的运行时类型。 * 如果列表适合指定的数组，则返回其中。 否则，将为指定数组的运行时类型和此列表的大小分配一个新数组。 * * * 如果列表适用于指定的数组，并有空余余地（即数组的列表null更多）， * 则紧跟在列表末尾的数组中的元素设置为null 。 （这仅在调用者知道列表不包含任何空元素的情况下才能确定列表的长度。） * * 像toArray()方法一样，此方法充当基于阵列和基于集合的API之间的桥梁。 * 此外，该方法允许精确地控制输出阵列的运行时类型，并且在某些情况下可以用于节省分配成本。 * * * 假设x是一个已知只包含字符串的列表。 以下代码可用于将列表转储到新分配的String数组中： * String[] y = x.toArray(new String[0]); * 请注意， toArray(new Object[0])功能与toArray()相同。 * * * @param a 要存储列表的元素的数组，如果它足够大; 否则，为此目的分配相同运行时类型的新数组。 * * @return 一个包含列表元素的数组 * @throws ArrayStoreException 如果指定数组的运行时类型不是此列表中每个元素的运行时类型的父类型 * @throws NullPointerException 如果指定的数组为空 */@SuppressWarnings("unchecked")public &lt;T&gt; T[] toArray(T[] a) &#123; //先检查传入的数组长度是否足够，如果不够就将数组扩容为size大小 if (a.length &lt; size) a = (T[])java.lang.reflect.Array.newInstance( a.getClass().getComponentType(), size); //在容量足够的前提下，将链表中的元素按顺序放入数组中 int i = 0; Object[] result = a; for (Node&lt;E&gt; x = first; x != null; x = x.next) result[i++] = x.item; //将第一个空出来的位置赋值为null，表示无元素 if (a.length &gt; size) a[size] = null; return a;&#125; 队列操作的方法因为Queue和Deque都是使用LinkedList实现的，所以这里也顺便提一下把，但是实际上有关队列操作的方法实则都是使用上面的核心方法实现的，所以这里就列一个表格对比一下： peek() ：等于getFirst()方法，唯一区别就是当链表为空时，peek方法返回null，getFirst抛出异常。 element()：等于getFirst()方法。 poll()：等于removeFirst()方法，唯一区别就是当链表为空时，poll方法返回null，removeFirst抛出异常。 remove():等于removeFirst()方法。 offer()：等于add()方法。 offerFirst()：等于addFirst()方法。 offerLast()：等于addLas()方法。 peekFirst()：等于peek()方法。 peekLast()：等于getLast()方法，唯一区别就是当链表为空时，peekLast方法返回null，getLast抛出异常。 pollFirst()：等于removeFirst()方法，唯一区别就是当链表为空时，pollFirst方法返回null，removeFirst抛出异常。 pollLast()：等于removeLast()方法，唯一区别就是当链表为空时，pollLast方法返回null，removeLast抛出异常。 push()：等于addFirst()方法. pop()：等于removeFirst()方法。 removeFirstOccurrence()：等于remove()方法。 removeLastOccurrence()：等于removeLast()方法。 总结总体来说，LinkedList的源码十分简单，从源码中可以简单得出以下几点： LinkedList的底层是双向链表。 有序。链表是有序的。 元素可重复，元素可为null。 随机访问效率低，增删效率高。]]></content>
      <categories>
        <category>Java容器</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL整体架构]]></title>
    <url>%2Fposts%2F10568%2F</url>
    <content type="text"><![CDATA[MySQL逻辑架构MySQL服务器逻辑架构图如图所示，MySQL的架构是三层架构，具体介绍如下： 最上层的服务（层）并不是MySQL独有的，大多数基于网络的客户端/服务器的工具或者服务都有类似的架构。这层的主要作用是连接处理、授权认证、安全等等。 第二层架构的是MySQL核心服务层，大多数MySQL的核心服务功能都在这一层，包括查询解析、分析、优化、缓存以及所有的内置函数，所有的跨存储引擎的功能都在这一层实现：存储过程、触发器、视图等。 第三层包括了存储引擎。存储引擎负责MySQL中数据的存储和提取。存储引擎不会去解析SQL（InnoDB例外，它会解析外键定义，因为MySQL服务器本身没有实现该功能），不同的存储引擎之间也不会相互通信，而只是简单地响应上层服务器的请求。 连接管理与安全性每个客户端都会在服务器进程中拥有一个线程，这个连接的查询只会在这个单独的线程中执行，该线程只能轮流在某个CPU核心或者CPU中运行。服务器会负责缓存线程，因此不需要为每一个新建的连接创建或者销毁线程。 当客户端连接到MySQL服务器时，服务器需要对其进行认证，认证基于用户名、原始主机信息和密码。 优化与执行MySQL会解析查询，并创建内部数据结构———解析树，然后对其进行各种优化，包括重写查询，决定表的读取顺序、以及选择合适的索引等。 用户可以通过特殊的关键字提示优化器，影响它的决策过程；也可以请求优化器解释优化过程的各个因素，使用户可以知道服务器是如何进行优化决策的，并提供一个参考基准，便于用户重构优化。 对于SELECT语句，在解析查询之前，服务器会先检查查询缓存，如果能在其中找到对应的查询，服务器就不必再执行查询解析、优化、和执行整个过程，而是直接返回查询缓存中的结果集。 并发控制这里讨论MySQL在两个层面的并发控制：服务器层和存储引擎层。 读写锁在处理并发或者写时，可以通过实现一个由两种类型的锁组成的锁系统来解决问题。这两种类型的锁通常被称为共享锁（shared lock）和排他锁（exclusive lock），也叫读锁（read lock）和写锁（write lock）。 读锁：是共享的，也就是相互不阻塞的。多个客户端在同一时刻可以同时读取同一个资源，而互不干扰。 写锁：是排他的，也就是一个写锁会阻塞其他的写锁和读锁，确保在给定的时间里，只有一个用户能执行写入，并防止其他用户读取正在写入的同一资源。 锁粒度一种提高共享资源并发性的方式就是让锁定对象更有选择性。尽量只锁定需要修改的部分数据，而不是所有的资源。更理想的方式是：只对会修改的数据片进行精确的锁定。下面是两种最重要的锁策略：表锁和行级锁。 表锁表锁（table lock）是MySQL中最基本的锁策略，并且是开销最小的策略：它会锁定整张表。 在特定场景表锁可能有良好的性能。另外，写锁也比读锁有更高的优先级，因此一个写锁请求可能会被插入到读锁列表的前面，反之读锁则不能插入到写锁的前面。 尽管存储引擎可以管理自己的锁，MySQL本身还是会使用各种有效的表锁来实现不同的目的。例如服务器会为ALTER TABLE之类的语句使用表锁，而忽略存储引擎的锁机制。 行级锁行级锁可以最大程度的支持并发，同时开销也最大。 行级锁只在存储引擎层实现，而MySQL服务层没有实现。 事务事务就是一组原子性的SQL查询，或者说一个独立的工作单元。如果数据库引擎能够成功的对数据库应用该组查询的全部语句，那么就执行该组查询。如果其中有任何一条语句因为崩溃或其他原因无法执行，那么所有的语句都不会执行。也就是说，事务内的语句，要么全部执行成功，要么全部执行失败。 可以同START TRANSACTION开始一个事务，然后要么用COMMIT提交事务将修改的数据持久保留，要么使用ROLLBACK撤销所有的修改。 数据库的特性有四条，简称为ACID，分别是：原子性（atomicity）、一致性（consistency）、隔离性（isolated）和持久性（durability）。 原子性（atomicity）：一个事务必须被视为一个不可分割的最小工作单元，整个事务操作要么全部提交成功，要么全部失败回滚，对于一个事务来说，不可能只执行其中一部分操作，这就是事务的原子性。 一致性（consistency）：数据库总是从一个一致性的状态转换到另一个一致性的状态。 隔离性（isolated）：通常来说，一个事务所做的修改在最终提交以前，对其他事务是不可见的。这里的“通常来说”与隔离级别有关。 持久性（durability）：一旦事务提交，则其所做的修改就会永久保存到数据库中。此时即使系统崩溃，修改的数据也不会丢失。 隔离级别在SQL标准中，定义了四种隔离级别。较低级别的隔离通常可以执行更高的并发，系统的开销也更低。 READ UNCOMMITED（未提交读）：在READ UNCOMMITTED级别，事务中的修改，即使没有提交，对其他事务也都是可见的。事务可以读取未提交的数据，这也称为脏读（Dirty Read）。这个级别会导致很多问题，而从性能上来讲却并不会比其他级别好太多，但缺少其他级别的很多好处，在实际使用中很少使用。 READ COMMITTED（提交读）：大多数数据库系统的默认隔离级别都是READ COMMITTED（但MySQL不是）。这个级别满足隔离性。这个级别有时候也叫做不可重复读，因为两次执行同样的查询，可能会得到不一样的结果。 REPEATABLE READ（可重复读）：这一级别解决了脏读的问题，该级别保证了在同一个事务中多次读取同样记录的结果是一致的。但是理论上，可重复读级别还是无法结局幻读问题：当某个事务再次读取某个范围内的记录时，另外一个事务又在该范围内插入了新的记录，当之前的事务再次读取该范围内的记录时，会产生幻行。InnoDB和XtraDB存储引擎通过多版本并发控制（MVCC）解决了幻读问题。可重复读是MySQL的默认事务隔离级别。 SERIALIZABLE（可串行化）：这个级别是最高的隔离级别，通过强制事务串行执行，避免了幻读问题。即会再读取的每一行数据上都加上锁，所以可能导致大量的超时和锁争用问题，只要在非常需要确保数据一致性而且可以接受没有并发的情况下，才考虑使用该级别。 死锁死锁是指两个或者多个事务在同一资源上相互占用，并请求锁定对方占用的资源，从而导致恶性循环的现象。 为了解决这种问题，数据库实现了各种死锁检测和死锁超时机制。一种解决方式是当查询的时间达到锁等待超时的设定后放弃锁请求，这种方式通常来说不太友好。InnoDB目前处理死锁的方法是：将持有最少行级排他锁的事务进行回滚。 死锁产生有双重原因：有些是因为真正的数据冲突，但有些则完全由于存储引擎的实现方式导致的。死锁发生以后，只有部分或者完全回滚其中一个事务，才能打破死锁。 事务日志使用事务日志在修改表的数据时只需要修改其内存拷贝，再把该修改行为记录到持久在硬盘上的事务日志中，而不用每次都将修改的数据本身持久到磁盘。 事务日志采用的是追加的方式，因此写日志的操作是磁盘上一小块区域的顺序I/O，所以采用事务日志的方式相对来说要快很多。 事务日志持久化以后，内存中被修改的数据在后台可以慢慢地刷回到磁盘。因此通常称之为预写式日志，修改数据需要写两次磁盘。 如果数据的修改已经记录到事务日志并持久化，但数据本身还没有写回磁盘，此时系统崩溃，存储引擎在重启时能够自动恢复这部分修改等数据。 MySQL中的事务MySQL提供了两种事务型的存储引擎：InnDB和NDB Cluster。另外还有一些第三方的存储引擎也支持事务，例如XtraDB和PBXT。 自动提交MySQL默认采用自动提交（AUTOCOMMIT）模式，也就是说，如果不是显式地开始一个事务，则每个查询都被当作一个事务执行提交操作。可以通过SET AUTOCOMMIT = ON/OFF来 开启/关闭 自动提交模式。 MySQL可以通过执行SET [GLOBAL | SESSION] TRANSATION ISOLATION LEVEL READ UNCOMMITTED| READ COMMITTED| REPEATABLE READ| SERIALIZABLE来设置全局/当前会话的隔离级别，新的隔离级别会在下一个事务开始的时候生效。 隐式和显式锁定InnoDB采用的是两阶段锁定协议。在执行过程中，随时都可以执行锁定，锁只有在执行COMMIT或者ROLLBACK的时候才会释放，并且所有的锁都是在同一时刻被释放。这就是隐式锁定。 InnoDB也支持通过特定的语句进行显式锁定：SELECT ... LOCK IN SHARE MODE 和 SELECT ... FOR UPDATE,MySQL也支持LOCK TABLES和UNLOCK TABLES语句，这是在服务层实现的，和存储引擎无关。 显示的使用这些语句不但没有必要，还会严重影响性能，实际上InnoDB的行级锁工作的更好，所以应当尽量避免使用LOCK TABLES。 多版本并发控制MySQL的大多数事务型存储引擎实现的都不是简单的行级锁，基于提升并发性能的考虑，它们一般都同时实现了多版本并发控制（MVCC）。可以认为MVCC是行级锁的一个变种，但是它在很多情况下避免了加锁操作，因此开销更低。虽然实现机制有可能不同，但大都实现了非阻塞的读操作，写操作也只锁定必要的行。 MVCC的实现是通过保存数据在某个时间点的快照来实现的。也就是说，不管需要执行多长时间，每个事务看到的数据都是一致的。根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。 InnoDB的MVCC是通过在每行记录后面保存两个隐藏的列来实现的。这两个列，一个保存了行的创建时间，一个保存行的过期时间（或删除时间）。当然存储的不是实际的时间值，而是系统版本号。每开始一个新的事务，系统版本号都会递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。 SELECT：InnoDB会根据以下两个条件检查每行记录： InnoDB只查找早于当前事务版本的数据行（即行的系统版本号小于或等于事务的系统版本号），这样可以确保读取的行要么在事务开始前已经存在，要么是事务自身插入或者修改过的。 行的删除要么未定义，要么大于当前事务版本号。这样可以确保读取的行在事务开始之前未被删除。 INSERT：InnoDB为新插入的每一行保存当前系统版本号作为行版本号。 DELETE：InnoDB为删除的每一行保存当前系统版本号作为行删除标识。 UPDATE：InnoDB为插入一行新记录，保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为行删除标识。 保存这两个额外系统版本号，优点就是使大多数读操作都可以不用加锁，读数据操作很简单，性能很好，并且也能保证只会读取到符合标准的行。缺点就是每行记录都需要额外的存储空间，需要做更多的行检查工作，以及一些额外的维护工作。 MVCC只在 REPEATABLE READ（可重复读）和READ COMMITTED（提交读）两个隔离级别下工作，因为READ UNCOMMITTED总是读取最新的数据行，而SERIALIZABLE则会对所有读取的行加锁。 MySQL的存储引擎可以使用SHOW TABLE STATUS 命令显示表的相关信息。例如： 1234567891011121314151617181920212223mysql&gt; show table status like 'user' \G*************************** 1. row *************************** Name: user //表名 Engine: InnoDB //表引擎 Version: 10 //版本 Row_format: Dynamic//行的格式。Dynamic的行长度是可变的，一般包含可变长度的字段，如VARCHAR //；Fixed的行长度则是固定的，只包含固定长度的列，如CHAR。Compressed的行只在压缩 //表中存在。 Rows: 5 //表中的行数，MyISAM和其他一些存储引擎是精确值，但InnoDB是估计值。 Avg_row_length: 59 //平均每行包含的字节数 Data_length: 16384 //表数据的大小（字节）Max_data_length: 0 //表的最大数据容量，该值与存储引擎关 Index_length: 0 //索引的大小 Data_free: 0 //对于MyISAM表，表示已分配但目前没有使用的空间 Auto_increment: 6 //下一个AUTO_INCREMENT的值 Create_time: 2018-11-19 20:58:21 //表的创建时间 Update_time: NULL //表数据的最后修改时间 Check_time: NULL //使用CHECK TABLE命令或者myisamchk工具最后一次检查表的时间 Collation: utf8_general_ci //表的默认字符集和字符列排序规则 Checksum: NULL //如果启用，保存的是整个表的实时校验和 Create_options: //创建表时指定的其他选项 Comment: //建表时的备注1 row in set (0.00 sec) InnoDB存储引擎InnoDB是MySQL的默认事务型引擎，也是最重要、使用最广泛的存储引擎。它被设计用来处理大量的短期事务，短期事务大部分情况是正常提交的，很少会回滚。InnoDB的性能和自动崩溃恢复特性，使得它在非事务型存储的需求中也很流行。 InnoDB的数据存储在表空间长，表空间是由InnoDB管理的一个黑盒子，有一系列的数据文件组成，InnoDB可以将每个表的数据和索引存放在单独的文件中。 InnoDB采用MVCC来支持高并发，并且实现了四个标准的隔离级别，默认为REPATABLE READ，并且通过间隙锁（next-key locking）策略防止幻读的出现。间隙锁使得InnoDB不仅仅锁定查询涉及的行，还会对索引中的间隙进行锁定，以防止幻影行的插入。 InnoDB表是基于聚簇索引建立的，聚簇索引对主键查询有很高的性能，不过它的二级索引中必须包含主键列。因此若表上的索引较多的话，主键应当尽可能的小。InnoDB是平台独立的，可以将数据和索引文件在平台之间拷贝。 InnoDB内部做了很多优化，包括从磁盘读取数据时采用的可预测性预读，能够自动在内存中创建hash索引以及加速读操作的自适应哈希索引，以及能够加速插入操作的插入缓冲区等。 MyISAM存储引擎M有IASM是MySQL5.1之前的默认存储引擎。MyISAM提供了大量的特性，包括全文索引、压缩、空间函数等，但MyIASM不支持事务和行级锁，而且有一个最大的缺陷就是崩溃后无法安全恢复。 存储MyIASM会将表存储在两个文件中：数据文件和索引文件，分别以.MYD和.MYI为扩展名。MyIASM表可以包含动态或者静态行。MySQL会根据表的定义来决定采用何种行格式。MyIASM表可以存储的行记录数，一般受限于可用的磁盘空间，或者操作系统中单个文件的最大尺寸。 在MySQL5.0中，MyIASM表如果是可变行，则默认配置只能处理256TB的数据，可以通过修改表单MAX_ROWS和AVG_ROW_lENGTH选项的值来实现，两者相乘就是表可能达到的最大大小。修改这两个参数会导致重建整个表和表的所有索引，这可能需要很长的时间才能完成。 MyIASM特性 加锁与并发：MyIASM对整张表加锁，而不是针对行。读取时会对需要读到的表加共享锁，写入时则对表加排他锁。但是在表有读取查询的同时，也可以往表中插入新的记录（也被称为并发插入）。 修复：对于MyIASM表，MySQL可以手工或者自动检查和修复操作。执行表的修复可能导致一些数据的丢失，而且修复操作是非常慢的。可以通过CHECK TABLE mytable检查表的错误，如果有错误可以通过执行REPAIR TABLE mytable来修复，或者使用myiasmchk命令行工具也可以。 索引特性：对于MyIASM表，即使是BLOB和TEXT等长字段，也可以基于其前500个字符创建索引。MyIASM也支持全文索引，这是一种基于分词创建的索引。 延迟更新索引键（Delayed Key Write）：如果在创建MyIASM表时指定了DELAY_KEY_WRITE选项，在每次修改执行完时，不会立刻将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理键缓冲区或者关闭表的时候才会将对应的索引块写入到磁盘。这种方式可以极大的提升写入性能。 MyIASM压缩表如果表在创建并导入以后，不会再进行修改操作，那么这样得表更适合采用MyIASM压缩表。 可以使用myiaspack对MyIASM表进行压缩。压缩表是不能进行修改的（除非先将表解压、修改数据、然后再次压缩）。压缩表可以极大的减少磁盘空间占用，减少磁盘I/O，从而提升查询性能。压缩表也支持索引，但索引也是只读的。 MyIASM性能MyIASM引擎数据以紧密格式存储，所以在默写场景下的性能很好。 MyIASM有一些服务器级别的性能扩展限制，比如对索引键缓冲区的Mutex锁，MariaDB基于段的索引键缓冲区机制来避免该问题，性能上的表锁问题。 选择合适的引擎对于如何选择存储引擎，可以简单的归纳为一句话：除非需要用到某些InnoDB不具备的特性，并且没有其他办法可以代替，否则都应该优先选择InnoDB引擎。 除非万不得已，否则建议不要混合使用多种存储引擎，佛祖额可能带来一系列复杂的问题，以及一些潜在的bug和边界问题。 如果需要不同的存储引擎，应先考虑以下几个因素： 事务：如果应用到事务，那么InnoDB是目前最稳定并且经过沿着轨道选择。如果不需要事务，并且主要是SELECT和INSERT操作，那么MyISAM是不错的选择。 备份：如果需要在线热备份，那么选择InnoDB就是基本的要求。 崩溃恢复：建议选择InnoDB，因为拥有自动恢复功能。 特有的特性：有些应用可能依赖一些存储引擎所独有的特性或者优化，比如聚簇索引的优化，应该综合各种情况考虑，选择满足特殊情况下最优的引擎。 转换表的引擎转换表的引擎有三种方法： ALTER TABLE：将表从一个引擎修改为另一个引擎最简单的办法是使用ALTER TABLE语句。例如：ALTER TABLE mytable ENGINE = InnoDB。这种语法是用于任何存储引擎，但是有一个缺点：需要执行很长时间。MySQL会按照行将数据从原表复制到一张新的表，在复制期间可能会消耗系统所有的I/O能力，同时原表会加上锁。同时如果转换表的存储引擎，将会失去和引擎相关的所有特性. 导出与导入：可以使用mysqldump工具将数据导出到文件，然后修改文件中CREATE TABLE语句的存储引擎选项。 创建与查询：这种方法不需要导出整个表，而是先创建一个新的存储引擎的表，然后利用INSERT–SELECT语法来导数据。123CREATE TABLE innodb_table LIKE myisam_table;ALTER TABLE innodb_table ENGINE = InnoDB;INSERT INTO innodb_table (SELECT * FROM myisam_table); 当然如果数据量很大的情况下，可以采用分批导入的方法。]]></content>
      <categories>
        <category>MySQL数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[类加载器]]></title>
    <url>%2Fposts%2F16040%2F</url>
    <content type="text"><![CDATA[类与类加载器对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立其在Java虚拟机中的唯一性，每一个类加载器，都拥有一个独立的类名称空间。 上面这句话的意思即：比较两个类是否“相等”，只有在这两个类是由同一个类加载器加载的前提下才有意义，否则，即使这两个类来源于同一个Class文件，被同一个虚拟机加载，只要加载它们的类加载器不同，那这两个类就必定不相等。 双亲委派模型从Java虚拟机的角度来讲，只存在两种不同的类加载器：一种是启动类加载器（Bootstrap ClassLoader），这个类加载器使用C++语言实现，是虚拟机自身的一部分；另一种就是所有其他的类加载器，这些加载器都由Java语言实现，独立于虚拟机外部，并且全都继承自抽象类java.lang.ClassLoader。 以下是三种系统提供的主要的类加载器： 启动类加载器（Bootstrap ClassLoader）：这个类加载器负责将存放在\lib目录中的，或者被-Xbootclasspath参数所指定的路径中的，并且是虚拟机识别的类库加载到虚拟机内存中。启动类加载器无法被Java程序直接引用。 扩展类加载器（Extension ClassLoader）：这个加载器它负责加载\lib\ext目录中的，或者被java.ext.dirs系统变量所指定的路径中的所有类库，开发者可以直接使用扩展类加载器。 应用程序类加载器（Application ClassLoader）：由于这个类加载器是ClassLoader中的getSystemClassLoader()方法的返回值，所以一般也称它为系统类加载器。它负责加载用户类路径（ClassPath）上所指定的类库，开发者可以直接使用这个类加载器。一般情况下这个就是应用程序中默认的类加载器。 下图展示的类加载器之间的这种层次关系，称为类加载器的双亲委派模型。 双亲委派模型要求除了顶层的启动类加载器之外，其余的类加载器都应当有自己的父类加载器，这里类加载器之间的父子关系一般不会以继承的关系来实现，而是都使用组合关系来复用父加载器的代码。 双亲委派模型并不是一个强制性的约束模型，而是Java设计者推荐给开发者的一种类加载器实现方式。 每个类加载都有一个父类加载器，通过下面的程序来验证。 1234567public class ClassLoaderDemo &#123; public static void main(String[] args) &#123; System.out.println("ClassLodarDemo's ClassLoader is " + ClassLoaderDemo.class.getClassLoader()); System.out.println("The Parent of ClassLodarDemo's ClassLoader is " + ClassLoaderDemo.class.getClassLoader().getParent()); System.out.println("The GrandParent of ClassLodarDemo's ClassLoader is " + ClassLoaderDemo.class.getClassLoader().getParent().getParent()); &#125;&#125; 输出结果为： 123ClassLodarDemo's ClassLoader is sun.misc.Launcher$AppClassLoader@18b4aac2The Parent of ClassLodarDemo's ClassLoader is sun.misc.Launcher$ExtClassLoader@1b6d3586The GrandParent of ClassLodarDemo's ClassLoader is null 由结果可以看出AppClassLoader的父类加载器为ExtClassLoader，ExtClassLoader的类父加载器为null，但是null并不代表没有父类加载，而是BootstrapClassLoader。 双亲委派模型这里的双亲更多的表达的是“父母这一辈人而已，并不是说真的有一个Mother ClassLoader和一个Father ClassLoader”。 双亲委派模型的工作过程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一层次的类加载器都是如此，因此所有的加载请求最终都应该传送到顶层的类加载器中，只有当父类加载器反馈自己无法完成这个请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去加载。 双亲委派模型的源码实现如下： 123456789101112131415161718192021222324252627282930313233343536private final ClassLoader parent; protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // 首先，检查请求的类是否已经被加载过 Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123;//父加载器不为空，调用父加载器loadClass()方法处理 c = parent.loadClass(name, false); &#125; else &#123;//父加载器为空，使用启动类加载器 BootstrapClassLoader 加载 c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; //抛出异常说明父类加载器无法完成加载请求 &#125; if (c == null) &#123; long t1 = System.nanoTime(); //自己尝试加载 c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; &#125; 这个过程为：先检查是否已经被加载过，若没有加载则调用父加载器的loadClass()方法，若父加载器为空，则默认使用启动类加载器作为父加载器。如果父加载器加载失败，抛出ClassNotFoundException异常后，再调用自己的findClass()方法去加载。 双亲委派模型的好处：Java类锁着它的类加载器一起具备了一种带有优先级的层次关系，保证了Java程序的稳定运行，可以避免类的重复加载，同时也保证了Java的核心API不被篡改。 破坏双亲委派模型的三种方式： 我们可以自己定义一个类加载器，然后重载loadClass()即可。 Java设计团队引入的线程上下文类加载器（Thread Context ClassLoader）。这个类加载器可以通过java.lang.Thread类的setContextClassLoader()方法进行设置，如果创建线程时还未设置，它将会从父线程中继承一个，如果再应用程序的全部范围内都没有设置过的化，那这个类的加载器默认就是应用程序类加载器。 OSGi环境下的网状结构类加载过程。]]></content>
      <categories>
        <category>Java虚拟机</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[虚拟机类加载过程]]></title>
    <url>%2Fposts%2F2708%2F</url>
    <content type="text"><![CDATA[虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。 在Java语言里面，类型的加载、连接和初始化都是在程序运行期间完成的，这种策略虽然会令加载时稍微增加一些性能开销，但是会为Java应用程序提供高度的灵活性。 类加载的时机类从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期包括：加载（Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）、初始化（Initialization）、使用（Using）和卸载（Unloading）7个阶段，其中验证、准备、解析3个部分通称为连接。图中加载、验证、准备、初始化和卸载这5个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班的开始（而不是完成，因为通常会在一个阶段执行的过程中调用、激活另外一个阶段），而解析阶段则不一定：它在某些情况下可以在初始化阶段之后再开始，这是为了支持Java语言的运行时绑定（也成为动态绑定或晚期绑定）。 对于初始化阶段，虚拟机规范严格指定了有且只有5种情况必须立即对类进行“初始化”： 遇到new、getstatic、putstatic或invokestatic这4条字节码指令时，如果类没有进行过初始化，则需要先触发其初始化。生成这4条指令最常见的Java代码场景是：使用new关键字实例化对象、读取或设置一个类的静态字段（被final修饰、已在编译期把结果放入常量池的静态字段除外），以及调用一个类的静态方法的时候。 使用java.lang.reflect包的方法对类进行反射调用的时候，如果类没有进行过初始化，则需要先触发其初始化。 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。 当虚拟机启动时，用户需要指定一个要执行的主类（包括main（）方法的那个类），虚拟机会先初始化这个主类。 当使用JDK1.7的动态语言支持时，如果一个java.lang.invoke.MethodHandle实例最后的解析结果REF_getStatic、REF_putStatic、REF_invokeStatic的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化。 “有且只有”这5种场景中的行为称为对一个类进行主动引用。除此之外，所有引用类的方式都不会触发初始化，称为被动引用。被动引用的场景例如： 通过子类引用父类的静态字段，不会导致子类初始化。 通过数组来定义引用类，不会触发此类的初始化。 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类。因此不会触发定义常量的类的初始化。 接口与类真正有所区别的是有且仅有需要开始初始化的5种场景中的第三种：当一个类在初始化时，要求其父类全部都已经初始化过了，但是一个接口在初始化时，并不要求其父接口全部完成了初始化，只有在真正使用到父接口的时候才会初始化。 类加载的过程接下来详细讲解一下Java虚拟机中类加载的全过程，也就是加载、验证、准备、解析、初始化这5个阶段所执行的具体操作。 加载在加载阶段，虚拟机需要完成以下3件事情： 通过一个类的全限定名来获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。 虚拟机规范对上面这3点并不具体，因此是非常灵活的。比如：”通过全类名获取定义此类的二进制字节流” 并没有指明具体从哪里获取、怎样获取。比如：比较常见的就是从 ZIP 包中读取（日后出现的JAR、EAR、WAR格式的基础）、其他文件生成（典型应用就是JSP）等等。 一个非数组类的加载阶段（加载阶段中获取类的二进制字节流的动作）是开发人员可控性最强的，因为加载阶段既可以通过使用系统提供的引导类加载器来完成，也可以由用户自动移动类加载器去完成（即重写一个loadClass（）方法）。 对于数组类而言，数组类本身不通过类加载器创建，它是由Java虚拟机直接创建的。一个数组类（称为C）的创建过程遵循以下规则： 如果数组的组件类型（即数组去掉一个维度的类型）是引用类型，那就递归采用本节中定义的加载过程去加载这个组件类型，数组C将在加载该组件类型的类加载器的类名称空间上被标识。 如果数组的组件类型不是引用类型（例如int[]数组）,Java虚拟机将会把数组C标记为与引导类加载器关联。 数组类的可见性与它的组件类型的可见性一致；如果数组的组件类型不是引用类型，那数组类的可见性将默认为public。 加载阶段完成后，虚拟机外部的二进制字节流就按照虚拟机所需的格式存储在方法区之中，然后再内存中实例化一个java.lang.Class类的对象。 加载阶段与连接阶段的部分内容（如一部分字节码文件格式验证动作）是交叉进行的。加载阶段尚未完成，连接阶段可能已经开始，在这些夹在加载阶段之中进行的动作，仍然属于连接阶段的内容，这两个阶段的开始时间仍然保持着固定的先后顺序。 验证验证这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 Java语言本身是相对安全的语言，但Class文件并不一定要求用Java源码编译而来，，在字节码层面上，有些Java代码无法做到的时区都是可能实现的，至少语义上可能表达出来。 对于虚拟机的类加载机制来说，验证阶段是一个非常重要的、但是不一定必要的阶段。如果运行的全部代码都已经被反复使用和验证过，那么在实施阶段就可以考虑使用-Xverify:none参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。 验证阶段大致会完成下面4个阶段的校验工作：文件格式验证、元数据验证、字节码验证、符号引用验证。 文件格式验证第一阶段要验证字节流是否符合Class文件格式的规范，并且能够被当前版本的虚拟机处理。这一阶段可能包括验证是否以魔数0xCAFEBABE开头、主、次版本号是否在当前虚拟机处理范围之内等等。 该阶段的主要目的是为了保证输入的字节流能正确地解析并存储于方法区之内，格式上符合描述一个Java类型信息的要求。 元数据验证第二阶段是对字节码描述的信息进行语义分析，以保证其描述的信息符合Java语言规范的要求。这个阶段可能包括但验证例如：这个类是否有父类、这个类的父类是否继承了不允许被继承的类等等。 这个阶段的主要目的是对类的元数据信息进行语义校验，保证不存在不符合Java语言规范的元数据信息。 字节码验证这阶段是整个验证过程中最复杂的一个阶段，主要目的是通过数据流和控制流分析，确定程序语义是合法的。在这个阶段对类的方法体进行校验分析，保证被校验类的方法在运行时不会做出危害虚拟机安全的事件，例如：保证任意时刻操作数栈的数据类型与指令代码序列都能配合工作，例如不会出现在操作栈放置了一个int类型的数据，使用时却按long类型来加载入本地变量表等等。 在JDK1.6之后的javac编译器和Java虚拟机中进行了一项优化，给方法体的Code属性表中增加了一项名为“StackMapTable”的属性，这项属性描述了方法体中所有的基本快开始时本地变量表和操作栈应有的状态，在字节码验证期间，就不需要根据程序推导这些状态的合法性，只需检查StackMapTable属性中的记录是否合法即可，这样可以节省时间。 符号引用验证最后一个阶段的校验发生在虚拟机将符号引用转化为直接引用的时候，这个转化动作将在连接的第三阶段（解析阶段）中发生。符号引用的目的是确保解析动作能正常执行。 准备准备阶段是正式为类变量（被static修饰的变量）分配内存并设置初始值的阶段，这些变量所使用的内存都将在方法区中进行分配，这阶段不包括实例变量，实例变量将会在对象实例化时随对象一起分配在Java堆中。 其次这里说的初始值通常情况下是数据类型的零值。假设一个类变量的定义为：public static int value = 123；，那变量value在准备阶段后的初始值是0而不是123，而赋值为123的指令putstatic被程序编译后，存放于类构造器方法中。 基本数据类型的零值： 相对于通常情况的特殊情况就是：如果类字段的字段属性表中存在ConstantValue属性，那在准备阶段变量value就会被初始化为ConstantValue属性所指定的值，例如public static final int value = 123;，在准备阶段会根据ConstantValue的设置将value赋值为123。 解析解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。 符号引用：以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义的定位到目标即可。与虚拟机实现的内存布局无关，引用的目标不一定已经加载到内存中。 直接引用：可以是直接指向目标的指针、相对偏移量或是一个能间件定位到目标的的句柄。和虚拟你实现的内存布局相关，引用到目标必定已经在内存中存在了。 虚拟机规范之中并未规定解析阶段发生的具体实现，只要求了在执行anewarray、checkcast、getfield、getstatic、instanceof、invokedynamic、invokeinterface、invokespecial、invokestatic、invokevirtual、ldc、ldc_w、multianewarray、new、putfield和putstatic这16个用于操作符号引用的字节码指令之前，先对他们所使用的符号引用进行解析。所以虚拟机实现可以根据需要来判断到底是在类被加载器加载时就对常量池中的符号引用进行解析，还是等到一个符号引用将要被使用前才去解析它。 对同一个符号引用进行多次解析请求是很常见的事情，虚拟机可以对第一次解析的结果进行缓存，从而避免解析动作重复进行。 解析动作主要针对类、接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行，分别对应常量池中的CONSTANT_Class_info、CONSTANT_Fieldref_info、CONSTANT_Methodref_info、CONSTANT_InterfaceMethodref_info、CONSTANT_MethodType_info、CONSTANT_MethodHandle_info和CONSTANT_InvokeDynamic_info 7种常量类型。下面讲解前四种静态符号引用的过程。 类或接口的解析加色和当前代码所处的类为D，如果要把一个从未解析过的符号引用N解析为一个类或接口C的直接引用，虚拟机解析过程如下三个步骤： 如果C不是一个数组类型，那虚拟机将会把代表N的全限定名传递给D的类加载器去加载这个类C。在加载过程中，由于元数据验证、字节码验证的需要，又可能触发其他相关类的加载动作，例如加载这个类的父类或实现的接口。一旦这个加载过程出现了任何异常，解析过程就宣告失败。 如果C视野更数组类型，并且数组的元素类型为对象，，那将会按照第1步的规则加载数组的元素类型。 解析完成之前进行符号引用验证，确认D是否已具备对C的访问权限，如果不具备访问权限，将抛出IllegalAccessError异常。 字段解析首先将会对字段表内class_index项中索引的CONSTAN_Class_info符号引用进行解析，也就是字段所属的类或接口的符号引用。如果解析成功，拿奖这个字段所属的类或接口用C表示，后续解析步骤如下： 如果C本身就包含了简单名称和字段描述符都与目标相匹配的字段，则返回这个字段的直接引用，解析结束。 否则，如果在C中实现了接口，将会按照继承关系从下往上递归搜索各个接口和它的父接口，如果接口中包含了简单名称和字段描述符都与目标相匹配的字段，则返回这个字段的直接引用，查找结束。 否则，如果C不是java.lang.Object的话，将会按照继承关系从下往上递归搜索其父类，如果在父类中包含了简单名称和字段描述符都与目标相匹配的字段，则返回这个字段直接引用，查找失败。 否则查找失败，抛出NoSuchFieldError异常。 如果查找过程成功返回了引用，将会对这个字段进行权限验证，如果发现具备对字段的访问权限，将抛出IllegalAccessError异常。 如果有一个同名字段同时存在于C的接口和父类中，或者同时在自己或父类的多个接口中出现，那编译器将可能拒绝编译。 类方法解析类方法解析的第一个步骤与字段解析一样，也需要先解析出类方法表的class_index项中索引的方法所属的类或接口的符号引用，如果解析成功，用C表示这个类。后续步骤如下： 类方法和接口方法符号引用的常量类型定义是分开的，如果在类方法表中发现class_index中索引的C是个接口，那就直接抛出java.lang.IncompatibleClassChangeError异常。 如果通过了第1步，在类C中查找是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。 否则，在类C的父类中递归查找是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。 否则，在类C实现的接口列表及他们的父接口之中递归查找是否有简单名称和描述符都与目标相匹配的方法，如果存在匹配的方法，说明类C是一个抽象，这时查找结束，抛出java.lang.AbstractMethodError异常。 否则，宣告方法查找失败，抛出java.lang.NoSuchMethodError。 最后，如果查找过程成功返回了直接引用，将会对这个方法进行权限验证，如果发现不具备对此方法的访问权限，将抛出java.lang.IllegalAccessError异常。 接口方法解析接口方法也需要先解析出接口方法表的class_index项中索引的方法所属的类或接口的符号引用，如果解析成功，依然用C表示这个接口，接下来虚拟机将会按照如下步骤进行后续的接口方法搜索。 与类方法解析不同，如果在接口方法表中发现class_index中的索引C是个类而不是接口，那就直接抛出java.lang.IncompatibleClassChangeError异常。 否则，在接口C中查找是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。 否则，在接口C的父接口中递归查找，直到java.lang.Object（查找范围会包括Object类）为止，看是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。 否则，宣告方法查找失败，抛出java.lang.NoSuchMethodError异常。 由于接口中的所有方法默认都是public，所以不存在访问权限的问题，因此接口方法的符号解析应当不会抛出java.lang.IllegalAccessError异常。 初始化到了初始化阶段，才真正开始执行类中定义的Java程序代码（或者说字节码）。也可以说初始化阶段是执行类的方法的过程。 方法是由编译器自动收集类中的所有类变量的复制动作和静态语句块中的语句合并产生的，编译器收集的顺序是由语句在源文件中出现的顺序所决定的，静态语句块中只能访问到定义在静态语句块之前的变量；定义i在它之后的变量，在前面的静态语句块可以赋值，但是不能访问。 方法不许需要显式的调用父类的构造器，虚拟机会保证在子类的方法执行之前，父类的方法已经执行完毕。因此在虚拟机中第一个被执行的方法的类肯定是java.lang.Object。 由于父类的方法先执行，因此父类中定义的静态语句块要优先于子类的变量赋值操作。 方法对于类或接口来说不是必须的，一个类中没有没有静态语句块，也没有对变量的赋值操作，那么编译器就可以不为这个类生成方法。 接口中不能使用静态语句块，但仍然有变量初始化的赋值操作，因此也会生出方法。但是与类不同，执行接口的方法不需要先执行父接口的方法，只有当父接口中定义的变量使用时，父接口才会被初始化，接口的实现类在初始化时也一样不会执行接口的方法。 虚拟机会保证一个类的方法在多线程环境中被正确的加锁、同步。如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的方法，其他线程都需要阻塞等待，直到活动线程执行方法完毕。]]></content>
      <categories>
        <category>Java虚拟机</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[类文件结构]]></title>
    <url>%2Fposts%2F31580%2F</url>
    <content type="text"><![CDATA[Java虚拟机不和包括Java在内的任何语言绑定，它只与“Class”文件这种特定的二进制文件格式所关联，Class文件中包含了Java虚拟机指令集和符号表以及若干其他辅助信息。 可以说Class文件是不同的语言在Java虚拟机之间的重要桥梁，同时也是支持Java跨平台很重要的一个原因。 Class类文件结构总览Class文件是一组以8位字节为基础单位的二进制流，各个数据项目严格地按照顺序紧凑地排列在Class文件之中，中间没有添加任何分隔符，当遇到需要占用8位字节以上空间的数据项时，则会按照高位在前的方式分割成若干个8位子节进行存储。 任何一个Class文件都对应着唯一一个类或接口的定义信息，但反过来说，类或接口并不一定都得定义在文件里（譬如类或接口也可以通过类加载器直接生成）。 Class文件格式采用一种类似于C语言结构体的伪结构来存储数据，这种伪结构中只有两种数据类型：无符号数和表。 无符号数属于基本的数据类型，以u1、u2、u4、u8来分别代表1个字节、2个字节、4个字节和8个字节的无符号数，无符号数可以用来描述数字、索引引用、数量值或者按照UTF-8编码构成字符串值。 表是由多个无符号数或者其他表作为数据项构成的复合数据类型，所以表都习惯性地以”_info”结尾，表用于描述有层次关系的复合结构的数据，整个Class文件本质上就是一张表。 Class文件格式： 12345678910111213141516u4 magic; //Class 文件的标志u2 minor_version; //Class 的小版本号u2 major_version; //Class 的大版本号u2 constant_pool_count; //常量池的数量cp_info constant_pool[constant_pool_count-1]; //常量池u2 access_flags; //Class 的访问标志u2 this_class; //当前类u2 super_class; //父类u2 interfaces_count; //接口u2 interfaces[interfaces_count]; //一个类可以实现多个接口u2 fields_count; //Class 文件的字段属性field_info fields[fields_count]; //一个类会可以有个字段u2 methods_count; //Class 文件的方法数量method_info methods[methods_count]; //一个类可以有个多个方法u2 attributes_count; //此类的属性表中的属性数attribute_info attributes[attributes_count]; //属性表集合 无论是无符号数还是表，当需要描述同一类型数据但数量不定时，经常会使用一个潜质的容量计数器加若干个连续的数据项形式，这时称这一系列的某一类型的数据为某一类型的集合。 Class文件字节码结构组织示意图： 魔数每个Class文件的头4个字节称为魔数（Magic Number），它的唯一作用就是确定这个文件是否为一个能被虚拟机接受到Class文件，很多文件存储标准中都使用魔数来进行身份识别而不是扩展名的主要原因是基于安全方面的考虑，因为文件扩展名可以随意的改动。 1u4 magic; //Class 文件的标志 Java中Class文件的魔数值为：0xCAFEBABE（咖啡宝贝？）。 Class文件版本紧接着魔术的4个字节存储的是Class文件的版本号：第5个和第6个是次版本号（Minor Version），第7个和第8个是主版本号（Major Version）。 12u2 minor_version; //Class 的次版本号u2 major_version; //Class 的主版本号 Java版本号是从45开始的，JDK1.1之后每个JDK大版本发布主版本号向上加一，高版本的JDK能向下兼容以前版本的Class文件，但不能运行以后版本的Class文件，即使文件格式并未发生任何变化。 常量池紧接着主次版本号之后的是常量池入口，常量池可以理解为Class文件之中的资源仓库，它是Class文件结构中与其他项目关联最多的数据类型，也是战役Class文件空间最大的数据项目之一，同时它还是在Class文件中第一个出现的表类型数据项目。 12u2 constant_pool_count; //常量池的数量cp_info constant_pool[constant_pool_count-1]; //常量池 常量池的入口为一项u2类型的数据，代表常量池容量计数值（constant_pool_count），这个容量计数是从1开始而不是从0开始的，值为0代表“不引用任何一个常量池项目”的特殊情况。Class文件结构中只有常量池的容量计数是从1开始的。 常量池中主要存放两大类常量：字面量和符号引用。字面量比较接近于Java语言层面的常量概念，如文本字符串、声明为final类型的常量值等；而符号引用则属于编译原理方面的概念，包括下面三类常量： 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 在Class文件之不会保存各个方法、字段的最终内存的布局信息，当虚拟机运行时，需要从常量池获得对应的符号引用，再在类创建时或运行时解析、翻译到具体的内存地址之中。 常量池中每一个项都是一个表，这14种表有一个共同的特点：表开始的第一位是一个u1类型的标志位（tag），代表这个常量属于哪种常量类型。 由于Class文件中方法、字段等都需要引用CONSTANT_Utf8_info型常量来描述名称，而这里的最大长度就是length的最大值，既u2类型能表达的最大值65535，所以Java程序中如果定义可超过64KB英文字符的变量或方法名，将会无法编译。 Class 文件可以通过javap -v class类名 指令来看一下其常量池中的信息(javap -v class类名-&gt; temp.txt ：将结果输出到 temp.txt 文件)。 常量池中一部分自动生成的常量的确都没有在Java代码里面出现过，但它们会被后面即将讲到的字段表（field_info）、方法表（method_info）、属性表（attribute_info）引用到，他们会用来描述一些不方便使用“固定字节”进行表达的内容，譬如描述方法的返回值是什么？有几个参数？每个参数的类型是什么？ 常量池中的14种常量项的结构总表： 访问标志在常量池结束之后，紧接着的两个字节代表访问标志（access_flags），这个标志用于识别一些类或者接口层次的访问信息，包括：这个Class是接口还是类；是否定义为public类型；是否定义为abstract类型；如果是类的话，是否声明为final等。 1u2 access_flags; //Class 的访问标记 最后这两个字节的值是上面这8个标志位的值的异或结果，没有用到的标志位一律为0。 类索引、父类索引与接口索引集合类索引和父类索引都是一个u2类型的数据，而接口索引集合是一组u2类型的数据的集合，Class文件中由这三项来确定这个类的继承关系。 1234u2 this_class; //当前类u2 super_class; //父类u2 interfaces_count; //接口u2 interfaces[interfaces_count]; //一个类可以实现多个接口 类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名。除了Java.lang.Object之外，所有的Java类都有父类，因此除了Java.lang.Object之外，所有Java类的父类索引都不为0， 接口索引集合就用来描述这个类实现了哪些接口，这些实现的接口将按implements语句（如果这个类本身是一个接口，则应当是extends语句）后的接口顺序从左到右排列在接口索引集合中。 类索引、父类索引和接口索引集合都按顺序排列在访问标志之后；对于接口索引集合，入口的第一项————u2类型的数据为接口计数器，表示索引表的容量，如果该类没有实现任何接口，则该计数器值为0。 字段表集合字段表（field_info）用于描述接口或者类中声明的变量。字段包括类级变量以及实例级变量，但不包括在方法内部声明的局部变量。 12u2 fields_count; //Class 文件的字段的个数field_info fields[fields_count]; //一个类会可以有个字段 字段表的结构： 字段修饰符放在access_flags项目中，在实际情况中，ACC_PUBLIC、ACC_PRIVATE、ACC_PROTECTED三个标志最多只能选择其一，ACC_FINAL、ACC_VOLATILE不能同时选择，接口之中的字段必须有ACC_PUBLIC、ACC_STATIC、ACC_FINAL标志，其可以设置的标志位和含义如图： 跟随access_flags标志的是两项索引值：name和descriptor_index。它们都是对常量池的引用，分别代表着字段的简单名称以及字段和方法的描述符。 全限定名：一个类的全名的“.”全部替换成“/”简单名称：没有类型和参数修饰的方法或字段名称，既只有名字描述符：用来描述字段的数据类型、方法的参数列表（包括数量、类型和顺序）和返回值。描述符标识字符含义：对于数组来说，每一维度将使用一个前置的“[”字符来描述；描述方法时，按照先参数列表，后返回值的顺序描述，参数列表按照参数的严格顺序放在一组小括号“（）”内 descriptor_index之后跟随着一个属性表集合用于存储一些额外的信息，字段都可以在描述表中描述零至多项的额外信息。 字段表集合中不会列出从超累或者父类中继承来的字段，但有可能列出原本Java代码之中不存在的字段，譬如在内部类中为了保持对外部类的访问性，会自动添加指向外部类实例的字段。另外如果两个字段的描述符不一致，那字段重名就是合法的。 方法表集合方法表的结构如同字段表一样： 12u2 methods_count; //Class 文件的方法的数量method_info methods[methods_count]; //一个类可以有个多个方法 方法表的 access_flag 取值： 方法里的Java代码，经过编译器编译成字节码指令后，存放在方法属性表集合中一个名为“Code”的属性里面，属性表作为Class文件格式中最具扩展性的一种数据项目，见下一节。 与字段表集合相对应的，如果父类方法在子类中没有进行重写（Override），方法表集合中就不会出现来自弗雷德方法信息。但同样有可能出现由编译器自动添加的方法，最典型的便是类构造器方法和实例构造器方法。 Java语言中，要重载（Overload）一个方法，除了要与原方法具有相同的简单名称之外，还要求必须拥有一个与原方法不同的特征签名，特征签名就是一个方法中各个参数在常量池中的字段符号引用的集合，也就是因为返回值不会包含在特征签名中，因此Java语言无法仅仅依靠返回值的不同来对一个已有方法进行重载。 属性表集合在Class文件、字段表、方法表都可以携带自己的属性表集合，以用于描述某些场景专有的信息。 12u2 attributes_count; //此类的属性表中的属性数attribute_info attributes[attributes_count]; //属性表集合 属性表的限制相对其他的数据项目稍微宽松了一些，不再要求各个属性表具有严格的顺序，并且只要不与已有属性名重复，任何人实现的编译器都可以向属性表中写入自己定义的属性信息，Java虚拟机运行时会忽略它不认识的属性。 对于每个属性，它的名称需要从常量池中引用一个CONSTANT_Utf8_info类型的常量来标识，而属性值的结构则是完全自定义的，只需要通过一个u4的长度属性去说明属性值所占用的位数即可。 Code属性Java程序方法体中的代码经过Javac编译器处理后，最后变为字节码指令存储在Code属性内。Code属性出现在方法表的属性集合之中，但并非所有的方法表都必须存在这个属性，譬如接口或者抽象类中的方法就不存在Code属性。 attribute_name_index是一项指向CONSTANT_Utf8_info型常量的索引，常量值固定为“Code”，它代表该属性的属性名称。 attribute_length指示了属性值的长度，由于属性名称索引与属性长度一共为6字节，所以属性值的长度固定为整个属性表长度减去6个字节。 max_stack代表了操作栈数深度的最大值。 max_locals代表了局部变量表所需的存储空间，单位是Slot，Slot时虚拟机为局部变量分配内存所使用的最小单位。对于byte、char、float、int、short、boolean和returnAdress等长度不超过32位的数据类型，每个局部变量占用1个Slot，而double和long这种64位的数据类型则需要两个Slot来存放。并不是在方法中用到了多少个局部变量，就把这些局部变量所占Slot之和作为max_locals的值，原因是局部变量表中的Slot可以重用，当代码执行超过一个局部变量的作用域时，这个局部变量所占的Slot可以被其他局部变量所使用，Javac编译器会根据变量的作用域来分配Slot给各个变量使用，然后计算出max_locals的大小。 code_length和code用来存储Java源程序编译后生成的字节码指令，code_length代表字节码长度，code是用于存储字节码指令的一系列字节流，每个指令长度为u1类型的单字节，虚拟机每次读入一个字节码指令。关于code_length，虽然他是一个u4类型的长度值，但虚拟机明确限制了一个方法不允许超过65535条字节码指令，即它实际只使用了u2的长度。 在实例方法的局部变量表中至少会存在一个指向当前对象实例的局部变量，局部变量表中也会预留出一个Slot位来存放对象实例的引用，方法参数值从1开始计算。 exception_table：异常表如下。如果当字节码在第start_pc行到第end_pc行之间（不含end_pc行）出现了类型为catch_type或者其子类的异常（catch_type为指向一个CONSTANT_Class_info型常量的引用），则转到第handler_pc行继续处理。当catch_type的值为0时，代表任意异常情况都需要转向到handler_pc行进行处理。 Exceptions属性Exceptions属性的作用是列举出方法中可能抛出的受查异常（Checked Exceptions），也就是方法描述时在throws关键字后面列举的异常。Exceptions属性中的number_of_exceptions项表示方法可能抛出number_of_exceptions种受查异常。每一种受查异常使用一个number_index_table项表示，exception_index_table是一个指向常量池中CONSTANT_Class_info型常量的索引，代表了该受查异常的类型。 LineNumberTable属性LineNumberTable属性用于描述Java源码行号与字节码（字节码的偏移量）之间的对应关系。line_number_table是一个数量为line_number_table_length、类型为line_number_info的集合，line_number_info表包括了start_pc和line_number两个u2类型的数据项，前者是字节码行号，后者是Java源码行号。 LocalVariableTable属性LocalVariableTable属性用于描述栈帧中局部变量表中与Java源码中定义的变量之间的关系。 start_pc和length属性分别代表了这个局部变量的生命周期开始地字节码偏移量及其作用范围覆盖的长度，两者结合起来就是这个局部变量在字节码之中的作用域范围。 name_index和descriptor_index都是指向常量池中CONSTANT_Utf8_info型常量的索引，分别代表了局部变量的名称以及这个局部变量的描述符。 index是这个局部变量在栈帧局部变量表中Slot的位置。当这个变量数据类型是64位类型时（double和long），他占用的Slot为index和index+1两个。 在JDK1.5引入泛型之后，LocalVariableTable属性增加了一个“姐妹属性”：LocalVariableTypeTable，这个新增的属性结构与LocalVariableTable非常相似，仅仅是吧记录的字段描述符的descriptor_index替换成了字段的特征签名（Signature），对于非泛型类型来说，描述符和特征签名能描述的信息是基本一致的，但是泛型引入后，由于描述符中反省的参数化类型被擦除掉，描述符就不能准确的描述泛型类型了，因此出现了LocalVariableTypeTable。 SourceFile属性SourceFile属性用于记录生成这个Class文件的源码文件名称。sourcefile_index数据项是指向常量池中CONSTANT_Utf8_info型常量的索引，常量值是源码文件的文件名。 ConstantValue属性ConstantValue属性的作用是通知虚拟机自动为静态变量赋值。只有被static关键字修饰的变量（类变量）才可以使用这项属性。对于非static类型的变量的赋值是在实力构造器方法中进行的；而对于类变量，则有两种方式可以选择：再类构造器方法中或者使用ConstantValue属性。 从数据结构中可以看出，ConstantValue属性是一个定长属性，他的attribute_length数据项值必须固定为2。constantvalue_index数据项代表了常量池中一个字面量常量的引用，根据字段类型的不同，字面量可以是CONSTANT_Long_info、CONSTANT_Float_info、CONSTANT_Double_info、CONSTANT_Integer_info、CONSTANT_String_info常量中的一种。 InnerClasses属性Inner属性用于记录内部类与宿主类之间的关联。如果一个类中定义了内部类，那编译器将会为它以及它所包含的内部类生成InnerClasses属性。 Deprecated及Synthetic属性Depreciated和Syntactic两个属性都属于标志类型的布尔属性，只存在有和没有的区别，没有属性值的概念。 Depreciated属性用于表示某个类、字段或者方法，已经被程序作者定为不再推荐使用，它可以通过在代码中使用@deprecated注解进行设置。 Syntactic属性代表此字段或者方法并不是由Java源码直接产生的，而是由编译器自行添加的。唯一例外的是实例构造器方法和类构造器方法。 StackMapTable属性StackMapTable属性在JDK1.6发布后增加到Class文件规范中，它是一个复杂的变长属性，位于Code属性的属性表中。 这个属性会在虚拟机类加载的字节码验证阶段被新类型检查验证器使用，目的在于代替以前比较消耗性能的基于数据流分析的类型推导验证器。 Signature属性Signature属性在KJDk1.5发布后增加到了Class文件规范中，他是一个可选的定长类属性，可以出现于类、字段表、和方法表结构的属性表中。 在JDK 1.5大幅增强了Java语言的语法，在此之后，任何类、接口、初始化方法或成员的泛型签名如果包含了类型变量（Type Variables）或参数化类型（Parameterized Types），则Signature属性会为它记录泛型签名信息。之所以要专门使用这样一个属性去记录泛型类型，是因为Java语言的泛型采用的是擦除法实现的伪泛型。 使用擦除法的好处是实现简单（主要修改Javac编译器，虚拟机内部只做了很少的改动）、非常容易实现Backport，运行期也能够节省一些类型所占的内存空间。但坏处是运行期就无法像C#等有真泛型支持的语言那样，将泛型类型与用户定义的普通类型同等对待，例如运行期做反射时无法获得到泛型信息。Signature属性就是为了弥补这个缺陷而增设的。 BootstrapMethods属性BootstrapMethods属性在JDK1.7发布后增加到了Class文件规范之中，它是一个复杂的变长属性，位于类文件的属性表中。这个属性用于保存invokeddynamic指令引用的引导方法限定符。目前的Javac暂时无法生成InvokeDynamic指令和BootstrapMethods属性，必须通过一些非常规的手段才能使用它们。]]></content>
      <categories>
        <category>Java虚拟机</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[虚拟机性能监控与故障处理工具]]></title>
    <url>%2Fposts%2F61312%2F</url>
    <content type="text"><![CDATA[JDK的命令行工具JDK的bin目录中除了有我们所熟知的“java.exe”、“Javac.exe”这两个命令行工具，还有也许我们并不了解的其他命令行工具。这里我们主要介绍一些用于监视虚拟机和故障处理的工具，这些关工具都非常稳定而且功能强大，能在处理应用程序性能问题、定位故障时发挥很大的作用。 这些命令行工具大多数是jdk/lib/tool.jar类库的一层薄包装而已，它们主要的功能代码是在tools类库中实现的。JDK开发团队选择采用Java代码来实现这些监控工具是有特别用意的：当应用程序部署到生产环境后，无论是直接接触物理服务器还是远程Telnet到服务器上都可能会受到限制，借助tools.jar类库里面的接口，我们可以直接在应用程序中实现功能强大的监控分析功能。 Sun JDK 监控和故障处理工具： 名称 主要作用 jps JVM Process Status Tool ， 显示指定系统内所有的HotSpot虚拟机进程 jstat JVM Statistics Monitoring Tool ， 用于收集HotSpot虚拟机各方面的运行数据 jinfo Configuration In for Java ， 显示虚拟机配置信息 jmap Memory Map for Java ， 生成虚拟机的内存转储快照（heapdump文件） jhat JVM Heap Dump Browser ， 用于分析headgump文件，它会建立一个HTTP/HTML服务器，让用户可以在浏览器上查看分析结果 jstack Stack Trace for Java ， 显示虚拟机的线程快照 jps：虚拟机进程状况工具JDK的很多小工具的命名方式采用了UNIX命令的命名方式，jps就是其中的典型，它的作用就是类似于UNIX的ps命令： 可以列出正在运行的虚拟机进程，并显示虚拟机执行主类（Main Class，main() 函数所在的类）名称以及这些进程的本地虚拟机唯一ID（Local Virtual Mechain Identifier，LVMID）。对于本地虚拟机进程来说，LVMID与操作系统的进程ID是一致的。 jps命令格式： 1jps [options] [hostid] jps工具主要选项： 选项 作用 -q 只输出LVMID，省略主类的名称 -m 输出虚拟机进程启动时，传递给主类main()函数的参数 -l 输出主类全名，如果进程执行的是Jar包，输出Jar路径 -v 输出虚拟机启动时JVM参数 jstat：虚拟机统计信息监视工具jstat是用于监视虚拟机各种运行状态信息的命令行工具。它可以显示虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。 jstat命令格式： 1jstat [option vmid/lvmid [interval[s|ms] [count]] ] vmid/lvmid分别是远程虚拟机进程与本地虚拟机进程。参数interval和count代表查询间隔和次数，如果省略这两个参数，说明只查询一次。 option主要分为三类：类装载、垃圾收集、运行期编译。jstat工具主要选项： 选项 作用 -class 监视类装载、卸载数量、总空间以及类装载所耗费的时间 -gc 监视Java堆状况，包括Eden区、两个Survivor区、老年代、永久代等的容量、已用空间、GC时间合计等信息 -gccapacity 监视内容与-gc基本相同，但输出主要关注Java堆各个区域使用到的最大、最小空间 -gcutil 监视内容与-gc基本相同，但输出主要关注已使用空间占总空间的百分比 -gccause 与-gcutil功能一样，但是会额外输出导致上一次GC产生的原因 -gcnew 监视新生代GC状况 -gcnewcapacity 监视内容与-gcnew基本相同，输出主要关注使用到的最大、最小空间 -gcold 监视老年代GC状况 -gcoldcapacity 监视内容与-gcold基本相同，输出主要关注使用到的最大、最小空间 -gcpermcapacity 输出永久代使用到的最大、最小空间 -compiler 输出JIT编译器编译过的方法、耗时等信息 -printcompilation 输出已经被JIT编译过的方法 jinfo：Java配置信息工具jinfo的作用是实时地查看和调整虚拟机各项参数。 jinfo的-flag选项可以查询未被显式指定的参数的系统默认值，可以使用-flag [+|-]或者-flag name=value修改一部分运行期可写的虚拟机参数值 。jinfo还可以使用-sysprops选项把虚拟机进程的System.getProperties()的内容打印出来。 jamp：Java内存映像工具jmap命令用于生成堆转储快照（一般为heapdump或者dump文件），如果不使用 jmap 命令，要想获取 Java 堆转储，可以使用 “-XX:+HeapDumpOnOutOfMemoryError” 参数，可以让虚拟机在 OOM 异常出现之后自动生成 dump 文件，Linux 命令下可以通过 kill -3 发送进程退出信号也能拿到 dump 文件。它还可以查询finalize执行队列、Java堆和永久代的详细信息，如空间使用率、当前使用的是哪种收集器等。和jinfo一样，jmap有不少功能在 Windows 平台下也是受限制的。 jamp命令格式： 1jmap [option] vmid jmap工具主要选项： 选项 作用 -dump 生成Java堆转出快照。格式为：-dump[live,]format=b,file=,其中live子参数说明是否只dump出存活对象 -finalizerinfo 显示在F-Queue中等待Finalizer线程等待执行finalize方法的对象。只在Linux/Solaris平台下有效 -heap 显示Java堆详细信息，如使用哪种回收器、参数配置、分代状况等。 -histo 显示堆中对象统计信息，包括类、实例数量、合计容量 -permstat 已ClassLoader为统计口径，显示永久代内存状况。只在Linux/Solaris平台下有效 -F 当虚拟机进程堆-dump选项没有响应时，可使用这个选项强制生成dump快照。只在Linux/Solaris平台下有效 jhat：虚拟机堆转储快照分析工具jhat与jmap搭配使用，来分析jmap生成的堆转储快照，可以在浏览器中查看。 但是一般不会直接使用jhat命令来分析dump文件，主要原因有二： 一般不会在部署应用程序的服务器上直接分析dump文件，即使可以这样做，也会尽量将dump文件复制到其他机器上分析，因为分析是一个耗时而且消耗硬件资源的过程，尽然都要在其他机器上进行，就没有必要受到命令行工具的限制了。 jhat的分析功能相对来说比较简陋，有更强的工具可以代替它。 jamp命令格式： 1jhat filename jstack：Java堆栈跟踪工具jstack命令用于生成虚拟机当前时刻的线程快照（一般称为threaddump或者javacore文件）。线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做什么，或者等待什么资源。 线程快照就是当前虚拟机每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如线程死锁、死循环、请求外部资源导致的长时间等待等都是导致线程长时间停顿的常见原因。 jstack命令格式： 1jstack [option] vmid jmap工具主要选项： 选项 作用 -F 当正常输出的请求不被响应时，强制输出线程堆栈 -l 除堆栈外，显示关于锁的附加信息 -m 如果调用到本地方法的话，可以显示C/C++的堆栈 JDK的可视化工具JDK中除了提供大量的命令行工具外，还有两个功能强大的可视化工具：JConsole和VisualVM。 JConsole：Java监视与管理控制台JConsole是一种基于JMX的可视化监视、管理工具。它管理部分的功能是针对JMX MBean进行管理。 启动JConsole提供JDK/bin目录下的“jconsole.exe”启动JConsole后，将自动搜索出本机运行的所有虚拟机进程，双击选择其中一个进程即可开始监控，也可以使用下面的“远程进程”功能来连接远程服务器，对远程虚拟机进行监控。“概述”页签显示的是整个虚拟机主要运行数据的概览，其中包括“堆内存使用情况”、“线程”、“类”、“CPU使用情况”4种信息的曲线图。 内存监控JConsole可以显示当前内存的详细信息，不仅包括堆内存/非堆内存的整体信息，还可以细化到Eden区、Survivor区等的使用情况。点击右边的“执行 GC(G)”按钮可以强制应用程序执行一个Full GC。 线程监控类似于jstack命令，遇到线程停顿时可以使用这个页签进行监控分析。最下面有一个”检测死锁 (D)”按钮，点击这个按钮可以自动为你找到发生死锁的线程以及它们的详细信息 。 VisualVM：多合一故障处理工具VisualVM是到目前为止随JDK发布的功能最强大的运行监视和故障处理程序。它除了运行监视、故障处理外，还提供了很多其他方面的功能，如性能分析。 VisualVM还有一个很大的优点：不需要被监视的程序基于特殊Agent运行，因此它对应用程序的实际性能的影响很小，使得它可以直接应用在生产环境中。 VisualVM 基于 NetBeans 平台开发，因此他一开始就具备了插件扩展功能的特性，通过插件扩展支持，VisualVM 可以做到： 显示虚拟机进程以及进程的配置、环境信息（jps、jinfo）。 监视应用程序的CPU、GC、堆、方法区以及线程的信息（jstat、jstack）。 dump以及分析堆转储快照（jmap、jhat）。 方法级的程序运行性能分析，找出被顶用最多、运行时间最长的方法。 离线程序快照：收集程序的运行时配置、线程dump、内存dump等信息建立一个快照，可以将快照发送开发者除进行Bug反馈。 其他plugins的无限的可能性……]]></content>
      <categories>
        <category>Java虚拟机</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[内存分配与回收策略]]></title>
    <url>%2Fposts%2F44680%2F</url>
    <content type="text"><![CDATA[Java技术体系中所提倡的自动内存管理最终可以归结为自动化地解决了两个问题：给对象分配内存以及回收分配给对象的内存。 对象的内存分配，往大方向讲，就是在堆上分配，对象主要分配在新生代的Eden区上，如果启动了本地线程分配缓冲（TLAB），将按线程优先在TLAB上分配。少数情况下也可能会直接分配在老年代中，分配堆规则并不是百分之百固定的。 新生代GC和老年代GC在了解分配策略之前，先了解一下新生代（Minor）GC和老年代（Full/Major）GC有什么不同。 新生代GC（Minor GC）：指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC非常频繁，一般回收速度也比较快。 老年代GC（Major/Full GC）：指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC（但并非绝对，在Parallel Scavenge收集器的收集策略里就有直接进行Major GC的策略选择过程）。Major GC的速度一般会比Minor GC慢10倍以上。 对象优先在Eden分配大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够的空间进行分配时，虚拟机将发起一次Minor GC。 当给一个对象分配内存的的时候，发现Eden已经被占用的了一部分,剩余的空间已不足以分配当前对象所需的内存，因此发生Minor GC。GC期间虚拟机首先尝试把Eden中的对象放入Survivor空间中，如果Survivor中的空间大小不足的话，就会通过分配担保机制提前转移到老年代去。 大对象直接进入老年代所谓的大对象是指，需要大量连续内存空间的Java对象，最典型的大对象就是那种很长的字符串以及数组。 虚拟机提供了一个-XX:PretenureSizeThreshold参数，令大于这个设置值的对象直接在老年代分配，这样做的目的是避免Eden区及两个Survivor区之间发生大量的内存复制（新生代采用复制算法收集内存）。 长期存活的对象将进入老年代既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。 如果对象在Eden出生经过一次Minor GC后仍然存活，并且能够被Survivor容纳的话，将被移动到Survivor空间中，并且对象年龄设为1。 对象在Survivor区中每熬过一次Minor GC，年龄就增加1岁，当他的年龄增加到一定程度（默认为15岁），就会被晋升到老年代中。对象晋升到老年代中的阈值，可以通过参数-XX:MaxTenuringThreshold设置。 动态对象年龄判定为了更好的适应不同程序的内存状况，虚拟机并不是永远的要求对象的年龄达到了阈值才能晋升到老年代中，如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到MaxTenuringThreshold中要求的年龄。 空间分配担保在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那么Minor GC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管这次Minor GC是有风险的；如果小于，或者HandlePromotionFailure设置不允许冒险，那这时也要改为进行一次Full GC。 取平均值进行比较其实仍然是一种动态概率的手段，也就是说，如果某次Minor GC存活后的对象突增，远远高于平均值的话，依然会导致担保失败（Handle Promotion Failure）。如果出现了HandlePromotionFailure失败，那就只好在失败后重新发起一次Full GC。 虽然担保失败时绕的圈子是最大的，但大部分情况下都还是会将HandlePromotionFailure开关打开，避免Full GC过于频繁。]]></content>
      <categories>
        <category>Java虚拟机</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[理解GC日志]]></title>
    <url>%2Fposts%2F41052%2F</url>
    <content type="text"><![CDATA[每一种收集器的日志形式都是由它们自身的实现所决定的,换而言之,每个收集器的日志格式都可以不一样。但虚拟机设计者为了方便用户阅读,将各个收集器的日志都维持一定的共性,例如以下两段典型的GC日志: 12333.125: [GC [DefNew: 3324K-&gt;152K(3712K), 0.0025925 secs] 3324K-&gt;152K(11904K), 0.0031680secs]100.667: [Full GC [Tenured: 0K-&gt;210K(10240K), 0.0149142 secs] 4603K-&gt;210K(19456K), [Perm : 2999K-&gt;2999K(21248K)], 0.0150007 secs] [Times: user=0.01 sys=0.00, real=0.02 secs] 最前面的数字“33.125”和“100.667”：代表了GC发生的时间，这个数字的含义是从Java虚拟机启动以来经过的秒数。 GC日志开头的“[GC”和“[Full GC”说明了这次垃圾收集的停顿类型，而不是用来区分新生代GC还是老年代GC的。如果有“Full”，说明这次GC是发生了Stop-The-World的。 接下来的“[DefNew”、“[Tenured”、“[Perm”表示GC发生的区域，这里显示的区域名称与使用的GC收集器是密切相关的。例如上面Serial收集器中的新生代名为“Default NewGeneration”，所以显示的收集“[DefNew”。如果是ParNew收集器，新生代名称就会变为“[ParNew”，意为“Parallel New Generation”。如果采用Parallel Scavenge收集器，那它配套的新生代为“PSYoungGen”，老年代和永久代同理,名称也是由收集器决定的。 后面方括号内部的“3324K-&gt;152K(3712K)” 含义是 “GC前该区域已使用容量 -&gt; GC后该区域已使用容量（该内存区域总容量）”。 而在方括号之外的“3324K-&gt;152K(11904K)” 表示 “GC前Java堆已使用容量 -&gt; GC后Java堆已使用容量（Java堆总容量）”。 再往后，“0.0031680secs”表示该内存区域GC所占用的时间，单位是秒。有的收集器会给出更具体的时间数据，如“[Times: user=0.01 sys=0.00, real=0.02 secs]”，这里面的user、sys和real与Linux的time命令所输出的含义一致，分别代表用户态消耗的CPU时间、内核态消耗的CPU时间和操作从开始到结束所经过的墙钟时间。 CPU时间与墙钟时间的区别是,墙钟时间包括各种非运算的等待耗时,例如等待磁盘I/O、等待线程阻塞,而CPU时间不包括这些耗时,但当系统有多CPU或者多核的话,多线程操作会叠加这些CPU时间,所以读者看到user或sys时间超过real时间是完全正常的。]]></content>
      <categories>
        <category>Java虚拟机</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JVM垃圾收集器]]></title>
    <url>%2Fposts%2F51819%2F</url>
    <content type="text"><![CDATA[本文索引关于内存分配和回收策略会在下一篇博文中讲解，本文就主要讲解后面三个关于GC的问题。 对象已经死亡吗？在里面存放着Java世界中几乎所有的对象实例，垃圾收集器在对堆进行回收之前，第一件事情就是要确定这些对象之中那些还“存活”着，哪些已经“死去”（即不可能再被任何途径使用的对象）。 引用计数算法引用计数法的算法是这样的：给对象添加一个引用计数器，每当有一个地方引用它时，计数器的值就加一；当引用失效时，计数器的值就减一；任何时刻计数器为零的对象就是不可能再被使用的。 虽然客观的说，引用计数算法的实现很简单，判定效率也很高，在大部分情况下它都是一个不错的算法。但是，至少主流的Java虚拟机里面没有选用引用计数算法来管理内存，其中最主要的原因就是它很难解决对象之间相互循环引用的问题。 具体的例子如下： 1234567891011public class ReferenceCountingGc &#123; Object instance = null; public static void main(String[] args) &#123; ReferenceCountingGc objA = new ReferenceCountingGc(); ReferenceCountingGc objB = new ReferenceCountingGc(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; &#125;&#125; 对象objA和objB都有字段instance，赋值令 objA.instance = objB及objB.instance = objA，除此之外，这两个对象再无任何引用，实际上这两个对象不可能再被访问，但是他们因为互相引用着对方，导致它们的引用计数都不为零，于是引用计数算法无法通知GC回收它们。 可达性分析算法这个算法的基本思路就是通过一系列的称为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连（用图论的话来说，就是从GC Roots到这个对象不可达）时，则证明此对象是不可用的。 在Java语言中，可作为GC Roots的对象包括下面几种： 虚拟机栈（栈帧中的本地变量表）中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法中JNI（即一般说的Native方法）引用的对象 再谈引用在JDK1.2之前，Java中引用的定义很传统：如果reference类型的数据存储的数值代表的是另一块内存的起始地址，就称这块内存代表一个引用在JDK1.2之后，Java对引用的概念进行了扩充，将引用分为 强引用、软引用、弱引用、虚引用 四种，这四种引用强度依次逐渐减弱。 强引用强引用（Strong Reference）就是指在程序代码中普遍存在的，类似Object obj = new Object()这类的引用，我们使用的大部分引用实际上都是强引用。 只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象。当内存空间不足，Java虚拟机宁愿抛出 OutOfMemoryError错误，使程序异常终止，也不会随意回收具有强引用的对象来解决内存不足的问题。 软引用软引用（Soft Reference）是用来描述一些还有用，但是并非必需的对象。 如果内存空间足够，垃圾收集器就不会回收它；如果内存空间不足，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收。如果这次回收还没有足够内存的话，才会抛出内存溢出异常。 弱引用弱引用（Weak Reference）也是用来描述非必需的对象，但是它的强度更弱，被弱引用关联的对象只能生存到下一次垃圾收集发生之前。 当垃圾收集器工作时，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。不过由于垃圾收集器是一个优先级很低的线程，因此不一定会很快发现那些只具有弱引用的对象。 虚引用虚引用（Phantom Reference）顾名思义，就是形同虚设，它是最弱的一种引用关系。 一个对象是否具有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象的实例。 为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。 在程序中一般很少使用弱引用和虚引用，使用软引用的情况比较多，因为软引用可以加速Java虚拟机堆垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemoryError）等问题发生。 生存还是死亡即使在可达性分析算法中不可达的对象，也并非是“非死不可”的，要真正宣告一个对象死亡，至少要经历两次标记过程： 如果对象在可达性分析后发现没有与GC Roots相连接的引用链，那它将会被标记并且进行第一次筛选，筛选的条件是此对象是否有必要执行finalize（）方法。当对象没有覆盖finalize（）方法，或者finalize（）方法已经被虚拟机调用过，虚拟机将这两种情况都视为“没有必要执行”。 如果这个对象被判定有必要执行finalize（）方法，那么这个对象将会被放置在一个叫做F-Queue的队列之中，并在稍后由一个由虚拟机自动建立的、低优先级的Finalizer线程去执行它。 被判定为需要执行的对象，将会被放在F-Queue队列中进行二次标记。如果对象在finalize（）中成功拯救了自己————只要重新与引用链上的任何一个对象建立关联即可，譬如把自己赋值给某个类变量或者对象的成员变量，那在第二次标记的时候它将被移除出“即将回收”的集合；如果这个对象这时候还没有逃脱，那他基本上就真的被回收了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/* * 此代码演示了两点： * 1.对象可以再被GC时自我拯救 * 2.这种自救的机会只有一次，因为一个对象的finalize()方法最多只会被系统自动调用一次 * */public class FinalizeEscapeGC &#123; public String name; public static FinalizeEscapeGC SAVE_HOOK = null; public FinalizeEscapeGC(String name) &#123; this.name = name; &#125; public void isAlive() &#123; System.out.println("yes, i am still alive :)"); &#125; @Override protected void finalize() throws Throwable &#123; super.finalize(); System.out.println("finalize method executed!"); System.out.println(this); FinalizeEscapeGC.SAVE_HOOK = this; &#125; @Override public String toString() &#123; return name; &#125; public static void main(String[] args) throws InterruptedException &#123; SAVE_HOOK = new FinalizeEscapeGC("leesf"); System.out.println(SAVE_HOOK); // 对象第一次拯救自己 SAVE_HOOK = null; System.out.println(SAVE_HOOK); System.gc(); // 因为finalize方法优先级很低，所以暂停0.5秒以等待它 Thread.sleep(500); if (SAVE_HOOK != null) &#123; SAVE_HOOK.isAlive(); &#125; else &#123; System.out.println("no, i am dead : ("); &#125; // 下面这段代码与上面的完全相同,但是这一次自救却失败了 // 一个对象的finalize方法只会被调用一次 SAVE_HOOK = null; System.gc(); // 因为finalize方法优先级很低，所以暂停0.5秒以等待它 Thread.sleep(500); if (SAVE_HOOK != null) &#123; SAVE_HOOK.isAlive(); &#125; else &#123; System.out.println("no, i am dead : ("); &#125; &#125;&#125; 运行结果： 123456leesfnullfinalize method executed!leesfyes, i am still alive :)no, i am dead : ( 从上面的运行结果可以看出，SAVE_HOOK对象的finalize（）方法确实被GC收集器触发过，并且在被收集前成功逃脱了。但是在第二次执行相同代码的时候，却逃脱失败，这是因为任何一个对象的finalize（）方法都只会被系统自动调用一次，如果对象面临下一次回收，它的finalize（）方法不会被再次执行，因此第二段代码的自救行动失败。 回收方法区很多人认为方法区是没有垃圾收集器的，虽然在方法区进行垃圾收集的性价比比较低，但是也并不代表在方法区去就一定没有垃圾收集的工作。 永久代的垃圾收集主要回收两部分内容：废弃常量和无用的类。 回收废弃常量与回收Java堆中的对象非常相似，以字符串为例，如果当前没有任何String对象引用常量池中的该字符串常量，也没有其他地方引用了这个字面量，就说明这个字符串常量为废弃常量。如果这是发生内存回收，而且有必要的话，这个常量就会被系统清理出常量池。 而判定一个类是否为“无用的类”的条件则苛刻的多，类需要同时满足下面三个条件才能算是“无用的类”： 该类所有的实例都已经被回收，也就是Java堆中不存在该类的任何实例。 加载该类的ClassLoader已经被回收。 该类对应的java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类。 虚拟机可以对满足上述三个条件的无用类进行回收，这里说的仅仅是“可以”，并不是必然被回收。 垃圾收集算法由于垃圾收集算法的实现涉及大量的程序细节，且各个平台的虚拟机操作内存的方法又各不相同，因此这里不过多的讨论实现的细节，仅介绍几种算法的思想。 标记-清除算法最基础的算法就是“标记-清除（Mark-Sweep）”算法，顾名思义，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。 它的标记过程在前一节已经介绍过了，而且之所以说它是最基础的算法，因为后续的收集算法都是基于这种思路，并对其不足进行改进而得到的。 它的不足有两个： 效率问题：标记和清除两个过程的效率都不高。 空间问题：标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后再程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 复制算法复制算法是为了解决标记-清除算法的效率问题，它将内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还活着的对象复制到另一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为了原来的一半，未免有点太高。 有统计表示，新生代中的对象98%是“朝生息死”的，所以并不需要按照1：1的比例来划分内存空间。 而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor。当Eden满了时，触发一次Minor GC，然后将Eden和Survivor中还存活着的对象一次性的复制到另一块Survivor空间上（这个过程非常重要，因为这种复制算法保证了S1中来自S0和Eden两部分的存活对象占用连续的内存空间，避免了碎片化的发生），最后清理掉Eden和刚刚使用过的Survivor空间。HotSpot虚拟机默认Eden和Survivor的大小比例是8：1。 如此循环往复，如果对象的复制次数达到了16次，该对象就会被送到老年代中。 其次当Survivor空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保。大对象（就是需要大量连续内存空间的对象）直接进入老年代，因为这样做为了避免大对象分配内存时由于分配担保机制带来的复制而降低效率。 复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低。 标记-整理算法因为复制算法的缺点，根据老年代的特点，有人提出另一种“标记-整理”算法。 标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 分代收集算法当前商业虚拟机的垃圾收集都采用“分代收集”算法。这种算法把“复制算法”和“标记-整理”结合起来，根据对象存活周期的不同将内存划分为几块，把Java堆划分为新生代和老年代，根据各个年代的特点选择合适的垃圾收集算法。 新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或者“标记-整理”算法来进行回收。 垃圾收集器如果说收集算法时内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。虽然我们对各个收集器进行比较，但并非要挑选出一个最好的收集器。因为直到现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，我们能做的就是根据具体应用场景选择适合自己的垃圾收集器。 Serial收集器Serial收集器是最基本、发展历史最悠久的收集器。这个收集器是一个单线程的收集器，但它的“单线程”意义并不仅仅说明它只会使用一个CPU或者一条收集线程去完成垃圾收集工作，更重要的是他在进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集完成。新生代采用复制算法，老年代采用标记-整理算法。 虽然现在一个个越来越优秀的收集器出现，用户线程的停顿时间在不断缩短，但是仍然没有办法完全消除。 但实际上到现在为止，它依然是虚拟机运行在Client模式下的默认新生代收集器。它的优点如下：简单而高效，对于限定单个CPU的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。 ParNew收集器ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集之外，其余行为（控制参数、收集算法、回收策略等等）都与Serial收集器完全一样。新生代采用复制算法，老年代采用标记-整理算法。 它是许多运行在Server模式下的虚拟机的首要选择，除了Serial收集器外，只有它能与CMS收集器（真正意义上的并发收集器，后面会介绍到）配合工作。 ParNew收集器在单CPU的环境中绝对不会有比Serial收集器更好的效果。 并发和并行概念的补充 并行：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发：指用户线程与垃圾收集线程同时执行（但不一定是并行的，可能会交替执行），用户程序在继续执行，而垃圾收集程序运行在另一个CPU上。 Parallel Scavenge收集器Parallel Scavenge收集器收集算法和线程方面与ParNew收集器一样，但Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量，因此该收集器也被称为“吞吐量优先”收集器。 该收集器提供了两个参数用于精确控制吞吐量，分别控制最大垃圾收集停顿时间的-XX:MaxGCPauseMillis参数以及直接设置吞吐量大小的-XX:GCTimeRatio参数。 该收集器还有一个参数-XX:+UseAdaptiveSizePolicy值得关注，这是一个开关参数。当这个参数打开之后，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量，这种调节方式称为GC自适应的调节策略。这也是该收集器与ParNew收集器的一个重要区别。 Serial Old收集器该收集器是Serial收集器的老年代版本。它主要的两大用途： 在JDK1.5以及之前的版本中与Parallel Scavenge收集器搭配使用。 作为CMS收集器的后备预案。 Parallel Old收集器Parallel Old收集器是Parallel Scavenge收集器的老年代版本。在注重吞吐量以及CPU资源的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器。工作过程如图所示。 CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器，它是HotSpot虚拟机第一款真正意义上的并发收集器，他第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。 CMS收集器是基于“标记-清除”算法实现的，整个过程分为4个步骤： 初始标记：暂停所有的其他线程，并记录下直接与GC Roots能直接关联的对象，速度很快。 并发标记：同时开启GC和用户线程,从GC Roots开始对堆中对象进行可达性分析,找出存活的对象，这阶段耗时较长。 重新标记：暂停所有的其他线程，为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。 并发清除：开启所有线程，同时GC线程对为标记的区域做清扫。 因为它的性能优点，也称它为并发低停顿收集器。但是它有以下三个明显的缺点： CMS收集器对CPU资源非常敏感。在并发阶段虽然不会导致用户线程停顿，但是会因为占用一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量降低。 CMS收集器无法处理浮动垃圾，可能出现“Concurrent Mode Failure”失败而导致另一次Full GC的产生。由于用户程序在运行，那么自然就会有新的垃圾产生，这部分垃圾被标记过后，CMS无法在当次集中处理它们（为什么？原因在于CMS是以获取最短停顿时间为目标的，自然不可能在一次垃圾处理过程中花费太多时间），只好在下一次GC的时候处理。这部分未处理的垃圾就称为“浮动垃圾”。 这使得并发清除时需要预留一定的内存空间，不能像其他收集器在老年代几乎填满再进行收集。在JDK1.6中，CMS收集器启动阈值已经提升至92%。要是CMS运行期间的内存无法满足程序需要，就会出现一次“Concurrent Mode Failure”失败，这时虚拟机启用后备预案：临时启用Serail Old收集器，而导致另一次Full GC的产生。 收集结束时会有大量空间碎片产生。因为它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。，所以为了解决这个问题，CMS收集器提供了一个开关参数-XX:+UseCMSCompactAtFullCollection（默认开启），用于在CMS收集器顶不住要进行Full GC时开启内存碎片的合并整理过程，但是会导致停顿时间变长。 G1收集器G1收集器是当今收集器技术发展的最前沿成果之一，以极高概率满足GC停顿时间要求的同时，还具备高吞吐量性能特征。它具备以下特点： 并行与并发：G1能充分利用多CPu、多核环境下的硬件优势，使用多个CPU（或者CPU核心）来缩短Stop-The-Word停顿的时间，部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让Java程序继续运行。 分代收集：同其他收集器一样保留了分代的概念，但是它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次GC的旧对象以获取更好的收集效果。 空间整合：不同于CMS的“标记-清除”算法，G1从整体看是基于“标记-整理”算法实现的收集器，从局部（两个Region之间）上来看是基于“复制”算法是实现的，但无论如何，这两种算法都意味着G1运行期间不会产生内存空间碎片。 可预测的停顿：这是G1相对于CMS的另一大优势，G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M浩渺的时间片段内。 G1收集器中Java堆的内存布局与其他收集器有很大区别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留新生代和老生代的概念，但新生代和老生代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合。 G1跟踪各个Region里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间吨经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region（这也是Garbage-First名称的由来）。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了G1收集器在有限的时间内可以获取尽可能高的收集效率。 在G1收集器中，Region之间的对象引用以及其他收集器中的新生代与老年代之间的对象引用，虚拟机都是使用Remembered Set 来避免全堆扫描的。G1中每个Region都有一个与之对应的Remembered Set，如果不计算维护Remembered Set的操作，G1收集器的运作大致可划分为以下几个步骤： 初始标记：仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS（Next Top At Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的Region中创建新对象。这阶段需要停顿线程，但耗时很短。 并发标记：这阶段是从GC Roots开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。 最终标记：这阶段则是为了修正在并发标记阶段期间因用户程序继续运行而导致标记产生变化的那一部分标记记录，虚拟机将这段时间变化记录在线程Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这阶段需要停顿线程，但可并行执行。 筛选回收：最后首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划，这个阶段其实也可以做到与用户程序一起并发执行。 垃圾收集器参数总结]]></content>
      <categories>
        <category>Java虚拟机</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HotSpot 虚拟机对象探秘]]></title>
    <url>%2Fposts%2F58276%2F</url>
    <content type="text"><![CDATA[对象的创建下图便是一个Java对象创建的过程 类加载检查在Java程序运行期间无时无刻都有对象被创建出来，在语言层面来说，创建对象通常仅仅是一个new关键字而已，而在虚拟机中，遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 分配内存在类加载检查通过之后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可完全确定（具体将在下一节介绍）。 为对象分配内存空间的任务等于把一块确定大小的内存从Java堆中划分出来。分配方式有两种：“内存碰撞”和“空闲列表”，选择哪种方式由Java堆是否规整决定，而Java堆是否规整取决于虚拟机所采用的垃圾收集器是否带有压缩整理功能。 指针碰撞：Java堆中内存是绝对规整的，所有用过的内存放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那么分配内存就是把指针向空闲空间那边挪动对象大小的距离即可。 空闲列表：如果Java堆中内存并不是规整的，已使用的内存与空闲的内存相互交错，那么虚拟机就必须维护一个列表，记录那些内存是可用的，在分配的时候从列表中找到一块足够大的内存空间划分给对象实例，并更新列表上的记录。 因为在虚拟机中对象创建是一个非常频繁的行为，即使是仅仅修改一个指针所指向的位置，在并发的情况下也并不是线程安全的。可能出现正在给A对象分配内存，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况。所以对于内存分配引发的并发问题有两种解决方案：CAS+失败重试和本地线程分配缓冲（TLAB） CAS+失败重试：以这种方式保证更新操作的原子性。 本地线程分配缓冲（TLAB）：把内存分配堆动作按线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存，每个线程在自己的TLAB上分配内存，只有当对象大于TLAB中的剩余内存或者TLAB用完时，采用同步锁定（synchronized）的方式分配新的TLAB。 初始化零值内存分配完成后，虚拟机将对分配到的内存空间都初始化为零值（不包括对象头），如果使用TLAB，这一步可以提前至TLAB分配时进行。 这一步的操作保证了对象的实例字段在Java代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 设置对象头初始化零值之后，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息都存放在对象头中。 根据虚拟机当前的运行状态不同，如是否启用偏向锁等，对象头会有相应的不同的设置方式，具体会在下一节做详细介绍。 执行init方法在上面四个步骤完成之后，在Java虚拟机的角度，一个新的对象已经产生了；但是在Java程序的角度来看，对象的创建才刚开始，&lt;init&gt;方法还没有执行，所有的字段都是零。 所以一般来说，执行new指令之后会接着执行&lt;init&gt;方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 对象的内存布局在HotSpot虚拟机中，对象在内存中存储的布局可以分为3块区域：对象头（Header）、实例数据（Instance Data）和对齐填充（Padding）。 对象头HotSpot虚拟机的对象头包括两部分信息:Mark Word 和 类型指针（Class Pointer）。 Java虚拟机中对象头的方式有以下俩种（以32位Java虚拟机为例）：普通对象：数组对象： Mark Word第一部分用于存储对象自身的运行时数据（mark Word），如哈希码、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等。这部分数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为32bit和64bit，也就是一个Word的大小，官方称它为“Mark Word”。 对象需要存储的运行时数据很多时，如果超出了32位或64位Bitmap结构所能记录的限度。考虑到虚拟机的空间效率，Mark Word被设计成一个非固定的数据结构以便在极小的空间内存储尽量多信息，它会根据对象的状态复用自己的存储空间。 大部分情况下，Mark Word的32bit空间中的25bit用于存储对象哈希码，4bit用于存储对象分代年龄，2bit用于存储锁状态标志位，1bit用于标记对象是否启用偏向锁 不同锁状态标志位标记位表示的整个Mark Word含义不同，具体如下： 其中各部分的含义如下： lock : 2位的锁状态标记位，该标记的值不同，整个Mark Word表示的含义不同。 存储内容 biased_lock lock（标记位） 状态 对象的哈希码、分代年龄 0 01 无锁 偏向线程ID、偏向时间戳、对象分代年龄 1 01 偏向锁 指向锁记录的指针 0 00 轻量级锁 指向重量级锁的指针 0 10 重量级锁 空，不需要记录信息 0 11 GC标记 biased_lock : 只占1位，用于标记对象是否启用偏向锁。为1时表示对象启用偏向锁，为0时表示对象没有偏向锁。 age ： 4位的Java对象分代年龄。由于只有4位，所以最大值为15。 identity_hashcode ： 25位的对象标识哈希码，采用延迟加载技术。调用System.identityHashCode()计算，并会将结果写到该对象头中。当对象被锁定时，该值会移动到管程Monitor中。 thread ： 占23位，表示持有偏向锁的线程ID。 epoch ： 占2位，表示偏向时间戳。 ptr_to_lock_record ： 占30位，指向栈中锁记录的指针。 ptr_to_heavyweight_monitor ： 占30位，指向管程Monitor的指针。 类型指针对象头的另一部分就是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 该指针的位长度为JVM的一个字大小，即32位的JVM为32位，64位的JVM为64位。如果应用的对象过多，使用64位的指针将浪费大量内存，统计而言，64位的JVM将会比32位的JVM多耗费50%的内存。为了节约内存可以开启压缩指针（+UseCompressedOops），，其中，OOPS（ordinary object pointers），即普通对象指针。开启该选项后，下列指针将压缩至32位： 每个Class的属性指针（即静态变量） 每个对象的属性指针（即对象变量） 普通对象数组的每个元素指针 如果对象是一个Java数组，那在对象头中还必须有一块用于记录数组长度的数据，这部分数据的长度也随着JVM架构的不同而不同：32位的JVM上，长度为32位；64位JVM则为64位。 实例数据实例数据部分是对象真正存储的有效信息，也就是在程序代码中所定义的各种类型的字段。 这部分的存储顺序会受到虚拟机分配策略和字段在Java源码中定义的顺序的影响，HotSpot虚拟机默认的分配策略为longs/doubles、ints、shorts/chars、bytes/booleans、OOPS（ordinary object pointers），从这个分配策略可以看出，子类中较窄的变量也可能会插入到父类变量的空隙之中。 对齐填充第三部分对齐填充并不是必然存在的，也没有特别的含义，仅仅起着占位符的作用，因为 Hotspot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，换句话说就是对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 关于对象头的具体实现处，可以参考ArrayList的数组默认最大长度（Integer.MAX_VALUE - 8）。 对象的访问定位建立对象就是为了使用对象，我们的Java程序需要通过栈上的reference数据来操作堆上的具体对象。 而由于reference类型在Java虚拟机规范中只规定了一个指向对象的引用，没有定义这个引用该通过何种方式去定位、访问堆中的对象的具体位置。所以目前主流的访问方式有两种：句柄和直接指针。 句柄如果使用句柄访问的话，那么Java堆中将会划分出一块内存来作为句柄池，reference中储存的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息。如图所示： 直接指针如果使用直接指针，那么Java堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而reference在储存的直接就是对象的地址。如图所示： 句柄和直接指针对比这两种方式各有优势。 句柄方式的最大好处就是reference中存储的是稳定的句柄地址，在对象被移动（GC回收时移动对象是非常普遍的行为）时只会改变句柄中的示例数据指针，而reference本身不需要改变。 直接指针方式的最大好处就是速度更快，它节省了一次指针定位的时间开销。由于对象的访问在Java中非常频繁，因此这类开销积少成多后也是一项非常可观的执行成本。]]></content>
      <categories>
        <category>Java虚拟机</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java内存区域]]></title>
    <url>%2Fposts%2F35314%2F</url>
    <content type="text"><![CDATA[运行时数据区域Java虚拟机（JVM）在执行Java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。而在JDK1.8前后数据区域的划分略有不同，下面会介绍到。JDK1.8之前：JDK1.8： 因此根据上面的运行时数据区划分图可以看出： 线程私有的： 程序计数器 虚拟机栈 本地方法栈 线程共享的： 堆 方法区 直接内存（非运行时数据区的一部分） 下面就按照上面的顺序逐个进行了解。 程序计数器程序计数器是一块较小的内存空间。在虚拟机的概念模型里面，字节码解释器工作时需要知道该执行哪一条字节码指令，而程序计数器的作用就是，通过改变程序计数器的值，来让字节码解释器知道，下一条需要执行的指令是什么。 其次，Java虚拟机的多线程执行，是通过线程之间轮流执行，而对于一个处理器（如果是多核处理器，那么就是一个内核），在任意一个确定的时刻，只会执行一条线程中的指令。因此，为了避免一个线程过长时间（可能因为计算时间过长或者陷入死循环等原因）占用处理器，导致系统崩溃，所以处理器会给每个线程分配执行的时间，如果当分配的时间结束时，该线程的任务还没有执行完，处理器会被剥夺并分配给另一个线程，直到到达下一次该线程的时间片，处理器才会切换回来，继续执行该线程。因此，为了线程切换后能恢复到正确的执行位置，所以每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 从上面的介绍中我们知道程序计数器主要有两个作用： 字节码解释器通过改变程序计数器，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理等。 在多线程情况下，程序计数器用于记录当前线程执行的位置，从而当线程切换回来的时候，能够知道该线程上次执行到哪里，接下来该执行什么指令。 注意： 如果线程正在执行的是一个Java方法，那么这个程序计数器是正在执行的方法的虚拟机字节码指令的地址。 如果线程正在执行的是一个Native方法，那么这个程序计数器则为空（Undefined）。因此程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 Java虚拟机栈Java虚拟机栈描述的是Java方法的内存模型，每次方法调用的数据都是通过栈传递的。而栈中储存的是一个个的栈帧，栈帧就是每个方法在执行的时候都会创建一个栈帧（Stack Frame），栈帧用于储存局部变量表、操作数栈、动态链接、方法出口等信息。因为线程每调用一个方法从开始到结束，都意味着一个栈帧在虚拟机栈中从入栈到出栈的过程。 Java内存可以粗糙的分为堆内存（Heap）和栈内存（Stack），其中的栈就是Java虚拟机栈，或者说是Java虚拟机栈中的局部变量表部分。 局部变量表存放了编译器可知的各种（八种）基本数据类型（boolean、byte、char、short、int、float、double、long）、对象引用（不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）和returnAdress类型（指向一条字节码指令地址）。其中64位的长度的double和long类型的数据都会占用两个局部变量空间，其余数据只会占用一个局部变量空间。局部变量表所需内存空间在编译期间完成分配，因此当进入一个方法时，这个局部变量表的大小就已经完全确定了，运行期间不会改变其大小。 Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError StackOverFlowError:若Java虚拟机栈的内存大小不允许动态扩展，那么如果线程请求的栈深度大于虚拟机所允许的最大深度，那么就会抛出StackOverFlowError异常。 OutOfMemoryError：若Java虚拟机栈的内存大小允许动态扩展，那么如果线程在扩展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。 Java方法的返回方式有两种：return语句和抛出异常，不管哪种方法，都会导致栈帧出栈。 本地方法栈本地方法栈的作用与Java虚拟机栈的结构和作用几乎完全一样，可以认为二者唯一的区别就是：Java虚拟机栈为虚拟机执行Java方法（也就是字节码）服务；而本地方法栈为虚拟机执行Native方法服务。甚至在HotSpot虚拟机栈中将两者合二为一。 总结得到一点：程序计数器、Java虚拟机栈和本地方法栈都是线程所私有的，故而他们的生命周期和线程相同，它们的生命周期随着线程的创建而创建，随着线程的结束而死亡。 Java堆Java堆（Heap）是Java虚拟机所管理的内存中最大的一块。Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存的唯一目的就是：存放对象实例，几乎所有的对象实例都在这里分配，在Java虚拟机规范中的描述是：所有对象的实例以及数组都要在堆上分配。，但是随着JIT编译器的发展，这种情况也不是那么绝对的了。 java堆也是垃圾收集器管理的主要区域，因此也被称为 GC堆 ，从垃圾回收的角度看，Java堆中还可细分为：新生代和老生代；再度细分可分：Eden 空间、From Survivor、To Survivor 空间等为；大部分情况下，对象都会首先在Eden区域分配，再一次新生代垃圾回收后，如果对象还存活，则会进入s0或是s1在，并且对象年龄还加一，当他的年龄增加到一定程度（默认为15岁）时，就会被划分到老年代中。 不论如何划分，都与存放的内容无关；不论哪个区域，存储的都是对象的实例。进一步划分的目的是为了更好的回收内存，更快的分配内存。 Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可。 Java堆的内存大小可以是固定大小的，也可以是可扩展的（大部分都是）。如果在堆中没有内存来完成实例的分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。 方法区方法区同样是各个线程共享的内存区域，它用于储存已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。方法区有一个别名叫“非堆（Non-Heap）”，目的就是为了将其与Java堆区分开来。 仅在HotSpot虚拟机中，方法区也被称为“永久代”，仅仅是因为在HotSpot虚拟机中把GC分代收集扩展至方法区，这样可以省去专门为方法区编写内存管理代码的工作。但是问题也因此而来，因为永久代有大小上限，所以当触碰到内存大小的上限时，会抛出OutOfMemoryError异常。 所以在JDK1.8之后，永久代被彻底删除了，取而代之的是元空间（MetaSpace），与永久代有JVM本身内存大小上限的限制不同的是，元空间使用的是直接内存，受到的是本机可用内存的上限限制，只有当触碰到本地内存的极限时，才会抛出OutofMemoryError异常（概率极小）。 与java堆一样，方法区同样不需要连续的内存和可以选择固定大小或可扩展外，还可以选择不实现垃圾收集。相对而言垃圾收集行为在该区域比较少见，因为该区域内存回收目标主要是针对常量池的回收和对类型的卸载。 运行时常量池JDK1.7之前，运行时常量池是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息，用于存放编译期生成的各种字面量和符号引用。运行时常量池相对于Class文件常量池还有一个重要特征是具备动态性，将运行期间可能得到的新的常量放入池中。因此既然运行时常量池是方法区的一部分，所以当常量池无法再申请到内存时会抛出 OutOfMemoryError 异常。 但是在JDK1.7及其之后版本的JVM中，将运行时常量池从方法区中移了出来，在Java堆中开辟了一块内存存放运行时常量池，这样也更加方便于垃圾回收的工作。 直接内存直接内存既不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域，但是这一部分内存被频繁的使用，而且也可能导致OutofMemoryError异常。 在JDK1.4中新加入了NIO类，引入了一种基于通道（Channel）于缓存区（Buffer）的I/O方式，它使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在Java堆和Native堆之间来回复制数据。 虽然本机直接内存并不会收到Java堆的内存大小限制，但是显然会受到本地总内存的大小限制，因此也可能会在动态扩展时抛出OutOfMemoryError异常。]]></content>
      <categories>
        <category>Java虚拟机</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ArrayList源码阅读]]></title>
    <url>%2Fposts%2F33665%2F</url>
    <content type="text"><![CDATA[ArrayList的数据结构ArrayList往往被人用来与LinkedList对比，它们俩最重要的差异之一就是：ArrayList的底层是由数组组成的，而LinkedList的底层则是由链表组成，对于LinkedList不再多赘述，具体可以看一下LinkedList的文章。回到ArrayList中来，其实现的数据结构是一个名为elementData的Object数组，可以存放所有Object对象，因此我们对ArrayList类的实例的所有的操作底层都是基于这个数组的。 顶部注释 List接口的可调整大小的数组实现。 实现所有可选列表操作，并允许所有元素，包括null 。 除了实现List接口之外，该类还提供了一些方法来处理内部用于存储列表的数组的大小。 （这个类大致相当于Vector ，除了它是不同步的。）该size ， isEmpty ， get ， set ， iterator ，并listIterator操作在固定时间内运行。 add操作以摊销的常数运行 ，即添加n个元素需要O（n）个时间。 所有其他操作都以线性时间运行（粗略地说）。 与LinkedList实现相比，常数因子较低。 每个ArrayList实例都有一个容量 。 容量是用于存储列表中的元素的数组的大小。 它总是至少与列表大小一样大。 当元素添加到ArrayList时，其容量会自动增长。 没有规定增长政策的细节，除了添加元素具有不变的摊销时间成本。 在使用ensureCapacity操作添加大量元素之前，应用程序可以增加ArrayList实例的容量。 这可能会减少增量重新分配的数量。 请注意，此实现不同步。 如果多个线程同时访问ArrayList实例，并且至少有一个线程在结构上修改列表，则必须在外部进行同步。 （结构修改是添加或删除一个或多个元素的任何操作，或明确调整后台数组的大小;仅设置元素的值不是结构修改。）这通常是通过在一些自然地封装了名单。 如果没有这样的对象存在，列表应该使用Collections.synchronizedList方法“包装”。 这最好在创建时完成，以防止意外的不同步访问列表： List list = Collections.synchronizedList(new ArrayList(…)); 由这个类的iterator和listIterator方法返回的迭代器是故障快速的 ：如果列表在迭代器创建之后的任何时间被结构地修改，除了通过迭代器自己的remove或add方法之外，迭代器将抛出一个ConcurrentModificationException 。 因此，面对并发修改，迭代器将快速而干净地失败，而不是在未来未确定的时间冒着任意的非确定性行为。 请注意，迭代器的故障快速行为无法保证，因为一般来说，在不同步并发修改的情况下，无法做出任何硬性保证。 失败快速的迭代器ConcurrentModificationException扔出ConcurrentModificationException 。 因此，编写依赖于此异常的程序的正确性将是错误的： 迭代器的故障快速行为应仅用于检测错误。 这个类是Java Collections Framework的成员。 总结上面的顶部注释可以得到以下几点： 底部实现：可调整大小的数组实现的。 是否允许null值：允许所有元素，包括null。 是否是线程安全的：不是线程安全的。 迭代器： 迭代器是fast-fail，但是迭代器的快速失败行为不能得到保证。 运行时间：在get，set，size等操作中，都是以常数时间运行，而add操作需要O(n)时间运行。 ArrayList的定义1public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable ArrayList：支持泛型的存储模式。 extends AbstractList：继承于AbstractList，继承了其中的方法，方便操作。 implements List：实现了List接口，与继承AbstractList作用相同，实现该接口提供的方法，方便了实现。但是据开发这个collection 的作者Josh说：这其实是一个mistake，因为他写这代码的时候觉得这个会有用处，但是其实并没什么用，但因为没什么影响，就一直留到了现在。 implements RandomAccess：实现了RandomAccess接口，表明支持固定时间的快速随机访问，这也是其在get和set方法时已固定时间运行的原因 implements Cloneable：实现了Cloneable接口，内部可以调用clone()方法来返回实例的浅拷贝(shallow copy)。 implements Serializable：实现了Serializable接口，表明该类时可以序列化的。 静态全局变量1234567891011121314151617181920212223242526272829303132333435363738394041/** * 默认的初始容量 */ private static final int DEFAULT_CAPACITY = 10; /** * 用于空实例的共享空数组实例。 * 也就是说当传入的指定容量为0的时候建立数组。 */ private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;; /** * 共享空数组实例，用于默认大小的空实例。 * 我们将其与EMPTY_ELEMENTDATA区分开来，以了解添加第一个元素时应该膨胀多少。 * 当无指定的容量传入时，返回的数组。其与EMPTY_ELEMENTDATA的区别在于： * EMPTY_ELEMENTDATA是当传入的指定容量为时候返回的 * DEFAULTCAPACITY_EMPTY_ELEMENTDATA是为传入指定容量参数时候返回的。 */ private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;; /** * 存储ArrayList元素的数组缓冲区。 * ArrayList的容量是这个数组缓冲区的长度。 * 当添加第一个元素时，任何带有elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA的空ArrayList都将扩展为DEFAULT_CAPACITY。 * 也就是底层用来存储元素的数组 */ transient Object[] elementData; // 非私有以简化嵌套类访问,这里是用来为subList方法使用的。 /** * ArrayList的大小(它包含的元素的数量)。 * ArrayList中实际包含的元素的数量 * * @serial */ private int size; /** * 最大可分配的数组大小，减去8是为了一些vm在数组中保留一些头信息。 * 试图分配更大的数组可能会导致OutOfMemoryError:请求的数组大小超过VM限制 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; 构造方法指定初始容量12345678910111213141516171819202122232425/** * 构造具有指定初始容量的空列表。 * * @param initialCapacity 列表的初始容量 * @throws IllegalArgumentException 如果指定初始容量是负的 */ public ArrayList(int initialCapacity) &#123; /** * 如果指定的初始容量大于零，则创建一个指定初始容量大小的数组 */ if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; /** * 如果指定的初始等于零，则使用空数组EMPTY_ELEMENTDATA */ this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; /** * 如果指定的初始为负数，抛出异常 */ throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); &#125; &#125; 如果传入的指定初始容量大于零，那就创建一个指定初始容量大小的数组用来存放数据 如果指定的初始等于零，则使用静态全局变量中的空数组EMPTY_ELEMENTDATA 如果指定的初始为负数，抛出异常 无指定初始容量1234567 /** * 当无指定初始容量参数时，使用默认容量，构造一个初始容量为10的空列表。 * 但是其实在初始化后，此时的数组容量为0，当第一次存入数据时，才对这个空数组进行扩容，变为长度为10的数组 */public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125; 传入集合初始化123456789101112131415161718192021222324/** * 构造包含指定集合的元素的列表，按集合的迭代器返回元素的顺序排列。 * * @param c 要将其元素放入此列表的集合 * @throws NullPointerException 如果指定的集合为空 */public ArrayList(Collection&lt;? extends E&gt; c) &#123; /** * 把传入的集合转化为数组 */ elementData = c.toArray(); /** * 判断传入的集合是否为空，如果为空则初始化为EMPTY_ELEMENTDATA数组，也就是等于指定初始容量为0时的情况 */ if ((size = elementData.length) != 0) &#123; // 为了防止传入集合转型后的数组的类型不是Object类型，所以在这里进行验证 // 如果不是Object类型，则使用Arrays.copyOf()的方法重新拷贝成Object[].class类型 if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // 用空数组替换。 this.elementData = EMPTY_ELEMENTDATA; &#125;&#125; 核心方法trimToSize 方法123456789101112131415161718192021/** * 修改此ArrayList实例的容量成为列表的当前大小。 应用程序可以使用此操作来最小化ArrayList实例的存储。 */public void trimToSize() &#123; /** * 因为是对结构进行了修改，所以modCount加一次 */ modCount++; /** * 如果当前数组中的元素数量小于数组长度，就对数组进行修改 */ if (size &lt; elementData.length) &#123; /** * 如果数组中的元素数量为0，则把数组变为EMPTY_ELEMENTDATA * 如果数组中的元素数量不为0，则把当前数组中的所有元素拷贝到一个新的数组，数组长度为元素的数量 */ elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); &#125;&#125; 该方法用于回收多余的内存。也就是说一旦我们确定集合不在添加多余的元素之后，调用 trimToSize() 方法会将实现集合的数组大小刚好调整为集合元素的大小。注意：该方法会花时间来复制数组元素，所以应该在确定不会添加元素之后在调用。 ensureCapacity 方法12345678910111213141516171819/** * 如果需要，增加此 ArrayList实例的容量，以确保它至少能够容纳最小容量参数指定的元素数。 * * @param minCapacity 所需的最小容量 */public void ensureCapacity(int minCapacity) &#123; /** * 先判断是否满足增加容量的条件： * 1.新的容量大于当前数组的长度，不然没有必要扩容 * 2.数组中有数据，或者数组中没有数据并且新的容量大于默认的容量长度 * 满足上面的两个条件后，modCount加一，然后调用grow方法进行数组的扩容和复制 */ if (minCapacity &gt; elementData.length &amp;&amp; !(elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA &amp;&amp; minCapacity &lt;= DEFAULT_CAPACITY)) &#123; modCount++; grow(minCapacity); &#125;&#125; grow 方法1234567891011121314151617/** * 增加容量，以确保它至少可以容纳由最小容量参数指定的元素数目。 * * @param minCapacity 所需的最小容量 * @throws OutOfMemoryError 如果minCapacity小于零 */private Object[] grow(int minCapacity) &#123; /** * 使用newCapacity方法获得新的合适的容量大小，因为minCapacity不一定时最合适的扩容容量 */ return elementData = Arrays.copyOf(elementData, newCapacity(minCapacity));&#125;private Object[] grow() &#123; return grow(size + 1);&#125; newCapacity 方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 返回至少与给定的最小容量相同大的容量。返回当前容量增加50%(如果足够的话)。 * 除非给定的最小容量大于MAX_ARRAY_SIZE，否则不会返回大于MAX_ARRAY_SIZE的容量。 * * @param minCapacity 所需的最小容量 * @throws OutOfMemoryError 如果minCapacity小于零 */private int newCapacity(int minCapacity) &#123; /** * 旧的容量是现在数组的长度 * 默认的新的容量是旧容量的1.5倍 */ int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); /** * 如果传入的要求的最小容量（newCapacity）大于等于默认的新的容量，就进入if做边界条件的判断 */ if (newCapacity - minCapacity &lt;= 0) &#123; /** * 如果当前数组是空数组，这种情况下就是数组进行了初始化，但是没有放入任何数据，还是一个空数组，所以上面得到的oldCapacity和newCapacity都是0 * 那么就取要求的最小容量和默认容量（16）二者中较大的那个进行扩容。 */ if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) return Math.max(DEFAULT_CAPACITY, minCapacity); /** * 因为上面的if判断的是 &lt;= 的情况，所以有可能传入的 minCapacity是负数 */ if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); /** * 边界没有溢出的话，就扩大为minCapacity */ return minCapacity; &#125; /** * 如果传入的要求的最小容量（newCapacity）小于默认的新的容量，就不使用传入的minCapacity * 如果默认的新的容量小于数组最大容量Integer.MAX_VALUE-8，那么就使用它，也就是数组扩容1.5倍 * 但是如果大于数组最大容量Integer.MAX_VALUE-8，就尝试使用minCapacity，进入hugeCapacity函数判断 */ return (newCapacity - MAX_ARRAY_SIZE &lt;= 0) ? newCapacity : hugeCapacity(minCapacity);&#125;private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // 溢出 throw new OutOfMemoryError(); /** * 如果newCapacity大于数组最大容量Integer.MAX_VALUE-8，但是minCapacity没有，就使用Integer.MAX_VALUE-8 * 但是如果newCapacity和minCapacity都大于了Integer.MAX_VALUE-8的话，就把数组扩容为Integer.MAX_VALUE */ return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;&#125; 总结来说，对于数组容量扩容过程如下： 先确定三个变量：传入的所需的最小容量（minCapacity），旧容量（oldCapacity）也就是当前现在数组的长度，默认的新的容量（newCapacity）是旧容量的1.5倍。 对比minCapacity和newCapacity，如果对比minCapacity大于等于（&gt;=）newCapacity,那么进入3，否则进入5。 如果elementData是只进行初始化，但是还没有存入数据的数组，那么它的长度肯定是0，所以这种情况下上面得到的oldCapacity和newCapacity是0，因此取默认初始容量（16）和minCapacity中的较大值，作为扩容后的容量。否则进入4。 判断如果传入的minCapacity是负数，那么抛出异常。否则将其作为扩容后的容量。 如果newCapacity小于等于MAX_ARRAY_SIZE（Integer.MAX_VALUE-8），那么newCapacity就是扩容大小，也就是扩容1.5倍。否则进行6。 如果newCapacity大于minCapacity，但是minCapacity其实是负数，那么直接抛出异常。否则再次判断minCapacity与MAX_ARRAY_SIZE的大小关系，如果minCapacity也大于MAX_ARRAY_SIZE，那么newCapacity和minCapacity都大于了MAX_ARRAY_SIZE，就把数组扩容为Integer.MAX_VALUE。否则进行7。 否则就只有newCapacity大于MAX_ARRAY_SIZE，而minCapacity小于等于MAX_ARRAY_SIZE，则数组扩容为MAX_ARRAY_SIZE。 size 方法12345678/** * 以常数时间返回此列表中的元素数量。 * * @return 列表中元素的数量 */public int size() &#123; return size;&#125; isEmpty 方法12345678/** * 如果此列表不包含任何元素，则返回true。 * * @return &#123;@code true&#125; 如果此列表不包含任何元素 */public boolean isEmpty() &#123; return size == 0;&#125; contains 方法12345678910111213/** * 如果此列表包含指定的元素，则返回true 。 * 更正式地说，返回true当且仅当此列表包含至少一个元素e这样Objects.equals(o, e) 。 * * @param o 其在此列表中的存在性将被测试 * @return &#123;@code true&#125; 如果此列表包含指定的元素 */public boolean contains(Object o) &#123; /** * 实则调用了indexOf方法得到其下标，只需判断得到的下标是否小于零即可 */ return indexOf(o) &gt;= 0;&#125; indexOf 于 lastIndexOf 方法12345678910111213141516171819202122232425/** * 以常数时间返回此列表中指定元素的第一次出现的索引，如果此列表不包含元素，则返回-1。 * 更正式地，返回最低下标i ，使得Objects.equals(o, get(i)) ，如果没有这样的下标则返回-1。 */public int indexOf(Object o) &#123; /** * 如果要寻找的对象是null，那么就遍历数组，找第一个null的下标 */ if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; /** * 如果要寻找的对象非null，那么就遍历数组，找第一个为o的元素的下标 */ for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; &#125; /** * 如果找不到的话，就返回-1 */ return -1;&#125; 1234567891011121314151617/** * 返回此列表中指定元素的最后一次出现的索引，如果此列表不包含元素，则返回-1。 * 更正式地说，返回满足i这样Objects.equals(o, get(i)) ，如果没有这样的索引则返回-1。 * 搜索方法与indexOf相似，同为遍历整个数组，区别就是该方法从后向前寻找。 */public int lastIndexOf(Object o) &#123; if (o == null) &#123; for (int i = size-1; i &gt;= 0; i--) if (elementData[i]==null) return i; &#125; else &#123; for (int i = size-1; i &gt;= 0; i--) if (o.equals(elementData[i])) return i; &#125; return -1;&#125; 该方法需要遍历整个数组，寻找对应的元素的下标，所以时间复杂度为O(N)。 clone​ 方法1234567891011121314151617181920212223/** * 返回此ArrayList实例的浅拷贝。（元素本身不被复制。） * * @return 这个 ArrayList实例的克隆 */public Object clone() &#123; try &#123; /** * 调用AbstractList的clone方法得到一个ArrayList * 然后给这个v的elementData数组复制为当前数组，同时modCount重置为0 * 返回这个ArrayList。 */ ArrayList&lt;?&gt; v = (ArrayList&lt;?&gt;) super.clone(); v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; &#125; catch (CloneNotSupportedException e) &#123; /** * 这不应该发生，因为我们是可克隆的 */ throw new InternalError(e); &#125;&#125; 该方法只返回此ArrayList实例的浅拷贝，元素本身不被复制。 toArray 方法123456789101112131415/** * 以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组。 * 返回的数组将是“安全的”，因为该列表不保留对它的引用。 （换句话说，这个方法必须分配一个新的数组）。 * 因此，调用者可以自由地修改返回的数组。 * * 此方法充当基于阵列和基于集合的API之间的桥梁。 * * @return 一个包含该列表中所有元素的数组 */public Object[] toArray() &#123; /** * 使用Arrays的copyOf拷贝elementData，得到并且返回一个新的数组。 */ return Arrays.copyOf(elementData, size);&#125; 12345678910111213141516171819202122232425262728293031/** * 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）; 返回的数组的运行时类型是指定数组的运行时类型。 * 如果列表适合指定的数组，则返回其中。 否则，将为指定数组的运行时类型和此列表的大小分配一个新数 * 如果列表符合指定的数组，则有剩余空间（即数组的列表数量较多），则紧跟在集合结束后的数组中的元素设置为null 。 * （这仅在调用者知道列表不包含任何空元素的情况下才能确定列表的长度。） * * @param a 要存储列表的元素的数组，如果它足够大; 否则，为此目的分配相同运行时类型的新数组。 * * @return 包含列表元素的数组 * @throws ArrayStoreException 如果指定数组的运行时类型不是此列表中每个元素的运行时类型的超类型 * * @throws NullPointerException 如果指定的数组为空 */@SuppressWarnings("unchecked")public &lt;T&gt; T[] toArray(T[] a) &#123; if (a.length &lt; size) /** * 如果传入的数组的长度小于当前的元素数量，则创建一个新的数组a的运行时类型的数组，把elementData数组中的元素复制到该数组中。 */ return (T[]) Arrays.copyOf(elementData, size, a.getClass()); /** * 否则直接把elementData数组中的元素复制到该数组中。 */ System.arraycopy(elementData, 0, a, 0, size); /** * 如果传入数组长度大于元素数量，那么就把最后一个元素的后面的元素设置为null。 */ if (a.length &gt; size) a[size] = null; return a;&#125; toArray 方法主要有两种方式，一种是无参方法，直接返回包含此列表中所有元素的数组；另一种是传入一个数组，然后把此列表中所有元素复制到该数组中，然后返回该数组。 get 方法123456789101112131415161718/** * 因为底层是数组，所以以常数时间返回此列表中指定位置的元素。 * * @param index 要返回的元素的索下标 * @return 该列表中指定位置的元素 * @throws IndexOutOfBoundsException 如果下标超出范围（ index &lt; 0 || index &gt;= size() ） */public E get(int index) &#123; /** * Objects.checkIndex方法调用了Preconditions.checkIndex(index, length, null)检查下标是否超出范围 * Preconditions.checkIndex() 方法判断如果index &lt; 0 || index &gt;= size()，就抛出异常，否则返回传出的index。 */ Objects.checkIndex(index, size); /** * 如果下标满足要求，返回elementData数组中对应下标处的元素。 */ return elementData(index);&#125; set 方法123456789101112131415161718192021/** * 用指定的元素替换此列表中指定位置的元素。 * * @param index 要替换的元素的下标 * @param element 要存储在指定位置的元素 * @return 该元素以前在指定的位置 * @throws IndexOutOfBoundsException 如果索引超出范围（ index &lt; 0 || index &gt;= size() ） */public E set(int index, E element) &#123; /** * 同get方法一样，先判断下表是否越界，如果越阶就抛出异常。 */ Objects.checkIndex(index, size); /** * 记录下elementData数组中指定位置处的旧元素，用于返回。 * 将elementData数组中指定位置处的元素设置为传入的元素，然后返回旧的元素 */ E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125; add 方法将指定的元素追加到此列表的末尾123456789101112131415/** * 以常数时间，将指定的元素追加到此列表的末尾。 * * @param e 要附加到此列表的元素 * @return &#123;@code true&#125; (由 Collection.add(E)指定) */public boolean add(E e) &#123; /** * 因为往数组中添加元素，所以结构发生了改变，因此modCount加一 * 调用内部的add方法添加元素，add方法见下面。 */ modCount++; add(e, elementData, size); return true;&#125; 内部的add方法添加元素1234567891011121314151617/** * 这个helper方法从add(E)中分离出来，以将方法字节码大小保持在35以下(-XX:MaxInlineSize默认值)，这有助于在c1编译的循环中调用add(E)。 * */private void add(E e, Object[] elementData, int s) &#123; /** * 先判断数组中的元素数量是否达到了数组长度 * 如果达到，则对数组进行扩容，扩容大小是原数组长度的1.5倍 */ if (s == elementData.length) elementData = grow(); /** * 然后将传入的元素添加到数组尾部，元素数量加一 */ elementData[s] = e; size = s + 1;&#125; 在指定位置插入指定的元素12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 以O(N)的时间，在此列表中的指定位置插入指定的元素。 * 将当前位于该位置的元素（如果有）和任何后续元素（向其索引添加一个）移动。 * * @param index 要在其中插入指定元素的下标 * @param element 要插入的元素 * @throws IndexOutOfBoundsException 如果索引超出范围（ index &lt; 0 || index &gt; size() ） */public void add(int index, E element) &#123; /** * rangeCheckForAdd方法见下面； * 同样把数组修改次数加一； */ rangeCheckForAdd(index); modCount++; final int s; Object[] elementData; /** * 如果数组中的元素数量是否达到了数组长度，对数组进行扩容，扩容大小是原数组长度的1.5倍 */ if ((s = size) == (elementData = this.elementData).length) elementData = grow(); /** * 然后将该index位置的元素和它后面的所有元素后移一位 * 把index的位置空出来，然后将其赋值为传入的元素 * 元素数量加一。 */ System.arraycopy(elementData, index, elementData, index + 1, s - index); elementData[index] = element; size = s + 1;&#125;/** * 由add和addAll使用的rangeCheck的一个版本。同为对下标范围的判断，本质与之前的checkIndex方法没什么区别。 * 只是自己自定义了抛出异常的语句而已 */private void rangeCheckForAdd(int index) &#123; if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125; 总结来说，如果在指定位置插入指定的元素，因为要移动指定位置后面的所有元素，那么O(N)的时间；如果将指定的元素追加到此列表的末尾，那么仅花费常数的时间，但是如果数组需要扩容的话，将花费时间对数组进行扩容，所以尽量在初始化该List时就指定好容量大小。 remove 方法删除指定位置的元素12345678910111213141516171819202122232425/** * 删除该列表中指定位置的元素。 将任何后续元素移动到左侧（从其索引中减去一个元素）。 * * @param index 要删除的元素的下标 * @return 从列表中删除的元素 * @throws IndexOutOfBoundsException 如果索引超出范围（ index &lt; 0 || index &gt;= size() ） */public E remove(int index) &#123; /** * 依旧先对下标范围进行检查 */ Objects.checkIndex(index, size); final Object[] es = elementData; /** * 把旧的元素暂存下来，调用fastRemove方法把指定位置的元素删除，等删除后返回。 */ @SuppressWarnings("unchecked") E oldValue = (E) es[index]; /** * fastRemove方法见下面 */ fastRemove(es, index); return oldValue;&#125; 删除第一个出现的指定元素12345678910111213141516171819202122232425262728293031323334/** * 从列表中删除第一个出现的指定元素（如果存在）。 * 如果列表不包含该元素，则它不会更改。 * 更正式地，删除具有最低索引i的元素，使得Objects.equals(o, get(i)) （如果这样的元素存在）。 * 如果此列表包含指定的元素（或等效地，如果此列表作为调用的结果而更改），则返回true 。 * * @param o 要从此列表中删除的元素（如果存在） * @return &#123;@code true&#125; 如果此列表包含指定的元素 */public boolean remove(Object o) &#123; final Object[] es = elementData; final int size = this.size; int i = 0; found: &#123; /** * 遍历整个数组，查找指定元素o的下标，如果数组中不存在该元素，就直接返回false */ if (o == null) &#123; for (; i &lt; size; i++) if (es[i] == null) break found; &#125; else &#123; for (; i &lt; size; i++) if (o.equals(es[i])) break found; &#125; return false; &#125; /** * 找到该元素的下标后，调用fastRemove方法进行删除。 */ fastRemove(es, i); return true;&#125; fastRemove 方法12345678910111213141516171819/** * 私有的remove方法，该方法跳过边界检查，并且不返回已删除的值。 */private void fastRemove(Object[] es, int i) &#123; /** * 数组修改次数加一 */ modCount++; final int newSize; /** * 将指定元素第一次出现的下标后面的元素全部左移一位，等于将指定元素覆盖掉 */ if ((newSize = size - 1) &gt; i) System.arraycopy(es, i + 1, es, i, newSize - i); /** * 然后将最后那个空出来的元素变赋值为null，同时size减小1 */ es[size = newSize] = null;&#125; clear 方法123456789101112131415/** * 从列表中删除所有元素。 此呼叫返回后，列表将为空。 */public void clear() &#123; /** * 数组修改次数加一 */ modCount++; final Object[] es = elementData; /** * 遍历整个数组，把所有下标置为null，同时size设置为0 */ for (int to = size, i = size = 0; i &lt; to; i++) es[i] = null;&#125; addAll 方法将指定集合中的所有元素追加到列表的末尾123456789101112131415161718192021222324252627282930313233343536373839/** * 按指定集合的Iterator返回的顺序 。 * 如果在操作进行中修改了指定的集合，则此操作的行为是不确定的。（这意味着如果指定的集合是此列表，则此调用的行为是不确定的，并且此列表是非空的。） * * @param c 包含要添加到此列表的元素的集合 * @return &#123;@code true&#125; 如果此列表因调用而更改 * @throws NullPointerException 如果指定的集合为空 */public boolean addAll(Collection&lt;? extends E&gt; c) &#123; /** * 把传入的集合转化为数组，方便进行拷贝，同时修改次数加一 */ Object[] a = c.toArray(); modCount++; /** * 如果传入的集合中没有元素，那么此列表没有更改，因此返回false */ int numNew = a.length; if (numNew == 0) return false; Object[] elementData; final int s; /** * elementData.length- size 得到数组剩余的空闲空间， * 如果传入的集合长度numNew大于数组剩余的空闲空间，因此当前数组放不下传入的元素，所以要对数组进行扩容 * 扩容的后的大小最小值为：当前元素数量加将要添加的元素数量 */ if (numNew &gt; (elementData = this.elementData).length - (s = size)) elementData = grow(s + numNew); /** * 将传入的集合元素数组拷贝到列表数组的后面 */ System.arraycopy(a, 0, elementData, s, numNew); /** * 元素数量加上传入的元素数量 */ size = s + numNew; return true;&#125; 从指定的位置开始，将指定集合中的所有元素插入到此列表中。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 将指定集合中的所有元素插入到此列表中，从指定的位置开始。 * 将当前位于该位置（如果有的话）的元素和随后的任何元素移动到右边（增加其索引）。 * 新元素将按照指定集合的迭代器返回的顺序显示在列表中。 * * @param index 从中指定集合插入第一个元素的索引 * @param c 包含要添加到此列表的元素的集合 * @return &#123;@code true&#125; 如果此列表因呼叫而更改 * @throws IndexOutOfBoundsException 如果索引超出范围（ index &lt; 0 || index &gt; size() ） * @throws NullPointerException 如果指定的集合为空 */ public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; /** * 使用自定义的方法对下标是否越界进行检查 */ rangeCheckForAdd(index); /** * 将传入的集合转化为数组，方便拷贝 * 同时修改次数加一 */ Object[] a = c.toArray(); modCount++; /** * 如果传入的集合中没有元素，那么此列表没有更改，因此返回false */ int numNew = a.length; if (numNew == 0) return false; Object[] elementData; final int s; /** * elementData.length- size 得到数组剩余的空闲空间， * 如果传入的集合长度numNew大于数组剩余的空闲空间，因此当前数组放不下传入的元素，所以要对数组进行扩容 * 扩容的后的大小最小值为：当前元素数量加将要添加的元素数量 */ if (numNew &gt; (elementData = this.elementData).length - (s = size)) elementData = grow(s + numNew); /** * s - index 计算得到需要向右移动的元素的长度 * 然后将其向右移动该长度 */ int numMoved = s - index; if (numMoved &gt; 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); /** * 把传入的集合中的元素拷贝到指定的位置，也就是上面数组向右移动后空出来的位置 */ System.arraycopy(a, 0, elementData, index, numNew); /** * 元素数量加上传入的集合中的元素数量 */ size = s + numNew; return true; &#125; removeAll 与 retainAll 方法removeAll 方法1234567891011121314/** * 从此列表中删除指定集合中包含的所有元素。 * * @param c 包含要从此列表中删除的元素的集合 * @return &#123;@code true&#125; 如果此列表因调用而更改 * @throws ClassCastException 如果此列表的元素的类与指定的集合不兼容（ 可选 ） * @throws NullPointerException 如果此列表包含空元素，并且指定的集合不允许空元素（ 可选 ），或者如果指定的集合为空 */public boolean removeAll(Collection&lt;?&gt; c) &#123; /** * batchRemove 方法见下面 */ return batchRemove(c, false, 0, size);&#125; retainAll 方法12345678910111213/** * 仅保留此列表中包含在指定集合中的元素。 * 换句话说，从此列表中删除其中不包含在指定集合中的所有元素。 * 本质就是求交集。 * * @param c 包含要保留在此列表中的元素的集合 * @return &#123;@code true&#125; 如果此列表因调用而更改 * @throws ClassCastException 如果此列表的元素的类与指定的集合不兼容（ 可选 ） * @throws NullPointerException 如果此列表包含空元素，并且指定的集合不允许空元素（ 可选 ），或者如果指定的集合为空 */public boolean retainAll(Collection&lt;?&gt; c) &#123; return batchRemove(c, true, 0, size);&#125; batchRemove 方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * 对比removeAll和retainAll方法，同样都是调用了batchRemove方法，唯一的区别就是传入的complement参数 * removeAll的参数是false，而retainAll方法传入的是true，所导致的结果则截然不同，所以这个complement是决定结果的关键 */private boolean batchRemove(Collection&lt;?&gt; c, boolean complement, final int from, final int end) &#123; /** * 该方法判断传入的集合c是否为null，如果是null则抛出异常，代码见下方 */ Objects.requireNonNull(c); final Object[] es = elementData; int r; /** * 从头开始遍历数组，它的作用就是找到数组中第一个在集合c包含或者不包含的元素的位置，具体看下面 */ for (r = from;; r++) &#123; /** * 如果r走到了最后依旧没找到任何一个集合c中包含或者不包含的元素 * 那么数组将不会发生任何变化，返回false */ if (r == end) return false; /** * 如果complement是false，那么在找到数组中第一个存在于集合c中的元素时，结束循环 * 如果complement是true，那么在找到数组中第一个不存在于集合c中的元素时，结束循环 */ if (c.contains(es[r]) != complement) break; &#125; /** * 看到w和r，顾名思义，w是write，r是read，也就是写和读，具体作用看下面就知道了 * 这里把r赋给了w，r加一 */ int w = r++; try &#123; for (Object e; r &lt; end; r++) /** * 从上一次循环中，扎到数组中第一个存在/不存在于集合c中的元素的下标开始遍历 * * 如果complement是false，那么在找到数组中一个不存在于集合c中的元素时，把他覆盖到刚刚找到的第一个存在于集合c中的元素的位置处 * 这里可能难以理解一点，可以这样想： * 因为complement是false的情况是删除重复的元素嘛，所以用数组后面不重复的元素覆盖前面的元素，以此代替了删除。 * * 同样如果complement是true，那么在找到数组中一个存在于集合c中的元素时，把他覆盖到刚刚找到的第一个不存在于集合c中的元素的位置处 */ if (c.contains(e = es[r]) == complement) es[w++] = e; &#125; catch (Throwable ex) &#123; /** * 即使c.contains()抛出异常，也可以保持与AbstractCollection的兼容性 * 将已经覆盖的元素后面重复出来的元素删除掉 */ System.arraycopy(es, r, es, w, end - r); w += end - r; throw ex; &#125; finally &#123; /** * 修改此处对应增加改变的数量 */ modCount += end - w; shiftTailOverGap(es, w, end); &#125; return true;&#125; public static &lt;T&gt; T requireNonNull(T obj) &#123; if (obj == null) throw new NullPointerException(); return obj; &#125; /**通过以下元素向下滑动，消除从lo到hi的间隔。 */ private void shiftTailOverGap(Object[] es, int lo, int hi) &#123; System.arraycopy(es, hi, es, lo, size - hi); for (int to = size, i = (size -= hi - lo); i &lt; to; i++) es[i] = null; &#125; 这里总结一下removeAll，retainAll和addAll方法之间的关系吧： removeAll 方法就是把存在于指定的集合中的元素全部删除掉，也就是求补集。 retainAll 方法就是把不存在于指定的集合中的元素全部删除掉，也就是求交集。 addAll 方法就把不存在于指定的集合中的元素全部添加到列表中，也就是求并集。 subList 方法1234567891011121314151617181920/** * 返回指定的fromIndex （含）和toIndex之间的列表部分的视图。 （如果fromIndex和toIndex相等，返回的列表为空。） * 返回的列表由此列表支持，因此返回列表中的非结构更改将反映在此列表中，反之亦然。 返回的列表支持所有可选列表操作。 * 该方法消除了对显式范围操作（对于数组通常存在的排序）的需要。 * 任何期望列表的操作都可以通过传递一个子列表视图而不是整个列表来用作范围操作。 例如，以下成语从列表中移除了一系列元素： list.subList(from, to).clear(); * 可以为indexOf(Object)和lastIndexOf(Object)构造类似的成语，并且可以将Collections类中的所有算法应用于子列表。 * 如果支持列表（即，此列表）以除了通过返回的列表之外的任何方式进行结构修改 ，则此方法返回的列表的语义将变为不正确。 * （结构修改是那些改变此列表的大小，或以其他方式扰乱它，使得正在进行的迭代可能产生不正确的结果）。 * * 简单的来说，就是返回整个列表中，指定范围那部分的列表的视图。 * 但是！！如果对返回的这部分列表进行修改，那么同时原列表的对应位置也会发生修改 * 所以本质就是返回了一部分引用而已。 * * @throws IndexOutOfBoundsException 如果端点索引值超出范围 (fromIndex &lt; 0 || toIndex &gt; size) * @throws IllegalArgumentException 如果端点索引不正确 (fromIndex &gt; toIndex) */public List&lt;E&gt; subList(int fromIndex, int toIndex) &#123; subListRangeCheck(fromIndex, toIndex, size); return new SubList&lt;&gt;(this, fromIndex, toIndex);&#125; 所以总结来说，尽量不要使用subList方法，如果要使用的话，一定要注意下面几点使用方法： 千万不要再对原 List 进行任何改动的操作(例如: 增删改), 查询和遍历倒是可以. 因为如果对原 List 进行了改动, 那么后续只要是涉及到子 List 的操作就一定会出问题. 而至于会出现什么问题呢? 具体来说就是:(1) 如果是对原 List 进行修改 (即: 调用 set() 方法) 而不是增删, 那么子 List 的元素也可能会被修改 (这种情况下不会抛出并发修改异常).(2) 如果是对原 List 进行增删, 那么此后只要操作了子 List , 就一定会抛出并发修改异常. 千万不要直接对子 List 进行任何改动的操作(例如: 增删改), 但是查询和间接改动倒是可以. 不要对子 List 进行直接改动, 是因为如果在对子 List 进行直接改动之前, 原 List 已经被改动过, 那么此后在对子 List 进行直接改动的时候就会抛出并发修改异常. 如果要进行操作，则使用例如：List subList = new ArrayList&lt;&gt;(list.subList(2, list.size())); 的方法，把分割出来的数组转化为一个新的列表，在新的列表基础上操作就不会对原列表产生任何影响。 补充考虑一点：elementData设置成了transient，那ArrayList是怎么把元素序列化的呢？ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // 防止序列化期间有修改 int expectedModCount = modCount; // 写出非transient非static属性（会写出size属性） s.defaultWriteObject(); // 写出元素个数 s.writeInt(size); // 依次写出元素 for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; // 如果有修改，抛出异常 if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125;private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; // 声明为空数组 elementData = EMPTY_ELEMENTDATA; // 读入非transient非static属性（会读取size属性） s.defaultReadObject(); // 读入元素个数，没什么用，只是因为写出的时候写了size属性，读的时候也要按顺序来读 s.readInt(); if (size &gt; 0) &#123; // 计算容量 int capacity = calculateCapacity(elementData, size); SharedSecrets.getJavaOISAccess().checkArray(s, Object[].class, capacity); // 检查是否需要扩容 ensureCapacityInternal(size); Object[] a = elementData; // 依次读取元素到数组中 for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125;&#125; 查看writeObject()方法可知，先调用s.defaultWriteObject()方法，再把size写入到流中，再把元素一个一个的写入到流中。 一般地，只要实现了Serializable接口即可自动序列化，writeObject()和readObject()是为了自己控制序列化的方式，这两个方法必须声明为private，在java.io.ObjectStreamClass#getPrivateMethod()方法中通过反射获取到writeObject()这个方法。 在ArrayList的writeObject()方法中先调用了s.defaultWriteObject()方法，这个方法是写入非static非transient的属性，在ArrayList中也就是size属性。同样地，在readObject()方法中先调用了s.defaultReadObject()方法解析出了size属性。 elementData定义为transient的优势，自己根据size序列化真实的元素，而不是根据数组的长度序列化元素，减少了空间占用。 总结 ArrayList内部使用数组存储元素，当数组长度不够时进行扩容，每次加一半的空间，ArrayList不会进行缩容； ArrayList支持随机访问，通过索引访问元素极快，时间复杂度为O(1)； ArrayList添加元素到尾部极快，平均时间复杂度为O(1)； ArrayList添加元素到中间比较慢，因为要搬移元素，平均时间复杂度为O(n)； ArrayList从尾部删除元素极快，时间复杂度为O(1)； ArrayList从中间删除元素比较慢，因为要搬移元素，平均时间复杂度为O(n)； ArrayList支持求并集，调用addAll(Collection&lt;? extends E&gt; c)方法即可； ArrayList支持求交集，调用retainAll(Collection&lt;? extends E&gt; c)方法即可； ArrayList支持求单向差集，调用removeAll(Collection&lt;? extends E&gt; c)方法即可；]]></content>
      <categories>
        <category>Java容器</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HashMap源码阅读]]></title>
    <url>%2Fposts%2F42557%2F</url>
    <content type="text"><![CDATA[HashMap的数据结构先介绍一点基础结构 HashMap的基础结构是由数组（Node&lt;K,V&gt;[] table）+ 链表 + 红黑树组成的，因为我对红黑树不太了解，所以就没有看后面红黑树部分的东西（1400行之后的代码基本全是在说红黑树部分的），下面就没有讲述红黑树部分的内容。数组的每个下标位置储存的是Node结点， 在Javadoc中把存放数据的table数组的每个下表称作bin（桶），数组每个下标的一开始存放的是链表，当链表长度大于等于（&gt;=）8的时候，会将链表转换为红黑树。 顶部注释： HashMap是Map接口基于哈希表的实现。这种实现提供了所有可选的Map操作，并允许key和value为null（除了HashMap是unsynchronized的和允许使用null外，HashMap和HashTable大致相同。）。此类不保证映射的顺序，特别是它不保证该顺序恒久不变。 此实现假设哈希函数在桶内适当地分布元素，为基本实现(get 和 put)提供了稳定的性能。迭代 collection 视图所需的时间与 HashMap 实例的“容量”（桶的数量）及其大小（键-值映射关系数）成比例。如果遍历操作很重要，就不要把初始化容量initial capacity设置得太高（或将加载因子load factor设置得太低），否则会严重降低遍历的效率。 HashMap有两个影响性能的重要参数：初始化容量initial capacity、加载因子load factor。容量是哈希表中桶的数量，初始容量只是哈希表在创建时的容量。加载因子是哈希表在其容量自动增加之前可以达到多满的一种尺度。initial capacityload factor就是当前允许的最大元素数目，超过initial capacityload factor之后，HashMap就会进行rehashed操作来进行扩容，扩容后的的容量为之前的两倍。 通常，默认加载因子 (0.75) 在时间和空间成本上寻求一种折衷。加载因子过高虽然减少了空间开销，但同时也增加了查询成本（在大多数 HashMap类的操作中，包括 get 和 put 操作，都反映了这一点）。在设置初始容量时应该考虑到映射中所需的条目数及其加载因子，以便最大限度地减少rehash操作次数。如果初始容量大于最大条目数除以加载因子，则不会发生rehash 操作。 如果很多映射关系要存储在 HashMap 实例中，则相对于按需执行自动的 rehash 操作以增大表的容量来说，使用足够大的初始容量创建它将使得映射关系能更有效地存储。 注意，此实现不是同步的。如果多个线程同时访问一个哈希映射，而其中至少一个线程从结构上修改了该映射，则它必须保持外部同步。（结构上的修改是指添加或删除一个或多个映射关系的任何操作；仅改变与实例已经包含的键关联的值不是结构上的修改。）这一般通过对自然封装该映射的对象进行同步操作来完成。如果不存在这样的对象，则应该使用 Collections.synchronizedMap 方法来“包装”该映射。最好在创建时完成这一操作，以防止对映射进行意外的非同步访问，如下所示：Map m = Collections.synchronizedMap(new HashMap(…)); 由所有此类的“collection 视图方法”所返回的迭代器都是fail-fast 的：在迭代器创建之后，如果从结构上对映射进行修改，除非通过迭代器本身的remove方法，其他任何时间任何方式的修改，迭代器都将抛出 ConcurrentModificationException。因此，面对并发的修改，迭代器很快就会完全失败，而不冒在将来不确定的时间发生任意不确定行为的风险。 注意，迭代器的快速失败行为不能得到保证，一般来说，存在非同步的并发修改时，不可能作出任何坚决的保证。快速失败迭代器尽最大努力抛出 ConcurrentModificationException。因此，编写依赖于此异常的程序的做法是错误的，正确做法是：迭代器的快速失败行为应该仅用于检测bug。 此类是 Java Collections Framework 的成员。 从上面的内容可以总结出以下几点： 底层： HashMap是Map接口基于哈希表实现的。 是否允许null： HashMap允许key和value为null。 是否有序：HashMap不保证映射到顺序，特别是它不保证顺序恒久不变。 两个影响HashMap性能的参数： 初始化容量initial capacity、加载因子load factor。 每次扩容大小：扩容后的的容量为之前的两倍。 初始化容量对性能的影响： 不应设置的太小，容量小虽然可以节省空间，但是可能会导致频繁的扩容，扩容操作非常消耗时间；也不应该设置的太大，容量大会导致严重降低遍历的效率以及内存空间的浪费。总结来说就是：小了会增大时间开销（频繁的扩容）；大了会增大空间开销和时间开销（降低遍历效率）。 加载因子对性能的影响： 0.75是一个折中的值，加载因子过高虽然减少了空间开销，但是也增加了查询到成本；而加载因子过低会导致频繁的扩容。 是否同步： HashMap不是同步的。 迭代器： 迭代器是fast-fail，但是迭代器的快速失败行为不能得到保证。 HashMap的定义public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable HashMap&lt;K,V&gt;：HashMap是以key-value形式存储数据。 extends AbstractMap&lt;K,V&gt;： 继承于AbstractMap，大大减少了实现Map接口时需要的工作。 implements Map&lt;K,V： 实现了Map接口，提供所有可选的Map操作。 implements Cloneable：实现了Cloneable接口，内部可以调用clone()方法来返回实例的浅拷贝(shallow copy)。 implements Serializable：实现了Serializable接口，表明该类时可以序列化的。 静态全局变量1234567891011121314151617181920212223242526272829303132333435/** * 默认初始容量—必须是2的幂。 */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // 也就是 16/** * 如果具有参数的任一构造函数隐式指定更高的值，则使用最大容量。 * 必须是2的幂 &lt;= 1 &lt;&lt; 30。 */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 也就是 2的30次方/** * 构造函数中没有指定时使用的加载因子，即默认的加载因子。 */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * 将链表转化成红黑树的临界值。 * 当链表长度(包括下标处开始的那个结点)大于等于8时，桶中的链表被转化成红黑树。 */static final int TREEIFY_THRESHOLD = 8;/** * 将红黑树恢复成链表时的临界值。 * 当红黑树的长度小于等于6时，桶中的红黑树被转化成链表。 */static final int UNTREEIFY_THRESHOLD = 6;/** * 桶被转化成红黑树的最小容量。 * 当链表长度大于等于8，且HashMap的总体大小大于等于64时，才会将桶中的链表被转化成红黑树。 * 否则只会采取扩容的方式来减少冲突。 * 该值不能小于 4 * TREEIFY_THRESHOLD */static final int MIN_TREEIFY_CAPACITY = 64; 静态内部类 Node12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * HashMap的基本节点类型，即是HashMap底层的组成元素，也是每个桶（bin）中的链表的组成元素。 */ static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; /** * key的hash值 */ final int hash; final K key; V value; /** * 指向下一个Node节点的引用 */ Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125; &#125; 静态工具hash方法详解123456789/** * 计算key.hashCode（）并将更高位的散列扩展（XOR）降低。 */ static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); &#125;i = (table.length - 1) &amp; hash; //这一步是在后面添加元素putVal()方法中进行位置的确定 主要分为三步： 取hashCode的值： key.hashCode()。调用Object. hashCode() 方法，该方法根据一定规则将与对象相关的信息，例如对象的存储地址，对象的字段等，映射成与一个32位 int 类型的值，这个数值称作为hash值。 让高位参与运算： h&gt;&gt;&gt;16 。将得到的hash值无符号右移十六位，空出来的高位补零。 取模运算： (n-1) &amp; hash 。 为了让数组元素分布均匀，把hash值对数组长度-1取余，也就是hash%n，得到在数组中保存的位置下标。 为什么要这样做的理由： 整个过程如上图所示，将原本的32位的hash值右移16位，然后与原值进行异或运算，是为了混合原始哈希码的高位和低位，以此来加大低位的随机性。 看到这里有个疑问，为什么要做异或运算？设想一下，如果n很小，假设为16的话，那么n-1即为15（0000 0000 0000 0000 0000 0000 0000 1111），这样的值如果跟hashCode()直接做与操作，实际上只使用了哈希值的后4位。如果当哈希值的高位变化很大，低位变化很小，这样很容易造成碰撞，所以把高低位都参与到计算中，从而解决了这个问题，而且也不会有太大的开销。然后将得到的最终的hash值对数组长度-1取余，就可以得到在数组中保存的位置下标。这也是为什么要保证数组的长度总是2的n次方的理由。当数组长度length总是2的n次方时，(n - 1) &amp; hash == hash % n，但是位运算的速度更快，因此保证效率更高。 comparableClassFor方法解读1234567891011121314151617181920212223/** * 当对象x的类型为X，并且X实现了Comparable接口（比较的参数本身必须为X类本身）时 * 返回x的运行时类型，否则返回null。 * */ static Class&lt;?&gt; comparableClassFor(Object x) &#123; if (x instanceof Comparable) &#123; Class&lt;?&gt; c; Type[] ts, as; ParameterizedType p; if ((c = x.getClass()) == String.class) // bypass checks return c; if ((ts = c.getGenericInterfaces()) != null) &#123; for (Type t : ts) &#123; if ((t instanceof ParameterizedType) &amp;&amp; ((p = (ParameterizedType) t).getRawType() == Comparable.class) &amp;&amp; (as = p.getActualTypeArguments()) != null &amp;&amp; as.length == 1 &amp;&amp; as[0] == c) // type arg is c return c; &#125; &#125; &#125; return null; &#125; 如注释所示，传参传入一个对象，当对象x的类型为X，并且X实现了Comparable接口（比较的参数本身必须为X类本身）时，返回x的运行时类型，否则返回null。接下来分析这个方法的每行代码。 instanceof1x instanceof Comparable instanceof可以理解为是某种类型的实例。不论是运行时类型，或者是他的父类、它实现的接口、他的父类实现的接口、甚至是他父类的父类的父类实现的接口的父类的父类，总之，只要在继承链上有这个类型就可以了。 getClass()1c = x.getClass() 与instanceof相应对的是getClass()方法，无论该对象如何转型，该方法返回的只会是它的运行时类型，可以简单的理解为它的实际类型，也就是new它的时候的类型。有一种例外情况：匿名对象。当匿名对象调用该方法时，返回的是依赖它的对象的运行时类型，并且以1，2，3…的索引区分。 1234567891011121314151617181920212223public class Demo &#123; public static void main(String[] args) &#123; D d = new D(); System.out.println(new A()&#123;&#125;.getClass()); // class Demo$1 System.out.println(new B()&#123;&#125;.getClass()); // class Demo$2 System.out.println(new Comparable&lt;Object&gt;()&#123; // class Demo$3 @Override public int compareTo(Object o) &#123; return 0; &#125;&#125;.getClass()); System.out.println(d.c.getClass()); // class D$1 &#125;&#125;abstract class A&#123;&#125;abstract class B&#123;&#125;abstract class C&#123;&#125;class D&#123; C c; D()&#123; c= new C()&#123;&#125;; &#125;&#125; getGenericInterfaces()1ts = c.getGenericInterfaces() getGenericInterfaces()方法返回的是该对象的运行时类型”直接实现”的接口，这意味着: 返回的一定是接口 必然是该类型自己直接实现的接口，继承过来的不算 getGenericSuperclass()和getSuperclass()这两个方法虽然没有出现在上述代码中，但是也顺便说一下： getGenericSuperclass()返回的是父类的直接类型，不包括泛型参数。 getSuperclass()返回的是包括泛型参数的父类类型，但是注意，如果子类在继承父类时，没有实现（声明）父类的泛型，那么这时候子类是没有泛型参数的。 ParameterizedType 1t instanceof ParameterizedType ParameterizedType是Type接口的子接口，表示实现了泛型参数的类型。需要注意： 如果直接用Bean对象 instanceof ParameterizedType，结果都是false。 Class对象不能 instanceof ParameterizedType，编译会报错。 只有用Type对象 instanceof ParameterizedType ，才能得到想要的比较结果。可以理解为：一个Bean类不会是ParameterizedType，只有代表这个Bean类的类型（Type）才有可能是ParameterizedType。 实现泛型参数，必须给泛型传入参数，例如：class Child2&lt;A,B&gt; extends Super&lt;A,B&gt;{} ;只声明泛型而不实现,例如：class Child3&lt;A,B&gt; extends Super{} , 对比结果为false。 getRawType()1((p = (ParameterizedType) t).getRawType() 该方法返回实现了这个类型的类或者接口，即去掉了泛型参数部分的类型对象。 getActualTypeArguments()1as = p.getActualTypeArguments() 该方法与getRawType()相对应，以数组形式返回泛型的参数列表。 当参数是真实类型时，打印的是全类名 当参数是另一个新声明的泛型参数时，打印的是代表该泛型类型的符号。 所以总结comparableClassFor(Object x)方法的实现为： 123456789101112131415161718static Class&lt;?&gt; comparableClassFor(Object x) &#123; if (x instanceof Comparable) &#123; // 判断是否实现了Comparable接口 Class&lt;?&gt; c; Type[] ts, as; Type t; ParameterizedType p; if ((c = x.getClass()) == String.class) return c; // 如果是String类型，直接返回String.class if ((ts = c.getGenericInterfaces()) != null) &#123; // 判断是否有直接实现的接口 for (int i = 0; i &lt; ts.length; ++i) &#123; // 遍历直接实现的接口 if (((t = ts[i]) instanceof ParameterizedType) &amp;&amp; // 该接口实现了泛型 ((p = (ParameterizedType)t).getRawType() == // 获取接口不带参数部分的类型对象 Comparable.class) &amp;&amp; // 该类型是Comparable (as = p.getActualTypeArguments()) != null &amp;&amp; // 获取泛型参数数组 as.length == 1 &amp;&amp; as[0] == c) // 只有一个泛型参数，且该实现类型是该类型本身 return c; // 返回该类型 &#125; &#125; &#125; return null; &#125; compareComparables 方法1234567891011/** * Returns k.compareTo(x) if x matches kc (k's screened comparable * class), else 0. * 如果x的类型是kc，返回 k.compareTo(x) 的比较结果 * 如果x为空，或者类型不是kc，返回0 */@SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) // for cast to Comparablestatic int compareComparables(Class&lt;?&gt; kc, Object k, Object x) &#123; return (x == null || x.getClass() != kc ? 0 : ((Comparable)k).compareTo(x));&#125; tableSizeFor 方法12345678910111213/** * Returns a power of two size for the given target capacity. * 返回给定数值的比第一个比它大的2的幂次方的数 */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 该方法是为了在构造函数中，把传入的指定容量转化为2的幂次方的整数，保证HashMap的容量为2的幂次方。 字段1234567891011121314151617181920212223242526272829303132333435363738394041/** * table数组，存放HashMap的所有元素的容器 * 在第一次使用的时候初始化，并且可以根据需要调整大小 * 当分配时，长度总是为2的幂次方 * 在某些操作中容忍长度为零，以允许当前不需要的引导机制 */transient Node&lt;K,V&gt;[] table;/** * 保存缓存的 entrySet * AbstractMap字段用于keySet（）和values（） */transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet;/** * HashMap中的包含的键值对数量 */transient int size;/** * 该HashMap经过结构修改的次数 * 结构修改指的是更改HashMap中的键值对数量或者以其他方式修改其内部结构（例如：rehash） * 该字段用于在迭代器中的快速失败（fail-fast），抛出 ConcurrentModificationException 的异常 * 因为HashMap时线程不安全的容器，所以当A线程遍历时HashMap时，还没有遍历到的部分，被线程B修改，如删除 * 那么当线程A遍历到被删除的地方时就会抛出该异常 */transient int modCount;/** * 下一个要调整HashMap大小的值，容量乘加载因子(capacity * load factor). * 因为当大小超过这个值时，哈希碰撞的概率会大大增加，所以达到该值时，对HashMap扩容 * @serial */int threshold;/** * 哈希表的加载因子 * 默认为 0.75f * @serial */final float loadFactor; 核心方法构造方法指定初始化容量和加载因子12345678910111213141516171819202122 /** * 构造具有指定初始容量和加载因子的空HashMap。 * * @param initialCapacity 初始容量 * @param loadFactor 加载因子 * @throws IllegalArgumentException 如果初始容量为负或负载因子为非正时，抛出该异常 * */ public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity);//当指定初始容量超过最大容量（2的30次方）时，把其值设置为最大容量 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor;//将传入指定容量转换为最近的2的整数次方 this.threshold = tableSizeFor(initialCapacity); &#125; 指定初始化容量1234567891011 /** * 构造一个具有指定初始容量和默认加载因子(0.75)的空HashMap。 * * * @param initialCapacity 初始容量 * @throws IllegalArgumentException 如果初始容量为负时，抛出该异常 */ public HashMap(int initialCapacity) &#123;//调用指定初始化容量和加载因子的构造方法，加载因子为默认（0.75） this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; 默认的初始化容量和加载因子123456/** * 构造一个具有默认初始容量(16)和默认负载因子(0.75)的空HashMap。 */public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // 所有其他字段都默认&#125; 使用与指定映射相同的映射123456789101112 /** * 使用与指定映射相同的映射构造新的HashMap。 * HashMap是使用默认负载因子(0.75)创建的，初始容量足以容纳指定映射中的映射。 * * @param m 要在此map中放置其键值对（映射）的map * @throws NullPointerException 如果指定的映射为空抛出该异常 */ public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR;//putMapEntries方法见核心方法putMapEntries()章节 putMapEntries(m, false); &#125; putMapEntries方法1234567891011121314151617181920212223242526272829303132 /** * 实现了Map接口的 Map.putAll and Map 构造方法 * 其中的加载因子等参数、是默认的 * * @param m 指定map * @param 在最初构造此映射时为false，否则为true * (传递到下面的afterNodeInsertion方法，该方法请详见允许LinkedHashMap后操作的回调节)。 */final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123; int s = m.size(); if (s &gt; 0) &#123; //如果table未初始化，对其进行初始化 if (table == null) &#123; // pre-size //使用默认的加载因子（0.75）和传入的map的大小计算出阈值（扩容的临界值） float ft = ((float)s / loadFactor) + 1.0F; //用上一步计算出的阈值与最大容量对比，如果超过最大容量，就把它赋为最大容量 int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); //如果当前默认的阈值小于t，就把当前的阈值扩容为大于t的最小的2的整数次方的整数 if (t &gt; threshold) threshold = tableSizeFor(t); &#125;//如果table已经初始化，且传入的map的大小超过阈值，就对table扩容（resize()方法请在核心方法章节查看） else if (s &gt; threshold) resize(); //做完初始化、扩容等准备工作，现在table已经可以放下传入的map的元素了，迭代map，挨个放入table中 for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123; K key = e.getKey(); V value = e.getValue(); //putVal()方法见下面 putVal(hash(key), key, value, false, evict); &#125; &#125;&#125; size方法12345678/** * 返回此映射中键值对的数目。 * * @return 此映射中键值映对的数目。 */public int size() &#123; return size;&#125; isEmpty方法123456789 /** * 如果此映射不包含键值映射，则返回&#123;@code true&#125;。 * * @return 如果此映射不包含键值映射，则返回&#123;@code true&#125;。 */ public boolean isEmpty() &#123;//键值对数目为零则为空 return size == 0; &#125; get方法12345678910111213141516/** *返回指定键映射到的值，如果该映射不包含键的映射，则返回null。 * * 更正式地说，如果这个映射包含从键k到值v的映射(key==null ?k==null:key.equals(k))， * 则该方法返回v;否则返回null。(最多可以有一个这样的映射。) * * * 返回值为null并不一定表示映射不包含键的映射;也有可能映射显式地将键映射为null。 * containsKey操作可用于区分这两种情况。 * @see #put(Object, Object) */public V get(Object key) &#123; Node&lt;K,V&gt; e; //getNode方法见下面 return (e = getNode(hash(key), key)) == null ? null : e.value;&#125; getNode方法123456789101112131415161718192021222324252627282930313233343536373839404142/** * 实现 Map接口的get方法 和其他相关方法 * * @param hash key的hash值 * @param key 键（key） * @return 返回节点，如果不存在的话返回null */ final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; /** * 1.如果table为空，那么代表HashMap没有进行初始化 * 2.如果table长度小于等于0，那么就代表HashMap中没有数据 * 3.如果根据key的hash值计算出的下标处，没有结点，那么不存在以该key为键得映射 * 满足以上三种情况得任意一种，直接返回null；只有三种情况全部满足的情况下，才进入链表/红黑树查找 */ if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //检查该下标处得第一个结点，如果符合即返回 if (first.hash == hash &amp;&amp; // 总是检查第一个结点 ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //头节点不合符，那么检查头结点后面的结点 if ((e = first.next) != null) &#123; //如果桶中的数据结构是红黑树，则用红黑树的方法查找 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; /** * 如果同桶中的数据结构是链表，从链表的第二个节点开始，遍历链表的每一个结点查找 * e.hash == hash 比较hash值是否相等 * key.equals(k) 和 (k = e.key) == key其实是一样的 * Object的equals方法内部调用的就是 == 来验证是否相等 * 此处体现出了严谨性 */ if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; get和getNode方法总结从上面的源码中可以看出，get方法可以分为三个步骤： 通过hash方法得到key的hash值（hash方法在上面有详细的解释） 将上一步得到的key的hash值和key传入getNode方法，得到该key对应的Node 如果该key对应的Node为空，则返回null，否则返回Node中的value，如果Node中的value为空，那么也返回null getNode方法步骤如下： 判断HashMap中存放数据的table的是否初始化，是否有数据（长度是否为0），根据key的hash值计算得到的该key在table中对应得下标处是否有结点；只有三种情况全部满足的情况下，才进入下标处得链表/红黑树查找，否则直接返回null 检查下标处的头节点是否匹配，匹配则返回该节点，否则检查头结点后面的结点 判断桶中存放数据的的数据结构是红黑树还是链表，如果桶中的数据结构是红黑树，则用红黑树的方法查找。 如果是链表则从链表的第二个节点开始，遍历链表的每一个结点查找，找到就返回对应的节点。 如果红黑树或链表的遍历中都没有找到，那么就返回null，代表不存在该节点。 containsKey方法12345678910/** * 如果此映射包含特定键的映射，则返回true。 * 否则返回false。 * * @param key 要测试在此映射中存在的key * @return &#123;@code true&#125; 如果此映射包含指定的key */public boolean containsKey(Object key) &#123; return getNode(hash(key), key) != null;&#125; 该方法其实本质调用了getNode的方法，判断是否存在以key的键的结点，如过Node存在则返回true，否则返回false。 put方法1234567891011/** * 将指定参数key和指定参数value插入map中，如果key已经存在，那就替换key对应的value * * @param key 指定key * @param value 指定value * @return 如果value被替换，则返回旧的value，否则返回null。当然，可能key对应的value就是null。 */public V put(K key, V value) &#123; //putVal方法的实现就在下面 return putVal(hash(key), key, value, false, true);&#125; put方法可以分为三个步骤： 通过hash方法获取到传入的key的hash值（hash方法在上面有详细的解释） 通过putVal方法放入map中 返回putVal方法的结果 putVal方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * 实现了 Map接口的 put和 相关方法。 * * @param hash key的hash值 * @param key 键 * @param value 要放入的值 * @param onlyIfAbsent 如果为true，即使指定参数key在map中已经存在，也不会替换value * @param evict 如果为false，则该表处于创建模式。 * @return 如果value被替换，则返回旧的value，否则返回null。当然，可能key对应的value就是null。 */final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; /** * 如果table为null，则代表table没有初始化；或者table数组的长度为0， * 这两种情况下，调用resize方法对table进行初始化， * resize方法不仅可以对table扩容，还可以对table初始化 * n用来记录table的长度 */ if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; /** * 如果通过key的hash值计算得到的下标处没有结点，那么新建一个链表结点放入 * newNode方法调用了Node的构造方法，生成了一个新的结点。 */ if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123;//下面就是产生了碰撞的情况 Node&lt;K,V&gt; e; K k; //如果第一个结点的key就与传入的key相等，那么就把这个结点记录下来，在后面覆盖 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //如果第一个key没有碰撞，而且桶中的结构是树，那么就调用相应的树的方法放置键值对 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; //如果第一个key没有碰撞，而且桶中的结构是链表，那么就遍历链表 for (int binCount = 0; ; ++binCount) &#123; //binCount记录了链表长度 //当遍历到链表尾部，新建节点然后插入链表尾部 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); /** * 如果插入后的链表长度大于等于8，那就把链表转化为树 * 这里减一是为了加上头结点，因为链表是从第二个结点开始遍历的 */ if (binCount &gt;= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); break; &#125; //如果在链表中某个结点的key就与传入的key相等，那么就把这个结点记录下来，在后面覆盖 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //如果发生了结点相等的情况，那么之前就记录了下来，所以e不为null，在这里进行覆盖 if (e != null) &#123; //把结点的原值记录下来，用来返回 V oldValue = e.value; //如果存在则覆盖或者旧节点的值为空，那么覆盖 if (!onlyIfAbsent || oldValue == null) e.value = value; //回调方法，文章最后会说 afterNodeAccess(e); //把旧值返回 return oldValue; &#125; &#125; //因为上面是覆盖，所以未发生结构性改变，但是如果是插入，那么久发生了结构改变，所以modCount加一 ++modCount; //如果table大小超过了阈值，那就进行扩容，扩容后面会详细讲解 if (++size &gt; threshold) resize(); ////回调方法，文章最后会说 afterNodeInsertion(evict); return null;&#125; 总结putVal方法，共有如下几个步骤： 判断table数组是否初始化，如果没有就进行初始化 根据key的hash值计算得到的下标处，如果该下标处没有节点，那么就新建一个结点放入桶中 如果该下标处已经存在节点，那么就代表发生了碰撞，开始对链表/红黑树进行遍历 如果第一个结点的key就与传入的key相等，那么就把这个结点记录下来，在后面覆盖； 如果第一个key没有碰撞，而且桶中的结构是树，那么就调用相应的树的方法放置键值对， 如果第一个key没有碰撞，而且桶中的结构是链表，那么就遍历链表 当遍历到链表尾部，新建节点然后插入链表尾部，然后判断链表长度，是否需要转化为红黑树，如果在遍历链表中发生了key相等，那么就把这个结点记录下来，在后面覆盖； 如果发生了key相等的情况，就对结点旧值覆盖，然后把旧值返回 如果没有发生key相等的情况，而是插入了新的结点，那么modCount和size都加一，判断size是否超过阈值，超过就扩容 返回null resize方法当像HashMap中不断地添加元素的时候，元素的数量就会增加，数量增大就不避免的增大了碰撞的概率。所以当元素的数量达到一个阈值的时候，就对HashMap进行扩容。当然数组是无法自动扩容的，扩容方法使用一个新的数组代替已有的容量小的数组。resize方法非常巧妙，因为每次扩容都是翻倍，保证了数组大小为2得整数次方，同时与原来计算（n-1）&amp;hash的结果相比，节点要么就在原来的位置，要么就被分配到“原位置+旧容量”这个位置。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105/** * 对table进行初始化或者大小翻倍的扩容。 * 如果为空，则按照字段阈值中包含的初始容量目标分配。 * 否则，因为我们使用的是2的幂展开，所以每个bin中的元素必须保持相同的索引，或者在新表中以2的幂偏移量移动。 * * @return 新的table数组 */ final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; //记录旧的容量大小和旧的阈值 int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; //定义新的容量和阈值 int newCap, newThr = 0; //如果旧的容量 &gt; 0 if (oldCap &gt; 0) &#123; //如果旧的容量 &gt; 最大容量，那么就把阈值变为最大值 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; //如果旧容量的二倍小于规定的最大容量，并且旧的容量大于默认容量 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) //则对数组的容量和阈值进行翻倍扩容，新的容量和阈值是旧值的二倍 newThr = oldThr &lt;&lt; 1; // double threshold &#125;//如果旧容量 = 0，而且旧临界值 &gt; 0，那么就把容量设置为旧的阈值 else if (oldThr &gt; 0) // 初始容量设置为阈值 newCap = oldThr; else &#123; // 如果旧容量 = 0，且旧阈值 = 0，表示使用默认值，容量为16，阈值为容量*加载因子 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; //在当上面的条件判断中，只有oldThr &gt; 0成立时，newThr == 0 if (newThr == 0) &#123; //ft为临时阈值，使用上面得到的新的容量和默认的加载因子计算得到 float ft = (float)newCap * loadFactor; //这个阈值是否合法，如果合法，那就是真正的临界值，如果超出了最大容量，那么就是最大容量 newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; //把阈值变为新阈值 threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) //创建一个新的数组，大小为新的容量，并且后面把旧的table中的数据全部转移到新的table中 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; //把系统的table变为新的table table = newTab; //如果旧table不为空，将旧table中的元素复制到新的table中 if (oldTab != null) &#123; //遍历旧的table的每个桶 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; //如果该桶中含有元素，那么久开始复制，先使用e复制下来 if ((e = oldTab[j]) != null) &#123; //然后把旧的桶赋为null，便于GC回收 oldTab[j] = null; //如果这个桶中只有一个结点，那么计算新的坐标后放入 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; //如果这个桶中的数据结构为红黑树，那么就使用红黑树的方法将其拆分后复制 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // 使用两个头尾对象保持顺序，是由于链表中的元素的下标在扩容后,要么是原下标+oldCap,要么不变,下面会证实 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123;//遍历链表，分别把要存放新坐标的结点和要存放旧坐标的结点放到两根链表中 next = e.next; //如果计算得到0，那么下标没有改变，使用旧的头尾对象保存 if ((e.hash &amp; oldCap) == 0) &#123; //如果链表中没有结点，就把该节点设置为头节点 if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123;//否则下标改变，使用新的头尾对象保存 //如果链表中没有结点，就把该节点设置为头节点 if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原下标对应的链表 if (loTail != null) &#123; // 尾部节点next设置为null，代码严谨 loTail.next = null; //下标没有改变 newTab[j] = loHead; &#125; // 新下标对应的链表 if (hiTail != null) &#123; hiTail.next = null; //新下标为就 旧的下标+新的容量 newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; resize方法总结：总体可以两大部分： 首先是计算新桶数组的容量 newCap 和新阈值 newThr 将原集合的元素重新映射到新集合中细节的过程如下： remove方法12345678910111213/** * 如果存在，则从此映射中删除指定键的映射，并且返回与该键相关联的值。 * * @param key 要从映射中删除其映射的键 * @return 与key关联的值，如果没有key的映射，则为null。 * (null返回值还可以代表将null与key关联的映射。) */ public V remove(Object key) &#123; Node&lt;K,V&gt; e; //removeNode方法就在下面 return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value; &#125; removeNode方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * 实现了 Map接口的remove方法 和其他相关方法 * * @param hash key（键）的hash值 * @param key 键 * @param value 如果matchValue为true，则value也作为确定被删除的node的条件之一，否则忽略 * @param matchValue 如果为true，则仅在键值都相等时删除 * @param movable 如果为false，删除时不会移动其他节点 * @return Node节点，如果没有，则为空 */ final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; //如果table数组不为空，且数组内有元素，且根据hash值计算得到的下标处的桶里有元素，才寻找，否则直接返回null if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; //如果桶上第一个node的就是要删除的node，那么就把他先记录下来，在下面删除 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; //如果第一个结点不是，并且还有后续结点，那么就在后续节点中还寻找 else if ((e = p.next) != null) &#123; //如果是红黑树，就是用红黑树的方法寻找这个结点，也记录下来 if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123;//如果是链表，就从链表的第二个个节点开始遍历寻找 do &#123;//如果找到，就把这个这个结点记录下来，在下面删除 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; //如果得到的node不为null且(matchValue为false||node.value和参数value匹配) if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; //如果是红黑树，就使用红黑树的方法删除 if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p)//如果第一个结点就是要删除的目标，则使用第二个结点代替第一个结点 tab[index] = node.next; else//如果要删除的目标结点在链表中，则使用下一个结点代替该结点 p.next = node.next; //结构修改记录加一，元素个数减一 ++modCount; --size; //回调函数，最后会讲 afterNodeRemoval(node); //把删除的结点返回 return node; &#125; &#125; //如果数组table为空或key映射到的桶为空，返回null。 return null; &#125; 总结removeNode方法为： 如果数组table为空或key映射到的桶为空，直接返回null。 如果key映射到的桶上第一个Node的就是要删除的Node，记录下来。 如果桶内不止一个Node，且桶内的结构为红黑树，记录key映射到的Node。 桶内的结构不为红黑树，那么桶内的结构就肯定为链表，遍历链表，找到key映射到的Node，记录下来。 如果被记录下来的Node不为null，则使用数据结构相对应的删除方法删除Node，++modCount;–size; 返回被删除的node。 clear方法1234567891011121314151617/** * 删除HashMap中的所有映射。 * 这个调用返回后HashMap将为空。 */public void clear() &#123; Node&lt;K,V&gt;[] tab; //结构修改次数+1 modCount++; //如果table不为空且其中有元素，就进行清空 if ((tab = table) != null &amp;&amp; size &gt; 0) &#123; //元素数量设置为零 size = 0; //遍历table数组每一个桶，将桶置为null，剩下的交给让GC自动回收 for (int i = 0; i &lt; tab.length; ++i) tab[i] = null; &#125;&#125; containsValue方法12345678910111213141516171819202122232425/** * 如果此HashMap中将一个或多个键映射到指定的值，则返回true。 * * * @param value 值，其在此映射中的存在性将被测试 * @return 如果此映射将一个或多个键映射到指定值，则返回true，否则返回false。 * */public boolean containsValue(Object value) &#123; Node&lt;K,V&gt;[] tab; V v; //如果table不为空且其中有元素，就进行寻找，否则直接返回false if ((tab = table) != null &amp;&amp; size &gt; 0) &#123; //遍历table数组中每个小标出处的桶寻找 for (Node&lt;K,V&gt; e : tab) &#123; //遍历桶中的Node结点链 for (; e != null; e = e.next) &#123; //如果有值匹配，就返回true if ((v = e.value) == value || (value != null &amp;&amp; value.equals(v))) return true; &#125; &#125; &#125; return false;&#125; 一些其他方法keySet方法1234567891011121314151617/** * 返回此映射中所包含的键的 Set 视图。 * 该 set 受映射的支持，所以对映射的更改将反映在该 set 中，反之亦然。 * 如果在对 set 进行迭代的同时修改了映射（通过迭代器自己的 remove 操作除外），则迭代结果是不确定的。 * 该 set 支持元素的移除，通过 Iterator.remove、 Set.remove、 removeAll、 retainAll 和 clear 操作可从该映射中移除相应的映射关系。 * 它不支持 add 或 addAll 操作。 * * @return 此映射中包含的键的 set 视图 */ public Set&lt;K&gt; keySet() &#123; Set&lt;K&gt; ks = keySet; if (ks == null) &#123; ks = new KeySet(); keySet = ks; &#125; return ks; &#125; values方法123456789101112131415161718/** * 返回此映射所包含的值的 Collection 视图。 * 该 collection 受映射的支持，所以对映射的更改将反映在该 collection 中，反之亦然。 * 如果在对 collection 进行迭代的同时修改了映射（通过迭代器自己的 remove 操作除外），则迭代结果是不确定的。 * 该 collection 支持元素的移除， * 通过 Iterator.remove、 Collection.remove、 removeAll、 retainAll 和 clear 操作可从该映射中移除相应的映射关系。 * 它不支持 add 或 addAll 操作。 * * @return a view of the values contained in this map */public Collection&lt;V&gt; values() &#123; Collection&lt;V&gt; vs = values; if (vs == null) &#123; vs = new Values(); values = vs; &#125; return vs;&#125; entrySet方法12345678910111213/** * 返回此映射所包含的映射关系的 Set 视图。 * 该 set 受映射支持，所以对映射的更改将反映在此 set 中，反之亦然。 * 如果在对 set 进行迭代的同时修改了映射（通过迭代器自己的 remove 操作，或者通过在该迭代器返回的映射项上执行 setValue 操作除外），则迭代结果是不确定的。 * 该 set 支持元素的移除，通过 Iterator.remove、 Set.remove、 removeAll、 retainAll 和 clear 操作可从该映射中移除相应的映射关系。 * 它不支持 add 或 addAll 操作。 * * @return a set view of the mappings contained in this map */public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() &#123; Set&lt;Map.Entry&lt;K,V&gt;&gt; es; return (es = entrySet) == null ? (entrySet = new EntrySet()) : es;&#125; clone方法1234567891011121314151617181920212223/** * 返回此HashMap实例的浅拷贝:键和值本身没有克隆。 * 浅拷贝与深拷贝的区别： * 简单的来说就是，在有指针的情况下，浅拷贝只是增加了一个指针指向已经存在的内存 * 而深拷贝就是增加一个指针并且申请一个新的内存，使这个增加的指针指向这个新的内存 * 采用深拷贝的情况下，释放内存的时候就不会出现在浅拷贝时重复释放同一内存的错误！ * * @return a shallow copy of this map */@SuppressWarnings("unchecked")@Overridepublic Object clone() &#123; HashMap&lt;K,V&gt; result; try &#123; result = (HashMap&lt;K,V&gt;)super.clone(); &#125; catch (CloneNotSupportedException e) &#123; // this shouldn't happen, since we are Cloneable throw new InternalError(e); &#125; result.reinitialize(); result.putMapEntries(this, false); return result;&#125; 回调方法1234// 允许LinkedHashMap后操作的回调void afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125;void afterNodeInsertion(boolean evict) &#123; &#125;void afterNodeRemoval(Node&lt;K,V&gt; p) &#123; &#125; 这三个回调方法在之前方法中出现过，它们的作用就是在给LinkedHashMap时继承使用，在HashMap中没有实质的作用，所以方法体为空。LinkedHashMap 是 HashMap 的一个子类，它保留插入的顺序，如果需要输出的顺序和输入时的相同，那么就选用 LinkedHashMap。 个人总结 可以看出HashMap在扩容时的操作是很花费时间的，所以尽量在创建HashMap的时候就把容量指定，避免扩容操作，增大运行时间。 不知道有没有人想过，为什么在很多方法中，都是新建局部变量，然后把相应的数据赋给局部变量，而不是直接使用全局变量呢？例如下面这样：12345Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k;tab = table;n = tab.length;first = tab[(n - 1) &amp; hash];k = first.key; 个人猜测这样做的原因是：新定义的变量在栈顶，出栈快，局部变量，用完就销毁，提高速度，也不额外占用内存。当然还有一种可能是因为HashMap不是线程安全的，所以可能因为使用全局变量的话会导致数据差异的原因，所以在每个方法里面，把这个方法开始的时候的数据保存下来，只对当前保存下来的数据进行运算，不影响其他线程和方法对数据的使用，同时也体现了高明的严谨性。 当然这只是个人猜测的结果，具体的原因也没有查到，所以这里就算是一个遗留的小问题吧。]]></content>
      <categories>
        <category>Java容器</category>
      </categories>
  </entry>
</search>
